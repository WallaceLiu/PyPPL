{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyPPL - A Py thon P i P e L ine framework Documentation | API | Change log | FAQ Features Easy-to-use command line parser. Fancy logs. Process caching. Script templating (using either builtin engine or Jinja2). Runner customization . Error handling for processes. Easy-switching running profile. Flowchat in DOT for your pipelines ( Details ). Aggregations (a set of processes predefined). Highly reusable processes (see a set of highly reusable bioinformatics processes ). Requirements OS: Linux or OSX Python packages: six , filelock , futures (suggested: graphviz , pyyaml and python-testly ). Installation # install latest version git clone https://github.com/pwwang/PyPPL.git cd PyPPL python setup.py install # or simply: pip install git+git://github.com/pwwang/PyPPL.git # install released version pip install PyPPL # run tests pip install python-testly # or pip install git+git://github.com/pwwang/testly.git make test # run tests only for python2 make test2 # run tests only for python3 make test3 # run tutorials make tutorials Get started See tutorials/getStarted/ Sort 5 files simultaneously: 1. from pyppl import PyPPL , Proc , Channel 2. pSort = Proc ( desc = Sort files. ) 3. pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} 4. pSort . output = outfile:file:{{in.infile | fn}}.sorted 5. pSort . forks = 5 6. pSort . exdir = ./export 7. pSort . script = sort -k1r {{in.infile}} {{out.outfile}} 8. PyPPL () . start ( pSort ) . run () Line 1 : Import the modules. Line 2 : Define the process with a description. Line 3 : Define the input data for the process. Line 4 : Define the output. Templates are also applied here. Line 5 : Define how many jobs are running simultaneously. Line 6 : Set the directory to export the output files. Line 7 : Set your script to run. Line 8 : Set the starting process and run the pipeline. ls -l ./export total 0 -rw-rw-rw- 1 pwwang pwwang 44 Sep 14 20 :50 test1.sorted -rw-rw-rw- 1 pwwang pwwang 56 Sep 14 20 :50 test2.sorted -rw-rw-rw- 1 pwwang pwwang 59 Sep 14 20 :50 test3.sorted -rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20 :50 test4.sorted -rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20 :50 test5.sorted Infer input channel from dependent process See tutorials/inputFromDependent/ If a process depends on another one, the input channel can be inferred from the output channel of the latter process. Sort 5 files and then add line number to each line. from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 pSort . script = sort -k1r {{in.infile}} {{out.outfile}} pAddPrefix = Proc ( desc = Add line number to each line. ) pAddPrefix . depends = pSort # automatically inferred from pSort.output pAddPrefix . input = infile:file pAddPrefix . output = outfile:file:{{in.infile | fn}}.ln pAddPrefix . exdir = ./export pAddPrefix . forks = 5 pAddPrefix . script = paste -d. (seq 1 $(wc -l {{in.infile}} | cut -f1 -d )) {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run () head -3 ./export/test1.ln 1.8984 2.663 3.625 Modify input channel See tutorials/transformInputChannels/ Sort 5 files, add line numbers, and merge them into one file. from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 pSort . script = sort -k1r {{in.infile}} {{out.outfile}} pAddPrefix = Proc ( desc = Add line number to each line. ) pAddPrefix . depends = pSort pAddPrefix . input = infile:file # automatically inferred from pSort.output pAddPrefix . output = outfile:file:{{in.infile | fn}}.ln pAddPrefix . forks = 5 pAddPrefix . script = paste -d. (seq 1 $(wc -l {{in.infile}} | cut -f1 -d )) {{in.infile}} {{out.outfile}} pMergeFiles = Proc ( desc = Merge files, each as a column. ) pMergeFiles . depends = pAddPrefix # Transform it into a list of files # [ test1.ln , test2.ln , ..., test5.ln ] pMergeFiles . input = { infiles:files : lambda ch : [ ch . flatten ()]} pMergeFiles . output = outfile:file:mergedfile.txt pMergeFiles . exdir = ./export pMergeFiles . script = paste {{in.infiles | asquote}} {{out.outfile}} PyPPL () . start ( pSort ) . run () head -3 ./export/mergedfile.txt 1.8984 1.6448 1.2915 1.7269 1.7692 2.663 2.3369 2.26223 2.3866 2.7536 3.625 3.28984 3.25945 3.29971 3.30204 Use a different language See tutorials/differentLang/ Plot heatmap using R. from pyppl import PyPPL , Proc pHeatmap = Proc ( desc = Draw heatmap. ) pHeatmap . input = { seed : 8525 } pHeatmap . output = outfile:file:heatmap.png pHeatmap . exdir = ./export # Use full path /path/to/Rscript if it s not in $PATH # You can also use a shebang in script # in this case: #!/usr/bin/env Rscript pHeatmap . lang = Rscript pHeatmap . script = set.seed({{in.seed}}) mat = matrix(rnorm(100), ncol=10) png(filename = {{out.outfile}} ) heatmap(mat) dev.off() PyPPL () . start ( pHeatmap ) . run () ./export/heatmap.png Use args See tutorials/useArgs/ If the jobs are sharing the same set of configurations (in this case, the number of rows and columns of the matrix), they can be set in pXXX.args . The other benefit is to make the channels intact if the configurations are not suppose to be channeling. from pyppl import PyPPL , Proc pHeatmap = Proc ( desc = Draw heatmap. ) pHeatmap . input = { seed : [ 1 , 2 , 3 ]} pHeatmap . output = outfile:file:heatmap{{in.seed}}.png pHeatmap . exdir = ./export pHeatmap . forks = 3 pHeatmap . args . ncol = 10 pHeatmap . args . nrow = 10 pHeatmap . lang = Rscript # or /path/to/Rscript if it s not in $PATH pHeatmap . script = set.seed({{in.seed}}) mat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}}) png(filename = {{out.outfile}} , width=150, height=150) heatmap(mat) dev.off() PyPPL () . start ( pHeatmap ) . run () ./export/heatmap1.png ./export/heatmap2.png ./export/heatmap3.png Use the command line argument parser See tutorials/useParams/ from pyppl import PyPPL , Proc , Channel , params params . datadir \\ . setRequired () \\ . setDesc ( The data directory containing the data files. ) # or # params.datadir.required = True # params.datadir.desc = The data directory containing the data files. params = params . parse () pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( params . datadir + /*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 pSort . exdir = ./export pSort . script = sort -k1r {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run () Run the pipeline: python useParams.py USAGE: useParams.py -datadir str REQUIRED OPTIONS: -datadir str The data directory containing the data files. OPTIONAL OPTIONS: -h, --help, -H, -? Print this help information. Provide value to -datadir : python useParams.py -datadir ./data Use a different runner See /tutorials/differentRunner/ from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted # specify the runner pSort . runner = sge # specify the runner options pSort . sgeRunner = { sge.q : 1-day } pSort . forks = 5 pSort . exdir = ./export pSort . script = sort -k1r {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run () # or run all process with sge runner: # PyPPL().start(pSort).run( sge ) # or: # PyPPL({ # default : { # runner : sge , # sgeRunner : { sge.q : 1-day } # } # }).start(pSort).run() Use Jinja2 as template engine See /tutorials/useJinja2/ from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} # Notice the different between builtin template engine and Jinja2 pSort . output = outfile:file:{{ fn(in.infile) }}.sorted # pSort.output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 # You have to have Jinja2 installed (pip install Jinja2) pSort . template = Jinja2 pSort . exdir = ./export pSort . script = sort -k1r {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run () Debug your script See /tutorials/debugScript/ You can directly go to workdir / job.index /job.script to debug your script, or you can also print some values out throught PyPPL log system. from pyppl import PyPPL , Proc pHeatmap = Proc ( desc = Draw heatmap. ) pHeatmap . input = { seed : [ 1 , 2 , 3 , 4 , 5 ]} pHeatmap . output = outfile:file:heatmap{{in.seed}}.png pHeatmap . exdir = ./export # Don t cache jobs for debugging pHeatmap . cache = False # Output debug information for all jobs, but don t echo stdout and stderr pHeatmap . echo = { jobs : range ( 5 ), type : } pHeatmap . args . ncol = 10 pHeatmap . args . nrow = 10 pHeatmap . lang = Rscript # or /path/to/Rscript if it s not in $PATH pHeatmap . script = set.seed({{in.seed}}) mat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}}) png(filename = {{out.outfile}} , width=150, height=150) # have to be on stderr cat( pyppl.log.debug:Plotting heatmap #{{job.index | lambda x: int(x) + 1}} ... , file = stderr()) heatmap(mat) dev.off() PyPPL ({ _log : { levels : basic , lvldiff : [] } }) . start ( pHeatmap ) . run () You will get something like this in your log: Switch runner profiles See tutorials/siwthcRunnerProfile/ We can define a set of runner profiles in a json file ( ./profiles.json ): { default : { runner : local , forks : 1 , sgeRunner : { sge.q : 1-day } }, local5 : { runner : local , forks : 5 }, sge7days : { runner : sge , sgeRunner : { sge.q : 7-days } } } or you can also use .yaml ( pyyaml is required) file: default : runner : local forks : 1 sgeRunner : sge.q : 1-day local5 : runner : local forks : 5 sge7days : runner : local sgeRunner : sge.q : 7-days To switch profile: # default profile (default) PyPPL ( cfgfile = ./profiles.json ) . start ( pHeatmap ) . run () # switch to local5 or sge7days: # PyPPL(cfgfile = ./profiles.json ).start(pHeatmap).run( local5 ) # PyPPL(cfgfile = ./profiles.json ).start(pHeatmap).run( sge7days ) # You may also use runner name as profile, which means to run using the runner with default options: # PyPPL(cfgfile = ./profiles.json ).start(pHeatmap).run( sge ) # use 1-day queue Draw the pipeline chart PyPPL can generate the graph in DOT language . from pyppl import PyPPL , Proc p1 = Proc () p2 = Proc () p3 = Proc () p4 = Proc () p5 = Proc () p6 = Proc () p7 = Proc () p8 = Proc () p9 = Proc () p1 p8 / \\ / p2 p3 \\ / p4 p9 / \\ / p5 p6 (export) \\ / p7 (export) p2 . depends = p1 p3 . depends = p1 , p8 p4 . depends = p2 , p3 p4 . exdir = ./export p5 . depends = p4 p6 . depends = p4 , p9 p6 . exdir = ./export p7 . depends = p5 , p6 p7 . exdir = ./export # make sure at least one job is created. p1 . input = { in : [ 0 ]} p8 . input = { in : [ 0 ]} p9 . input = { in : [ 0 ]} PyPPL () . start ( p1 , p8 , p9 ) . flowchart () . run () drawFlowchart.pyppl.dot : digraph PyPPL { p8 [color= #259229 fillcolor= #ffffff fontcolor= #000000 shape= box style= filled ] p1 [color= #259229 fillcolor= #ffffff fontcolor= #000000 shape= box style= filled ] p9 [color= #259229 fillcolor= #ffffff fontcolor= #000000 shape= box style= filled ] p7 [color= #d63125 fillcolor= #ffffff fontcolor= #c71be4 shape= box style= filled ] p5 [color= #000000 fillcolor= #ffffff fontcolor= #000000 shape= box style= rounded,filled ] p4 [color= #000000 fillcolor= #ffffff fontcolor= #c71be4 shape= box style= rounded,filled ] p2 [color= #000000 fillcolor= #ffffff fontcolor= #000000 shape= box style= rounded,filled ] p3 [color= #000000 fillcolor= #ffffff fontcolor= #000000 shape= box style= rounded,filled ] p6 [color= #000000 fillcolor= #ffffff fontcolor= #c71be4 shape= box style= rounded,filled ] p2 - p4 p3 - p4 p1 - p2 p1 - p3 p6 - p7 p4 - p5 p4 - p6 p5 - p7 p8 - p3 p9 - p6 } To generate svg file, you have to have graphviz installed. drawFlowchart.pyppl.svg : Enjoy pipelining!!!","title":"Introduction"},{"location":"#pyppl-a-python-pipeline-framework","text":"Documentation | API | Change log | FAQ","title":"PyPPL - A Python PiPeLine framework"},{"location":"#features","text":"Easy-to-use command line parser. Fancy logs. Process caching. Script templating (using either builtin engine or Jinja2). Runner customization . Error handling for processes. Easy-switching running profile. Flowchat in DOT for your pipelines ( Details ). Aggregations (a set of processes predefined). Highly reusable processes (see a set of highly reusable bioinformatics processes ).","title":"Features"},{"location":"#requirements","text":"OS: Linux or OSX Python packages: six , filelock , futures (suggested: graphviz , pyyaml and python-testly ).","title":"Requirements"},{"location":"#installation","text":"# install latest version git clone https://github.com/pwwang/PyPPL.git cd PyPPL python setup.py install # or simply: pip install git+git://github.com/pwwang/PyPPL.git # install released version pip install PyPPL # run tests pip install python-testly # or pip install git+git://github.com/pwwang/testly.git make test # run tests only for python2 make test2 # run tests only for python3 make test3 # run tutorials make tutorials","title":"Installation"},{"location":"#get-started","text":"See tutorials/getStarted/ Sort 5 files simultaneously: 1. from pyppl import PyPPL , Proc , Channel 2. pSort = Proc ( desc = Sort files. ) 3. pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} 4. pSort . output = outfile:file:{{in.infile | fn}}.sorted 5. pSort . forks = 5 6. pSort . exdir = ./export 7. pSort . script = sort -k1r {{in.infile}} {{out.outfile}} 8. PyPPL () . start ( pSort ) . run () Line 1 : Import the modules. Line 2 : Define the process with a description. Line 3 : Define the input data for the process. Line 4 : Define the output. Templates are also applied here. Line 5 : Define how many jobs are running simultaneously. Line 6 : Set the directory to export the output files. Line 7 : Set your script to run. Line 8 : Set the starting process and run the pipeline. ls -l ./export total 0 -rw-rw-rw- 1 pwwang pwwang 44 Sep 14 20 :50 test1.sorted -rw-rw-rw- 1 pwwang pwwang 56 Sep 14 20 :50 test2.sorted -rw-rw-rw- 1 pwwang pwwang 59 Sep 14 20 :50 test3.sorted -rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20 :50 test4.sorted -rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20 :50 test5.sorted","title":"Get started"},{"location":"#infer-input-channel-from-dependent-process","text":"See tutorials/inputFromDependent/ If a process depends on another one, the input channel can be inferred from the output channel of the latter process. Sort 5 files and then add line number to each line. from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 pSort . script = sort -k1r {{in.infile}} {{out.outfile}} pAddPrefix = Proc ( desc = Add line number to each line. ) pAddPrefix . depends = pSort # automatically inferred from pSort.output pAddPrefix . input = infile:file pAddPrefix . output = outfile:file:{{in.infile | fn}}.ln pAddPrefix . exdir = ./export pAddPrefix . forks = 5 pAddPrefix . script = paste -d. (seq 1 $(wc -l {{in.infile}} | cut -f1 -d )) {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run () head -3 ./export/test1.ln 1.8984 2.663 3.625","title":"Infer input channel from dependent process"},{"location":"#modify-input-channel","text":"See tutorials/transformInputChannels/ Sort 5 files, add line numbers, and merge them into one file. from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 pSort . script = sort -k1r {{in.infile}} {{out.outfile}} pAddPrefix = Proc ( desc = Add line number to each line. ) pAddPrefix . depends = pSort pAddPrefix . input = infile:file # automatically inferred from pSort.output pAddPrefix . output = outfile:file:{{in.infile | fn}}.ln pAddPrefix . forks = 5 pAddPrefix . script = paste -d. (seq 1 $(wc -l {{in.infile}} | cut -f1 -d )) {{in.infile}} {{out.outfile}} pMergeFiles = Proc ( desc = Merge files, each as a column. ) pMergeFiles . depends = pAddPrefix # Transform it into a list of files # [ test1.ln , test2.ln , ..., test5.ln ] pMergeFiles . input = { infiles:files : lambda ch : [ ch . flatten ()]} pMergeFiles . output = outfile:file:mergedfile.txt pMergeFiles . exdir = ./export pMergeFiles . script = paste {{in.infiles | asquote}} {{out.outfile}} PyPPL () . start ( pSort ) . run () head -3 ./export/mergedfile.txt 1.8984 1.6448 1.2915 1.7269 1.7692 2.663 2.3369 2.26223 2.3866 2.7536 3.625 3.28984 3.25945 3.29971 3.30204","title":"Modify input channel"},{"location":"#use-a-different-language","text":"See tutorials/differentLang/ Plot heatmap using R. from pyppl import PyPPL , Proc pHeatmap = Proc ( desc = Draw heatmap. ) pHeatmap . input = { seed : 8525 } pHeatmap . output = outfile:file:heatmap.png pHeatmap . exdir = ./export # Use full path /path/to/Rscript if it s not in $PATH # You can also use a shebang in script # in this case: #!/usr/bin/env Rscript pHeatmap . lang = Rscript pHeatmap . script = set.seed({{in.seed}}) mat = matrix(rnorm(100), ncol=10) png(filename = {{out.outfile}} ) heatmap(mat) dev.off() PyPPL () . start ( pHeatmap ) . run () ./export/heatmap.png","title":"Use a different language"},{"location":"#use-args","text":"See tutorials/useArgs/ If the jobs are sharing the same set of configurations (in this case, the number of rows and columns of the matrix), they can be set in pXXX.args . The other benefit is to make the channels intact if the configurations are not suppose to be channeling. from pyppl import PyPPL , Proc pHeatmap = Proc ( desc = Draw heatmap. ) pHeatmap . input = { seed : [ 1 , 2 , 3 ]} pHeatmap . output = outfile:file:heatmap{{in.seed}}.png pHeatmap . exdir = ./export pHeatmap . forks = 3 pHeatmap . args . ncol = 10 pHeatmap . args . nrow = 10 pHeatmap . lang = Rscript # or /path/to/Rscript if it s not in $PATH pHeatmap . script = set.seed({{in.seed}}) mat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}}) png(filename = {{out.outfile}} , width=150, height=150) heatmap(mat) dev.off() PyPPL () . start ( pHeatmap ) . run () ./export/heatmap1.png ./export/heatmap2.png ./export/heatmap3.png","title":"Use args"},{"location":"#use-the-command-line-argument-parser","text":"See tutorials/useParams/ from pyppl import PyPPL , Proc , Channel , params params . datadir \\ . setRequired () \\ . setDesc ( The data directory containing the data files. ) # or # params.datadir.required = True # params.datadir.desc = The data directory containing the data files. params = params . parse () pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( params . datadir + /*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 pSort . exdir = ./export pSort . script = sort -k1r {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run () Run the pipeline: python useParams.py USAGE: useParams.py -datadir str REQUIRED OPTIONS: -datadir str The data directory containing the data files. OPTIONAL OPTIONS: -h, --help, -H, -? Print this help information. Provide value to -datadir : python useParams.py -datadir ./data","title":"Use the command line argument parser"},{"location":"#use-a-different-runner","text":"See /tutorials/differentRunner/ from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} pSort . output = outfile:file:{{in.infile | fn}}.sorted # specify the runner pSort . runner = sge # specify the runner options pSort . sgeRunner = { sge.q : 1-day } pSort . forks = 5 pSort . exdir = ./export pSort . script = sort -k1r {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run () # or run all process with sge runner: # PyPPL().start(pSort).run( sge ) # or: # PyPPL({ # default : { # runner : sge , # sgeRunner : { sge.q : 1-day } # } # }).start(pSort).run()","title":"Use a different runner"},{"location":"#use-jinja2-as-template-engine","text":"See /tutorials/useJinja2/ from pyppl import PyPPL , Proc , Channel pSort = Proc ( desc = Sort files. ) pSort . input = { infile:file : Channel . fromPattern ( ./data/*.txt )} # Notice the different between builtin template engine and Jinja2 pSort . output = outfile:file:{{ fn(in.infile) }}.sorted # pSort.output = outfile:file:{{in.infile | fn}}.sorted pSort . forks = 5 # You have to have Jinja2 installed (pip install Jinja2) pSort . template = Jinja2 pSort . exdir = ./export pSort . script = sort -k1r {{in.infile}} {{out.outfile}} PyPPL () . start ( pSort ) . run ()","title":"Use Jinja2 as template engine"},{"location":"#debug-your-script","text":"See /tutorials/debugScript/ You can directly go to workdir / job.index /job.script to debug your script, or you can also print some values out throught PyPPL log system. from pyppl import PyPPL , Proc pHeatmap = Proc ( desc = Draw heatmap. ) pHeatmap . input = { seed : [ 1 , 2 , 3 , 4 , 5 ]} pHeatmap . output = outfile:file:heatmap{{in.seed}}.png pHeatmap . exdir = ./export # Don t cache jobs for debugging pHeatmap . cache = False # Output debug information for all jobs, but don t echo stdout and stderr pHeatmap . echo = { jobs : range ( 5 ), type : } pHeatmap . args . ncol = 10 pHeatmap . args . nrow = 10 pHeatmap . lang = Rscript # or /path/to/Rscript if it s not in $PATH pHeatmap . script = set.seed({{in.seed}}) mat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}}) png(filename = {{out.outfile}} , width=150, height=150) # have to be on stderr cat( pyppl.log.debug:Plotting heatmap #{{job.index | lambda x: int(x) + 1}} ... , file = stderr()) heatmap(mat) dev.off() PyPPL ({ _log : { levels : basic , lvldiff : [] } }) . start ( pHeatmap ) . run () You will get something like this in your log:","title":"Debug your script"},{"location":"#switch-runner-profiles","text":"See tutorials/siwthcRunnerProfile/ We can define a set of runner profiles in a json file ( ./profiles.json ): { default : { runner : local , forks : 1 , sgeRunner : { sge.q : 1-day } }, local5 : { runner : local , forks : 5 }, sge7days : { runner : sge , sgeRunner : { sge.q : 7-days } } } or you can also use .yaml ( pyyaml is required) file: default : runner : local forks : 1 sgeRunner : sge.q : 1-day local5 : runner : local forks : 5 sge7days : runner : local sgeRunner : sge.q : 7-days To switch profile: # default profile (default) PyPPL ( cfgfile = ./profiles.json ) . start ( pHeatmap ) . run () # switch to local5 or sge7days: # PyPPL(cfgfile = ./profiles.json ).start(pHeatmap).run( local5 ) # PyPPL(cfgfile = ./profiles.json ).start(pHeatmap).run( sge7days ) # You may also use runner name as profile, which means to run using the runner with default options: # PyPPL(cfgfile = ./profiles.json ).start(pHeatmap).run( sge ) # use 1-day queue","title":"Switch runner profiles"},{"location":"#draw-the-pipeline-chart","text":"PyPPL can generate the graph in DOT language . from pyppl import PyPPL , Proc p1 = Proc () p2 = Proc () p3 = Proc () p4 = Proc () p5 = Proc () p6 = Proc () p7 = Proc () p8 = Proc () p9 = Proc () p1 p8 / \\ / p2 p3 \\ / p4 p9 / \\ / p5 p6 (export) \\ / p7 (export) p2 . depends = p1 p3 . depends = p1 , p8 p4 . depends = p2 , p3 p4 . exdir = ./export p5 . depends = p4 p6 . depends = p4 , p9 p6 . exdir = ./export p7 . depends = p5 , p6 p7 . exdir = ./export # make sure at least one job is created. p1 . input = { in : [ 0 ]} p8 . input = { in : [ 0 ]} p9 . input = { in : [ 0 ]} PyPPL () . start ( p1 , p8 , p9 ) . flowchart () . run () drawFlowchart.pyppl.dot : digraph PyPPL { p8 [color= #259229 fillcolor= #ffffff fontcolor= #000000 shape= box style= filled ] p1 [color= #259229 fillcolor= #ffffff fontcolor= #000000 shape= box style= filled ] p9 [color= #259229 fillcolor= #ffffff fontcolor= #000000 shape= box style= filled ] p7 [color= #d63125 fillcolor= #ffffff fontcolor= #c71be4 shape= box style= filled ] p5 [color= #000000 fillcolor= #ffffff fontcolor= #000000 shape= box style= rounded,filled ] p4 [color= #000000 fillcolor= #ffffff fontcolor= #c71be4 shape= box style= rounded,filled ] p2 [color= #000000 fillcolor= #ffffff fontcolor= #000000 shape= box style= rounded,filled ] p3 [color= #000000 fillcolor= #ffffff fontcolor= #000000 shape= box style= rounded,filled ] p6 [color= #000000 fillcolor= #ffffff fontcolor= #c71be4 shape= box style= rounded,filled ] p2 - p4 p3 - p4 p1 - p2 p1 - p3 p6 - p7 p4 - p5 p4 - p6 p5 - p7 p8 - p3 p9 - p6 } To generate svg file, you have to have graphviz installed. drawFlowchart.pyppl.svg : Enjoy pipelining!!!","title":"Draw the pipeline chart"},{"location":"SUMMARY/","text":"Summary Introduction Basics and folder structure Templating Channels Input and output of a process The heart: script Output file exporting Caching and resuming processes Runners Other attributes of a process Log configuration Pipeline configuration Pipeline flowchart Command line argument parser Aggregations Command line tool API FAQ Change log","title":"Summary"},{"location":"SUMMARY/#summary","text":"Introduction Basics and folder structure Templating Channels Input and output of a process The heart: script Output file exporting Caching and resuming processes Runners Other attributes of a process Log configuration Pipeline configuration Pipeline flowchart Command line argument parser Aggregations Command line tool API FAQ Change log","title":"Summary"},{"location":"aggregations/","text":"Aggregations Imagine that you have a set of processes predefined, and every time when you deal with similar problems (i.e. format a file and plot the data or some next generation sequencing data analysis), you will consistently use those processes, then you have to configure and call them every time. Aggregations are designed for this kind of situations, you can just define an aggregations with those processes, and adjust the dependencies, input and arguments, you will be able to re-use the aggregation with very less configuration. For example: pTrimmomaticPE . input = input channel pAlignPEByBWA . depends = pTrimmomaticPE pSortSam . depends = pAlignPEByBWA pMarkDuplicates . depends = pSortSam pIndexBam . depends = pMarkDuplicates pRealignerTargetCreator . depends = pIndexBam pIndelRealigner . depends = pIndexBam , pRealignerTargetCreator pBaseRecalibrator . depends = pIndelRealigner pPrintReads . depends = pIndelRealigner , pBaseRecalibrator pPrintReads . exportdir = exdir pMarkDuplicates . args . params . tmpdir = /local2/tmp/ pAlignPEByBWA . args . reffile = reffile pRealignerTargetCreator . args . reffile = reffile pRealignerTargetCreator . args . params . tmpdir = /local2/tmp/ pIndelRealigner . args . reffile = reffile pIndelRealigner . args . params . tmpdir = /local2/tmp/ pBaseRecalibrator . args . reffile = reffile pBaseRecalibrator . args . knownSites = dbsnp pBaseRecalibrator . args . params . tmpdir = /local2/tmp/ pPrintReads . args . reffile = reffile pPrintReads . args . params = /local2/tmp/ PyPPL ({ proc : { forks : 100 , runner : sge , sgeRunner : { sge.q : lg-mem } } }) . start ( pTrimmomaticPE ) . run () This is a very commonly used Whole Genome Sequencing data cleanup pipeline from the raw reads according to the GATK best practice . And it will be used pretty much every time when the raw read files come. With an aggregation defined, you don't need to configure and call those processes every time: from pyppl import Aggr from bioprocs import params # some paramters defined in params aFastqPE2Bam = Aggr ( pTrimmomaticPE , pAlignPEByBWA , pSortSam , pMarkDuplicates , pIndexBam , pRealignerTargetCreator , pIndelRealigner , pBaseRecalibrator , pPrintReads ) # dependency adjustment aFastqPE2Bam . pIndelRealigner . depends = aFastqPE2Bam . pIndexBam , aFastqPE2Bam . pRealignerTargetCreator aFastqPE2Bam . pPrintReads . depends = aFastqPE2Bam . pIndelRealigner , aFastqPE2Bam . pBaseRecalibrator # input adjustment # args adjustment aFastqPE2Bam . pMarkDuplicates . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pAlignPEByBWA . args . reffile = params . hg19fa aFastqPE2Bam . pRealignerTargetCreator . args . reffile = params . hg19fa aFastqPE2Bam . pRealignerTargetCreator . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pIndelRealigner . args . reffile = params . hg19fa aFastqPE2Bam . pIndelRealigner . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pBaseRecalibrator . args . reffile = params . hg19fa aFastqPE2Bam . pBaseRecalibrator . args . knownSites = params . dbsnp aFastqPE2Bam . pBaseRecalibrator . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pPrintReads . args . reffile = params . hg19fa aFastqPE2Bam . pPrintReads . args . params . tmpdir = params . tmpdir Then every time you just need to call the aggregation: aFastqPE2Bam . input = channel . fromPairs ( datadir + /*.fastq.gz ) aFastqPE2Bam . exdir = exdir PyPPL ({ proc : { sgeRunner : { sge.q : 1-day } } }) . start ( aFastqPE2Bam ) . run () Initialize an aggregation Like previous example shows, you just need to give the constructor all the processes to construct an aggretation. However, there are several things need to be noticed: The dependencies are automatically constructed by the order of the processes. a = Aggr ( p1 , p2 , p3 ) # The dependencies will be p1 - p2 - p3 The starting and ending processes are defined as the first and last processes, respectively. If you need to modify the dependencies, keep that in mind whether the starting and ending processes are changed. a = Aggr ( p1 , p2 , p3 ) # a.starts == [p1] # a.ends == [p3] # / p2 # change the dependencies to p1 # \\ p3 # both p2, p3 depend on p1, and p3 depends on p2 a . p3 . depends = p1 , p2 # but remember the ending processes are changed from [p3] to [p2, p3] a . ends = [ p2 , p3 ] You can also specify the dependencies manually: a = Aggr ( p1 , p2 , p3 , depends = False ) a . p2 . depends = p1 a . p3 . depends = p1 , p2 a . starts = [ p1 ] a . ends = [ p2 , p3 ] If you have one process used twice in the aggregation, copy it with a different id: a = Aggr ( p1 , p2 , p1 . copy ( id = p1copy ) ) # then to access the 2nd p1: a.p1copy Each process is copied by aggregation, so the original one can still be used. The tag of each process is regenerated by the id of the aggregation. Add a process on the run aggr.addProc(p, where=None) You can add a process, and also define whether to put it in starts , ends , both or None . Delegate attributes of processes to an aggregation You can Delegate the attributes directly for a process to an aggregation: aFastqPE2Bam . delegate ( args.reffile , pAlignPEByBWA ) Then when you want to set args.reffile for pAlignPEByBWA , you can just do: aFastqPE2Bam . args . reffile = /path/to/hg19.fa You may use starts/ends represents the start/end processes. Delegate an attribute to multiple processes: aFastqPE2Bam . delegate ( args.reffile , pAlignPEByBWA, pPrintReads ) # or aFastqPE2Bam . delegate ( args.reffile , [ pAlignPEByBWA , pPrintReads ]) # or aFastqPE2Bam . delegate ( args.reffile , [ aFastqPE2Bam . pAlignPEByBWA , aFastqPE2Bam . pPrintReads ]) Delegate multiple attributes at one time: aFastqPE2Bam . delegate ( args.reffile, args.tmpdir , pAlignPEByBWA, pPrintReads ) # or aFastqPE2Bam . delegate ([ args.reffile , args.tmpdir ], pAlignPEByBWA, pPrintReads ) Caution Undelegated attributes will be delegated to all processes. But remember if an attribute has been set before, say p.runner = 'local' then, aggr.runner will not overwrite it if runner is not delegated. If it is, then it'll be overwritten. Note For attributes that have sub-attributes (i.e. p.args.params.inopts.cnames ), you may just delegate the first parts, then the full assignment of the attribute will still follow the delegation. For example: aggr . delegate ( args.params , p1,p2 ) aggr . args . params . inopts . cnames = True # only affects p1, p2 Keep in mind that shorter delegations always come first. In the above case, if we have another delegation: aggr.delegate('args.params.inopts', 'p3') , then the assignment will still affect p1, p2 (the first delegation) and p3 (the second delegation). Default delegations By default, input/depends are delegated for start processes, and exdir/exhow/exow/expart for end processes. Importantly, as aggr.starts is a list, the values for input/depends must be a list as well, with elements corresponing to each start process. Besides, we have two special attributes for aggregations: input2 and depends2 . Unlike input and depends , input2 and depends2 try to pass everything it gets to each process, instead of passing corresponind element to each process. For example: # aggr.starts = [aggr.p1, aggr.p2] aggr . input = [[ a ], [ b ]] # then: # aggr.p1.config[ input ] == [ a ] # aggr.p2.config[ input ] == [ b ] aggr . input2 = [[ a ], [ b ]] # then: # aggr.p1.config[ input ] == [[ a ], [ b ]] # aggr.p2.config[ input ] == [[ a ], [ b ]] Set attribute value for specific processes of an aggregation There are several ways to do that: # refer to the process directly aFastqPE2Bam . pPrintReads . args . tmpdir = /tmp aFastqPE2Bam . pPrintReads . runner = local # refer to the index of the process aFastqPE2Bam [ 8 ] . args . tmpdir = /tmp aFastqPE2Bam [ 8 ] . runner = local # refer to the name of the process aFastqPE2Bam [ pPrintReads ] . args . tmpdir = /tmp aFastqPE2Bam [ pPrintReads ] . runner = local # for multiple processes aFastqPE2Bam [: 3 ] . args . tmpdir = /tmp aFastqPE2Bam [ 0 , 1 , 3 ] . args . tmpdir = /tmp aFastqPE2Bam [ aFastqPE2Bam , pPrintReads ] . args . tmpdir = /tmp aFastqPE2Bam [ aFastqPE2Bam, pPrintReads ] . args . tmpdir = /tmp # or you may use starts/ends to refer to the start/end processes # has to be done after aggr.starts/aggr.ends assigned # or initialized with depends = True aFastqPE2Bam [ starts ] . args . tmpdir = /tmp aFastqPE2Bam [ ends ] . args . tmpdir = /tmp Hint If an attribute is delegated for other processes, you can still set the value of it by the above methods. Note When use __getitem__ to select processes from aggregations, only the index or the id will return the processes itself (instance of Proc ), otherwise it will return an instance _Proxy , which is proxy used to pass attribute values to a set of processes. So pay attention to it, because we can do aggr['pXXX1, pXXX2'].depends = \"pXXX3, pXXX4\" to aggr.pXXX3 and aggr.pXXX4 as the dependent of aggr.pXXX1 and aggr.pXXX2 respectively, but if you do aggr['pXXX1'] = 'pXXX3' will raise an error. Because _Proxy helps the aggregation to select the processes from itselt, but a Proc instance doesn't know how to. You may turn it into _Proxy from Proc instance simply by add a comma to the selector: isinstance ( aggr [ 0 ], Proc ) isinstance ( aggr [ 0 ,], _Proxy ) isinstance ( aggr [ pXXX1 ], Proc ) isinstance ( aggr [ pXXX1, ], _Proxy ) # or isinstance ( aggr [ pXXX1 ,], _Proxy ) Define modules of an aggregation. We can define some modules for an aggregation, later on we can switch them on or off. To define a module, you can simply do: aggr . module ( name , starts = pStart , ends = pEnd , depends = { pEnd : pStart }) Later on, when we switch this module on: aggr.on('name') , then aggr.pStart will be added to aggr.starts , aggr.pEnds will be added to aggr.ends , and aggr.pEnd will be set to depend on aggr.pStart . When we switch it off, then aggr.pStart will be removed from aggr.starts , aggr.pEnds will be removed from aggr.ends , and aggr.pStart will be removed from aggr.pStart 's dependents. If you want to keep some processes from being removed when the module is switched off, as they are used by other modules, you may do: aggr . module ( name , starts = pStart , ends = pEnd , depends = { pEnd : pStart }, ends_shared = { pEnd : othermod } ) Then when the module is switched off, aggr.pEnd will be kept. If you have something else to be done when a module is switched on/off, you may use moduleFunc to define them: def name_on ( a ): a . addStart ( a . pStart ) a . pEnd . depends = a . pStart a . addEnd ( a . pEnd ) # more stuff go here def name_off ( a ): a . delStart ( a . pStart ) a . pEnd . depends = [] a . delEnd ( a . pEnd ) # more stuff go here aggr . moduleFunc ( name , on , off ) Hint You may use aggr.on() to switch all modules on and aggr.off() to switch all modules off. Set an aggregation as start aggregation for a pipeline You can do it just like setting a process as the starting process of pipeline (see here ). Actually the starting processes in the aggregation ( aggr.starts ) will be set as the starting processes of the pipeline. The dependency of aggregations and processes An aggregation can depend on aggregations and/or processes, you just treat the aggregations as processes. A process can also depend on aggregations and/or processes. What am I? Whom I am depending on? Real relations Aggr ( a1 ) Aggr ( a2 ) a1.starts depends on a2.ends Proc ( p ) Aggr ( a ) p depends on a.ends Note You have to specify depends for start processes of an aggregation. Copy an aggregation Aggr.copy(tag = 'notag', depends = True, id = None, delegates = True, modules = True) You may copy an aggregation, all the processes in the aggregation will be copied, and the dependencies will be switched to the corresponding copied processes, as well as the starting and ending processes, if copyDeps == True . You can keep the ids of processes unchanged but give a new tag and also give the aggregation an new id instead of the variable name: a = Aggr ( p1 , p2 , p3 ) # access the processes: # a.p1, a.p2, a.p3 a2 = a . copy ( copied ) # a2.procs == [ # proc with id p1 and tag copied , # proc with id p2 and tag copied , # proc with id p3 and tag copied , # ] # a2.id == a2 # to access the processes: # a2.p1, a2.p2, a2.p3 a2 = a . copy ( copied , id = newAggr ) # a2.id == newAggr","title":"Aggregations"},{"location":"aggregations/#aggregations","text":"Imagine that you have a set of processes predefined, and every time when you deal with similar problems (i.e. format a file and plot the data or some next generation sequencing data analysis), you will consistently use those processes, then you have to configure and call them every time. Aggregations are designed for this kind of situations, you can just define an aggregations with those processes, and adjust the dependencies, input and arguments, you will be able to re-use the aggregation with very less configuration. For example: pTrimmomaticPE . input = input channel pAlignPEByBWA . depends = pTrimmomaticPE pSortSam . depends = pAlignPEByBWA pMarkDuplicates . depends = pSortSam pIndexBam . depends = pMarkDuplicates pRealignerTargetCreator . depends = pIndexBam pIndelRealigner . depends = pIndexBam , pRealignerTargetCreator pBaseRecalibrator . depends = pIndelRealigner pPrintReads . depends = pIndelRealigner , pBaseRecalibrator pPrintReads . exportdir = exdir pMarkDuplicates . args . params . tmpdir = /local2/tmp/ pAlignPEByBWA . args . reffile = reffile pRealignerTargetCreator . args . reffile = reffile pRealignerTargetCreator . args . params . tmpdir = /local2/tmp/ pIndelRealigner . args . reffile = reffile pIndelRealigner . args . params . tmpdir = /local2/tmp/ pBaseRecalibrator . args . reffile = reffile pBaseRecalibrator . args . knownSites = dbsnp pBaseRecalibrator . args . params . tmpdir = /local2/tmp/ pPrintReads . args . reffile = reffile pPrintReads . args . params = /local2/tmp/ PyPPL ({ proc : { forks : 100 , runner : sge , sgeRunner : { sge.q : lg-mem } } }) . start ( pTrimmomaticPE ) . run () This is a very commonly used Whole Genome Sequencing data cleanup pipeline from the raw reads according to the GATK best practice . And it will be used pretty much every time when the raw read files come. With an aggregation defined, you don't need to configure and call those processes every time: from pyppl import Aggr from bioprocs import params # some paramters defined in params aFastqPE2Bam = Aggr ( pTrimmomaticPE , pAlignPEByBWA , pSortSam , pMarkDuplicates , pIndexBam , pRealignerTargetCreator , pIndelRealigner , pBaseRecalibrator , pPrintReads ) # dependency adjustment aFastqPE2Bam . pIndelRealigner . depends = aFastqPE2Bam . pIndexBam , aFastqPE2Bam . pRealignerTargetCreator aFastqPE2Bam . pPrintReads . depends = aFastqPE2Bam . pIndelRealigner , aFastqPE2Bam . pBaseRecalibrator # input adjustment # args adjustment aFastqPE2Bam . pMarkDuplicates . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pAlignPEByBWA . args . reffile = params . hg19fa aFastqPE2Bam . pRealignerTargetCreator . args . reffile = params . hg19fa aFastqPE2Bam . pRealignerTargetCreator . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pIndelRealigner . args . reffile = params . hg19fa aFastqPE2Bam . pIndelRealigner . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pBaseRecalibrator . args . reffile = params . hg19fa aFastqPE2Bam . pBaseRecalibrator . args . knownSites = params . dbsnp aFastqPE2Bam . pBaseRecalibrator . args . params . tmpdir = params . tmpdir aFastqPE2Bam . pPrintReads . args . reffile = params . hg19fa aFastqPE2Bam . pPrintReads . args . params . tmpdir = params . tmpdir Then every time you just need to call the aggregation: aFastqPE2Bam . input = channel . fromPairs ( datadir + /*.fastq.gz ) aFastqPE2Bam . exdir = exdir PyPPL ({ proc : { sgeRunner : { sge.q : 1-day } } }) . start ( aFastqPE2Bam ) . run ()","title":"Aggregations"},{"location":"aggregations/#initialize-an-aggregation","text":"Like previous example shows, you just need to give the constructor all the processes to construct an aggretation. However, there are several things need to be noticed: The dependencies are automatically constructed by the order of the processes. a = Aggr ( p1 , p2 , p3 ) # The dependencies will be p1 - p2 - p3 The starting and ending processes are defined as the first and last processes, respectively. If you need to modify the dependencies, keep that in mind whether the starting and ending processes are changed. a = Aggr ( p1 , p2 , p3 ) # a.starts == [p1] # a.ends == [p3] # / p2 # change the dependencies to p1 # \\ p3 # both p2, p3 depend on p1, and p3 depends on p2 a . p3 . depends = p1 , p2 # but remember the ending processes are changed from [p3] to [p2, p3] a . ends = [ p2 , p3 ] You can also specify the dependencies manually: a = Aggr ( p1 , p2 , p3 , depends = False ) a . p2 . depends = p1 a . p3 . depends = p1 , p2 a . starts = [ p1 ] a . ends = [ p2 , p3 ] If you have one process used twice in the aggregation, copy it with a different id: a = Aggr ( p1 , p2 , p1 . copy ( id = p1copy ) ) # then to access the 2nd p1: a.p1copy Each process is copied by aggregation, so the original one can still be used. The tag of each process is regenerated by the id of the aggregation.","title":"Initialize an aggregation"},{"location":"aggregations/#add-a-process-on-the-run","text":"aggr.addProc(p, where=None) You can add a process, and also define whether to put it in starts , ends , both or None .","title":"Add a process on the run"},{"location":"aggregations/#delegate-attributes-of-processes-to-an-aggregation","text":"You can Delegate the attributes directly for a process to an aggregation: aFastqPE2Bam . delegate ( args.reffile , pAlignPEByBWA ) Then when you want to set args.reffile for pAlignPEByBWA , you can just do: aFastqPE2Bam . args . reffile = /path/to/hg19.fa You may use starts/ends represents the start/end processes. Delegate an attribute to multiple processes: aFastqPE2Bam . delegate ( args.reffile , pAlignPEByBWA, pPrintReads ) # or aFastqPE2Bam . delegate ( args.reffile , [ pAlignPEByBWA , pPrintReads ]) # or aFastqPE2Bam . delegate ( args.reffile , [ aFastqPE2Bam . pAlignPEByBWA , aFastqPE2Bam . pPrintReads ]) Delegate multiple attributes at one time: aFastqPE2Bam . delegate ( args.reffile, args.tmpdir , pAlignPEByBWA, pPrintReads ) # or aFastqPE2Bam . delegate ([ args.reffile , args.tmpdir ], pAlignPEByBWA, pPrintReads ) Caution Undelegated attributes will be delegated to all processes. But remember if an attribute has been set before, say p.runner = 'local' then, aggr.runner will not overwrite it if runner is not delegated. If it is, then it'll be overwritten. Note For attributes that have sub-attributes (i.e. p.args.params.inopts.cnames ), you may just delegate the first parts, then the full assignment of the attribute will still follow the delegation. For example: aggr . delegate ( args.params , p1,p2 ) aggr . args . params . inopts . cnames = True # only affects p1, p2 Keep in mind that shorter delegations always come first. In the above case, if we have another delegation: aggr.delegate('args.params.inopts', 'p3') , then the assignment will still affect p1, p2 (the first delegation) and p3 (the second delegation).","title":"Delegate attributes of processes to an aggregation"},{"location":"aggregations/#default-delegations","text":"By default, input/depends are delegated for start processes, and exdir/exhow/exow/expart for end processes. Importantly, as aggr.starts is a list, the values for input/depends must be a list as well, with elements corresponing to each start process. Besides, we have two special attributes for aggregations: input2 and depends2 . Unlike input and depends , input2 and depends2 try to pass everything it gets to each process, instead of passing corresponind element to each process. For example: # aggr.starts = [aggr.p1, aggr.p2] aggr . input = [[ a ], [ b ]] # then: # aggr.p1.config[ input ] == [ a ] # aggr.p2.config[ input ] == [ b ] aggr . input2 = [[ a ], [ b ]] # then: # aggr.p1.config[ input ] == [[ a ], [ b ]] # aggr.p2.config[ input ] == [[ a ], [ b ]]","title":"Default delegations"},{"location":"aggregations/#set-attribute-value-for-specific-processes-of-an-aggregation","text":"There are several ways to do that: # refer to the process directly aFastqPE2Bam . pPrintReads . args . tmpdir = /tmp aFastqPE2Bam . pPrintReads . runner = local # refer to the index of the process aFastqPE2Bam [ 8 ] . args . tmpdir = /tmp aFastqPE2Bam [ 8 ] . runner = local # refer to the name of the process aFastqPE2Bam [ pPrintReads ] . args . tmpdir = /tmp aFastqPE2Bam [ pPrintReads ] . runner = local # for multiple processes aFastqPE2Bam [: 3 ] . args . tmpdir = /tmp aFastqPE2Bam [ 0 , 1 , 3 ] . args . tmpdir = /tmp aFastqPE2Bam [ aFastqPE2Bam , pPrintReads ] . args . tmpdir = /tmp aFastqPE2Bam [ aFastqPE2Bam, pPrintReads ] . args . tmpdir = /tmp # or you may use starts/ends to refer to the start/end processes # has to be done after aggr.starts/aggr.ends assigned # or initialized with depends = True aFastqPE2Bam [ starts ] . args . tmpdir = /tmp aFastqPE2Bam [ ends ] . args . tmpdir = /tmp Hint If an attribute is delegated for other processes, you can still set the value of it by the above methods. Note When use __getitem__ to select processes from aggregations, only the index or the id will return the processes itself (instance of Proc ), otherwise it will return an instance _Proxy , which is proxy used to pass attribute values to a set of processes. So pay attention to it, because we can do aggr['pXXX1, pXXX2'].depends = \"pXXX3, pXXX4\" to aggr.pXXX3 and aggr.pXXX4 as the dependent of aggr.pXXX1 and aggr.pXXX2 respectively, but if you do aggr['pXXX1'] = 'pXXX3' will raise an error. Because _Proxy helps the aggregation to select the processes from itselt, but a Proc instance doesn't know how to. You may turn it into _Proxy from Proc instance simply by add a comma to the selector: isinstance ( aggr [ 0 ], Proc ) isinstance ( aggr [ 0 ,], _Proxy ) isinstance ( aggr [ pXXX1 ], Proc ) isinstance ( aggr [ pXXX1, ], _Proxy ) # or isinstance ( aggr [ pXXX1 ,], _Proxy )","title":"Set attribute value for specific processes of an aggregation"},{"location":"aggregations/#define-modules-of-an-aggregation","text":"We can define some modules for an aggregation, later on we can switch them on or off. To define a module, you can simply do: aggr . module ( name , starts = pStart , ends = pEnd , depends = { pEnd : pStart }) Later on, when we switch this module on: aggr.on('name') , then aggr.pStart will be added to aggr.starts , aggr.pEnds will be added to aggr.ends , and aggr.pEnd will be set to depend on aggr.pStart . When we switch it off, then aggr.pStart will be removed from aggr.starts , aggr.pEnds will be removed from aggr.ends , and aggr.pStart will be removed from aggr.pStart 's dependents. If you want to keep some processes from being removed when the module is switched off, as they are used by other modules, you may do: aggr . module ( name , starts = pStart , ends = pEnd , depends = { pEnd : pStart }, ends_shared = { pEnd : othermod } ) Then when the module is switched off, aggr.pEnd will be kept. If you have something else to be done when a module is switched on/off, you may use moduleFunc to define them: def name_on ( a ): a . addStart ( a . pStart ) a . pEnd . depends = a . pStart a . addEnd ( a . pEnd ) # more stuff go here def name_off ( a ): a . delStart ( a . pStart ) a . pEnd . depends = [] a . delEnd ( a . pEnd ) # more stuff go here aggr . moduleFunc ( name , on , off ) Hint You may use aggr.on() to switch all modules on and aggr.off() to switch all modules off.","title":"Define modules of an aggregation."},{"location":"aggregations/#set-an-aggregation-as-start-aggregation-for-a-pipeline","text":"You can do it just like setting a process as the starting process of pipeline (see here ). Actually the starting processes in the aggregation ( aggr.starts ) will be set as the starting processes of the pipeline.","title":"Set an aggregation as start aggregation for a pipeline"},{"location":"aggregations/#the-dependency-of-aggregations-and-processes","text":"An aggregation can depend on aggregations and/or processes, you just treat the aggregations as processes. A process can also depend on aggregations and/or processes. What am I? Whom I am depending on? Real relations Aggr ( a1 ) Aggr ( a2 ) a1.starts depends on a2.ends Proc ( p ) Aggr ( a ) p depends on a.ends Note You have to specify depends for start processes of an aggregation.","title":"The dependency of aggregations and processes"},{"location":"aggregations/#copy-an-aggregation","text":"Aggr.copy(tag = 'notag', depends = True, id = None, delegates = True, modules = True) You may copy an aggregation, all the processes in the aggregation will be copied, and the dependencies will be switched to the corresponding copied processes, as well as the starting and ending processes, if copyDeps == True . You can keep the ids of processes unchanged but give a new tag and also give the aggregation an new id instead of the variable name: a = Aggr ( p1 , p2 , p3 ) # access the processes: # a.p1, a.p2, a.p3 a2 = a . copy ( copied ) # a2.procs == [ # proc with id p1 and tag copied , # proc with id p2 and tag copied , # proc with id p3 and tag copied , # ] # a2.id == a2 # to access the processes: # a2.p1, a2.p2, a2.p3 a2 = a . copy ( copied , id = newAggr ) # a2.id == newAggr","title":"Copy an aggregation"},{"location":"api/","text":"API Module PyPPL The PyPPL class @static variables: `TIPS`: The tips for users `RUNNERS`: Registered runners `DEFAULT_CFGFILES`: Default configuration file __init__ (self, config, cfgfile) Constructor params: config : the configurations for the pipeline, default: {} cfgfile : the configuration file for the pipeline, default: ~/.PyPPL.json or ./.PyPPL _any2procs (*args) [@staticmethod] Get procs from anything (aggr.starts, proc, procs, proc names) params: arg : anything returns: A set of procs _checkProc (proc) [@staticmethod] Check processes, whether 2 processes have the same id and tag params: proc : The process returns: If there are 2 processes with the same id and tag, raise ValueError . _registerProc (proc) [@staticmethod] Register the process params: proc : The process _resume (self, *args, **kwargs) Mark processes as to be resumed params: args : the processes to be marked. The last element is the mark for processes to be skipped. flowchart (self, fcfile, dotfile) Generate graph in dot language and visualize it. params: dotfile : Where to same the dot graph. Default: None ( path.splitext(sys.argv[0])[0] + \".pyppl.dot\" ) fcfile : The flowchart file. Default: None ( path.splitext(sys.argv[0])[0] + \".pyppl.svg\" ) For example: run python pipeline.py will save it to pipeline.pyppl.svg dot : The dot visulizer. Default: \"dot -Tsvg {{dotfile}} {{fcfile}}\" returns: The pipeline object itself. registerRunner (runner) [@staticmethod] Register a runner params: runner : The runner to be registered. resume (self, *args) Mark processes as to be resumed params: args : the processes to be marked returns: The pipeline object itself. resume2 (self, *args) Mark processes as to be resumed params: args : the processes to be marked returns: The pipeline object itself. run (self, profile) Run the pipeline params: profile : the profile used to run, if not found, it'll be used as runner name. default: 'default' returns: The pipeline object itself. showAllRoutes (self) start (self, *args) Set the starting processes of the pipeline params: args : the starting processes returns: The pipeline object itself. Module Proc The Proc class defining a process @static variables: `RUNNERS`: The regiested runners `ALIAS`: The alias for the properties `LOG_NLINE`: The limit of lines of logging information of same type of messages @magic methods: `__getattr__(self, name)`: get the value of a property in `self.props` `__setattr__(self, name, value)`: set the value of a property in `self.config` __init__ (self, tag, desc, id, **kwargs) Constructor params: tag : The tag of the process desc : The description of the process id : The identify of the process config: id, input, output, ppldir, forks, cache, cclean, rc, echo, runner, script, depends, tag, desc, dirsig exdir, exhow, exow, errhow, errntry, lang, beforeCmd, afterCmd, workdir, args, aggr callfront, callback, expect, expart, template, tplenvs, resume, nthread props input, output, rc, echo, script, depends, beforeCmd, afterCmd, workdir, expect expart, template, channel, jobs, ncjobids, size, sets, procvars, suffix, logs _buildInput (self) Build the input data Input could be: 1. list: ['input', 'infile:file'] = ['input:var', 'infile:path'] 2. str : \"input, infile:file\" = input:var, infile:path 3. dict: {\"input\": channel1, \"infile:file\": channel2} or {\"input:var, input:file\" : channel3} for 1,2 channels will be the combined channel from dependents, if there is not dependents, it will be sys.argv[1:] _buildJobs (self) Build the jobs. _buildOutput (self) Build the output data templates waiting to be rendered. _buildProcVars (self) Build proc attribute values for template rendering, and also echo some out. _buildProps (self) Compute some properties _buildScript (self) Build the script template waiting to be rendered. _checkCached (self) Tell whether the jobs are cached returns: True if all jobs are cached, otherwise False _readConfig (self, profile, profiles) Read the configuration params: config : The configuration _runCmd (self, key) Run the beforeCmd or afterCmd params: key : \"beforeCmd\" or \"afterCmd\" returns: The return code of the command _runJobs (self) Submit and run the jobs _saveSettings (self) Save all settings in proc.settings, mostly for debug _suffix (self) Calcuate a uid for the process according to the configuration The philosophy: 1. procs from different script must have different suffix (sys.argv[0]) 2. procs from the same script: - procs with different id or tag have different suffix - procs with different input have different suffix (depends, input) returns: The uniq id of the process _tidyAfterRun (self) Do some cleaning after running jobs self.resume can only be: - '': normal process - skip+: skipped process but required workdir and data exists - resume: resume pipeline from this process, no requirement - resume+: get data from workdir/proc.settings, and resume _tidyBeforeRun (self) Do some preparation before running jobs copy (self, tag, desc, id) Copy a process params: id : The new id of the process, default: None (use the varname) tag : The tag of the new process, default: None (used the old one) desc : The desc of the new process, default: None (used the old one) returns: The new process log (self, msg, level, key) The log function with aggregation name, process id and tag integrated. params: msg : The message to log level : The log level key : The type of messages name (self, aggr) Get my name include aggr , id , tag returns: the name run (self, profile, profiles) Run the jobs with a configuration params: config : The configuration Module Channel The channen class, extended from list _tuplize (tu) [@staticmethod] A private method, try to convert an element to tuple If it's a string, convert it to (tu, ) Else if it is iterable, convert it to tuple(tu) Otherwise, convert it to (tu, ) Notice that string is also iterable. params: tu : the element to be converted returns: The converted element attach (self, *names, **kwargs) Attach columns to names of Channel, so we can access each column by: ch.col0 == ch.colAt(0) params: names : The names. Have to be as length as channel's width. None of them should be Channel's property name flatten : Whether flatten the channel for the name being attached cbind (self, *cols) Add columns to the channel params: cols : The columns returns: The channel with the columns inserted. colAt (self, index) Fetch one column of a Channel params: index : which column to fetch returns: The Channel with that column collapse (self, col) Do the reverse of expand length: N - 1 width: M - M params: col : the index of the column used to collapse returns: The collapsed Channel copy (self) Copy a Channel using copy.copy returns: The copied Channel create (l) [@staticmethod] Create a Channel from a list params: l : The list, default: [] returns: The Channel created from the list expand (self, col, pattern, t, sortby, reverse) expand the Channel according to the files in , other cols will keep the same [(dir1/dir2, 1)].expand (0, \"*\") will expand to [(dir1/dir2/file1, 1), (dir1/dir2/file2, 1), ...] length: 1 - N width: M - M params: col : the index of the column used to expand pattern : use a pattern to filter the files/dirs, default: * t : the type of the files/dirs to include 'dir', 'file', 'link' or 'any' (default) sortby : how the list is sorted 'name' (default), 'mtime', 'size' reverse : reverse sort. Default: False returns: The expanded Channel filter (self, func) Alias of python builtin filter params: func : the function. Default: None returns: The filtered Channel filterCol (self, func, col) Just filter on the first column params: func : the function col : the column to filter returns: The filtered Channel flatten (self, col) Convert a single-column Channel to a list (remove the tuple signs) [(a,), (b,)] to [a, b] params: col : The column to flat. None for all columns (default) returns: The list converted from the Channel. fold (self, n) Fold a Channel. Make a row to n-length chunk rows a1 a2 a3 a4 b1 b2 b3 b4 if n==2, fold(2) will change it to: a1 a2 a3 a4 b1 b2 b3 b4 params: n : the size of the chunk returns The new Channel fromArgv () [@staticmethod] Create a Channel from sys.argv[1:] \"python test.py a b c\" creates a width=1 Channel \"python test.py a,1 b,2 c,3\" creates a width=2 Channel returns: The Channel created from the command line arguments fromChannels (*args) [@staticmethod] Create a Channel from Channels params: args : The Channels returns: The Channel merged from other Channels fromFile (fn, header, skip, delimit) [@staticmethod] Create Channel from the file content It's like a matrix file, each row is a row for a Channel. And each column is a column for a Channel. params: fn : the file header : Whether the file contains header. If True, will attach the header So you can use channel. header to fetch the column skip : first lines to skip delimit : the delimit for columns returns: A Channel created from the file fromPairs (pattern) [@staticmethod] Create a width = 2 Channel from a pattern params: pattern : the pattern returns: The Channel create from every 2 files match the pattern fromParams (*pnames) [@staticmethod] Create a Channel from params params: *pnames : The names of the option returns: The Channel fromPattern (pattern, t, sortby, reverse) [@staticmethod] Create a Channel from a path pattern params: pattern : the pattern with wild cards t : the type of the files/dirs to include 'dir', 'file', 'link' or 'any' (default) sortby : how the list is sorted 'name' (default), 'mtime', 'size' reverse : reverse sort. Default: False returns: The Channel created from the path get (self, idx) Get the element of a flattened channel params: idx : The index of the element to get. Default: 0 return: The element insert (self, cidx, *cols) Insert columns to a channel params: cidx : Insert into which index of column? cols : the columns to be bound to Channel returns: The combined Channel Note, self is also changed length (self) Get the length of a Channel It's just an alias of len(chan) returns: The length of the Channel map (self, func) Alias of python builtin map params: func : the function returns: The transformed Channel mapCol (self, func, col) Map for a column params: func : the function col : the index of the column. Default: 0 returns: The transformed Channel nones (length, width) [@staticmethod] Create a channel with None s params: length : The length of the channel width : The width of the channel returns: The created channel rbind (self, *rows) The multiple-argument versoin of rbind params: rows : the rows to be bound to Channel returns: The combined Channel Note, self is also changed reduce (self, func) Alias of python builtin reduce params: func : the function returns: The reduced value reduceCol (self, func, col) Reduce a column params: func : the function col : the column to reduce returns: The reduced value repCol (self, n) Repeat column and return a new channel params: n : how many times to repeat. returns: The new channel with repeated columns repRow (self, n) Repeat row and return a new channel params: n : how many times to repeat. returns: The new channel with repeated rows rowAt (self, index) Fetch one row of a Channel params: index : which row to fetch returns: The Channel with that row slice (self, start, length) Fetch some columns of a Channel params: start : from column to start length : how many columns to fetch, default: None (from start to the end) returns: The Channel with fetched columns split (self, flatten) Split a Channel to single-column Channels returns: The list of single-column Channels t (self) Transpose a channel transpose (self) Transpose a channel unfold (self, n) Do the reverse thing as self.fold does params: n : How many rows to combind each time. default: 2 returns: The unfolded Channel unique (self) Make the channel unique, remove duplicated rows Try to keep the order width (self) Get the width of a Channel returns: The width of the Channel Module Job Job class, defining a job in a process __init__ (self, index, proc) Constructor params: index : The index of the job in a process proc : The process _indexIndicator (self) Get the index indicator in the log returns: The \"[001/100]\" like indicator _linkInfile (self, orgfile) Create links for input files params: orgfile : The original input file returns: The link to the original file. _prepInput (self) Prepare input, create link to input files and set other placeholders _prepOutput (self) Build the output data. Output could be: 1. list: ['output:var:{{input}}', 'outfile:file:{{infile.bn}}.txt'] or you can ignore the name if you don't put it in script: ['var:{{input}}', 'path:{{infile.bn}}.txt'] or even (only var type can be ignored): ['{{input}}', 'file:{{infile.bn}}.txt'] 2. str : 'output:var:{{input}}, outfile:file:{{infile.bn}}.txt' 3. OrderedDict: {\"output:var:{{input}}\": channel1, \"outfile:file:{{infile.bn}}.txt\": channel2} or {\"output:var:{{input}}, output:file:{{infile.bn}}.txt\" : channel3} for 1,2 channels will be the property channel for this proc (i.e. p.channel) _prepScript (self) Build the script, interpret the placeholders _reportItem (self, key, maxlen, data, loglevel) Report the item on logs params: key : The key of the item maxlen : The max length of the key data : The data of the item loglevel : The log level cache (self) Truly cache the job (by signature) checkOutfiles (self, expect) Check whether output files are generated, if not, add - to rc. done (self) Do some cleanup when job finished export (self) Export the output files init (self) Initiate a job, make directory and prepare input, output and script. isExptCached (self) Prepare to use export files as cached information True if succeed, otherwise False isTrulyCached (self) Check whether a job is truly cached (by signature) pid (self, val) Get/Set the job id (pid or the id from queue system) params: val : The id to be set rc (self, val) Get/Set the return code params: val : The return code to be set. If it is None, return the return code. Default: None If val == -1000: the return code will be negative of current one. 0 will be '-0' returns: The return code if val is None If rcfile does not exist or is empty, return 9999, otherwise return -rc A negative rc (including -0) means output files not generated report (self) Report the job information to logger reset (self, retry) Clear the intermediate files and output files showError (self, totalfailed) Show the error message if the job failed. signature (self) Calculate the signature of the job based on the input/output and the script returns: The signature of the job succeed (self) Tell if the job is successful by return code, and output file expectations. returns: True if succeed else False Module Jobmgr Job Manager __init__ (self, proc, runner) Job manager constructor params: proc : The process runner : The runner class _exit (self) allJobsDone (self) Tell whether all jobs are done. No need to lock as it only runs in one process (the watcher process) returns: True if all jobs are done else False canSubmit (self) Tell whether we can submit jobs. returns: True if we can, otherwise False halt (self, halt_anyway) Halt the pipeline if needed progressbar (self, jid, loglevel) run (self) Start to run the jobs runPool (self, rq, sq) The pool to run jobs (wait jobs to be done) params: rq : The run queue sq : The submit queue submitPool (self, sq) The pool to submit jobs params: sq : The submit queue watchPool (self, rq, sq) The watchdog, checking whether all jobs are done. Module Aggr The aggregation of a set of processes @magic methods: `__setattr__(self, name, value)`: Set property value of an aggregation. - if it's a common property, set it to all processes - if it is `input` set it to starting processes - if it is `depends` set it to the end processes - if it is related to `export` (startswith `ex`), set it to the end processes - if it is in ['starts', 'ends', 'id'], set it to the aggregation itself. - Otherwise a `ValueError` raised. - You can use `[aggr].[proc].[prop]` to set/get the properties of a processes in the aggregation. __init__ (self, *args, **kwargs) Constructor params: args : the set of processes depends : Whether auto deduce depends. Default: True id : The id of the aggr. Default: None (the variable name) tag : The tag of the processes. Default: None (a unique 4-char str according to the id) _select (self, key, forceList, flatten) Select processes # self._procs = OrderedDict([ # ( a , Proc(id = a )), # ( b , Proc(id = b )), # ( c , Proc(id = c )), # ( d , Proc(id = d )) # ]) self[ a ] # proc a self[0] # proc a self[1:2] # _Proxy of (proc b, proc c) self[1,3] # _Proxy of (proc b, proc d) self[ b , c ] # _Proxy of (proc b, proc c) self[ b,c ] # _Proxy of (proc b, proc c) self[Proc(id = d )] # proc d #### `addEnd (self, *procs) ` #### `addProc (self, p, tag, where, copy) ` Add a process to the aggregation. Note that you have to adjust the dependencies after you add processes. - **params:** `p`: The process `where`: Add to where: starts , ends , both or None (default) - **returns:** the aggregation itself #### `addStart (self, *procs) ` #### `copy (self, tag, depends, id, delegates, modules) ` Like `proc` s `copy` function, copy an aggregation. Each processes will be copied. - **params:** `tag`: The new tag of all copied processes `depends`: Whether to copy the dependencies or not. Default: True - dependences for processes in starts will not be copied `id`: Use a different id if you don t want to use the variant name `delegates`: Copy delegates? Default: `True` `configs`: Copy configs? Default: `True` - **returns:** The new aggregation #### `delEnd (self, *procs) ` #### `delStart (self, *procs) ` #### `delegate (self, attrs, procs) ` Delegate the procs to have the attributes set by: `aggr.args.a.b = 1` Instead of setting `args.a.b` of all processes, `args.a.b` of only delegated processes will be set. `procs` can be `starts`/`ends`, but it cannot be set with other procs, which means you can do: `aggr.delegate( args , starts )`, but not `aggr.delegate( args , [ starts , pXXX ])` #### `module (self, name, starts, depends, ends, starts_shared, depends_shared, ends_shared) ` Define a function for aggr. The shared parameters will be indicators not to remove those processes when the shared function is on. - **params:** `name` : The name of the function `starts` : A list of start processes. `depends` : A dict of dependences of the procs `ends` : A list of end processes `starts_shared` : A dict of functions that shares the same starts `depends_shared`: A dict of functions that shares the same depends `ends_shared` : A dict of functions that shares the same ends - For example: `{ procs : func }` #### `moduleFunc (self, name, on, off) ` #### `off (self, *names) ` #### `on (self, *names) ` ## Module `flowchart.Flowchart` Draw flowchart for pipelines @static variables: `THEMES`: predefined themes #### `__init__ (self, fcfile, dotfile) ` The constructor - **params:** `fcfile`: The flowchart file. Default: `path.splitext(sys.argv[0])[0] + .pyppl.svg ` `dotfile`: The dot file. Default: `path.splitext(sys.argv[0])[0] + .pyppl.dot ` #### `_assemble (self) ` Assemble the graph for printing and rendering #### `addLink (self, node1, node2) ` Add a link to the chart - **params:** `node1`: The first node. `node2`: The second node. #### `addNode (self, node, role) ` Add a node to the chart - **params:** `node`: The node `role`: Is it a starting node, an ending node or None. Default: None. #### `generate (self) ` Generate the dot file and graph file. #### `setTheme (self, theme, base) ` Set the theme to be used - **params:** `theme`: The theme, could be the key of Flowchart.THEMES or a dict of a theme definition. `base` : The base theme to be based on you pass custom theme ## Module `parameters.Parameter` The class for a single parameter #### `__init__ (self, name, value) ` Constructor - **params:** `name`: The name of the parameter `value`: The initial value of the parameter #### `_forceType (self) ` Coerce the value to the type specified TypeError will be raised if error happens #### `setDesc (self, d) ` Set the description of the parameter - **params:** `d`: The description #### `setName (self, n) ` Set the name of the parameter - **params:** `n`: The name #### `setRequired (self, r) ` Set whether this parameter is required - **params:** `r`: True if required else False. Default: True #### `setShow (self, s) ` Set whether this parameter should be shown in help information - **params:** `s`: True if it shows else False. Default: True #### `setType (self, t) ` Set the type of the parameter - **params:** `t`: The type of the value. Default: str - Note: str rather then str #### `setValue (self, v) ` Set the value of the parameter - **params:** `v`: The value ## Module `parameters.Parameters` A set of parameters #### `__init__ (self, command, theme) ` Constructor #### `_coerceValue (value, t) [@staticmethod]` #### `_getType (self, argname, argtype) ` #### `_parseName (self, argname) ` If `argname` is the name of an option - **params:** `argname`: The argname - **returns:** `an`: clean argument name `at`: normalized argument type `av`: the argument value, if `argname` is like: `-a=1` #### `_putValue (self, argname, argtype, argval) ` #### `_setDesc (self, desc) ` #### `_setHbald (self, hbald) ` #### `_setHopts (self, hopts) ` #### `_setPrefix (self, prefix) ` #### `_setTheme (self, theme) ` #### `_setUsage (self, usage) ` #### `_shouldPrintHelp (self, args) ` #### `asDict (self) ` Convert the parameters to Box object - **returns:** The Box object #### `help (self, error, printNexit) ` Calculate the help page - **params:** `error`: The error message to show before the help information. Default: ` ` `printNexit`: Print the help page and exit the program? Default: `False` (return the help information) - **return:** The help information #### `loadDict (self, dictVar, show) ` Load parameters from a dict - **params:** `dictVar`: The dict variable. - Properties are set by param .required , param .show , ... `show`: Whether these parameters should be shown in help information - Default: False (don t show parameter from config object in help page) - It ll be overwritten by the `show` property inside dict variable. - If it is None, will inherit the param s show value #### `loadFile (self, cfgfile, show) ` Load parameters from a json/config file If the file name ends with .json , `json.load` will be used, otherwise, `ConfigParser` will be used. For config file other than json, a section name is needed, whatever it is. - **params:** `cfgfile`: The config file `show`: Whether these parameters should be shown in help information - Default: False (don t show parameter from config file in help page) - It ll be overwritten by the `show` property inside the config file. #### `parse (self, args) ` Parse the arguments from `sys.argv` ## Module `logger` A customized logger for pyppl #### `class: Box` Allow dot operation for OrderedDict #### `class: LoggerThemeError` Theme errors for logger #### `class: PyPPLLogFilter` logging filter by levels (flags) #### `class: PyPPLLogFormatter` logging formatter for pyppl #### `class: PyPPLStreamHandler` #### `class: TemplatePyPPL` Built-in template wrapper. #### `Value (typecode_or_type, *args, **kwds) [@staticmethod]` Returns a synchronized shared object #### `_formatTheme (theme) [@staticmethod]` Make them in the standard form with bgcolor and fgcolor in raw terminal color strings If the theme is read from file, try to translate COLORS.xxx to terminal color strings - **params:** `theme`: The theme - **returns:** The formatted colors #### `_getColorFromTheme (level, theme) [@staticmethod]` Get colors from a them - **params:** `level`: Our own log record level `theme`: The theme - **returns:** The colors #### `_getLevel (record) [@staticmethod]` Get the flags of a record - **params:** `record`: The logging record #### `getLogger (levels, theme, logfile, lvldiff, pbar, name) [@staticmethod]` Get the default logger - **params:** `levels`: The log levels(tags), default: basic `theme`: The theme of the logs on terminal. Default: True (default theme will be used) - False to disable theme `logfile`:The log file. Default: None (don t white to log file) `lvldiff`:The diff levels for log - [ -depends , jobdone , +debug ]: show jobdone, hide depends and debug `name`: The name of the logger, default: PyPPL - **returns:** The logger ## Module `utils` A set of utitities for PyPPL #### `class: Box` Allow dot operation for OrderedDict #### `alwaysList (data) ` Convert a string or a list with element - **params:** `data`: the data to be converted - **examples:** ```python data = [ a, b, c , d ] ret = alwaysList (data) # ret == [ a , b , c , d ] returns: The split list asStr (s, encoding) Convert everything (str, unicode, bytes) to str with python2, python3 compatiblity briefList (l) Briefly show an integer list, combine the continuous numbers. params: l : The list returns: The string to show for the briefed list. dictUpdate (origDict, newDict) Update a dictionary recursively. params: origDict : The original dictionary newDict : The new dictionary examples: od1 = { a : { b : { c : 1 , d : 1 }}} od2 = { key : value for key : value in od1 . items ()} nd = { a : { b : { d : 2 }}} od1 . update ( nd ) # od1 == { a : { b : { d : 2}}}, od1[ a ][ b ] is lost dictUpdate ( od2 , nd ) # od2 == { a : { b : { c : 1, d : 2}}} filter (func, vec) Python2 and Python3 compatible filter params: func : The filter function vec : The list to be filtered returns: The filtered list formatSecs (seconds) Format a time duration params: seconds : the time duration in seconds returns: The formated string. For example: \"01:01:01.001\" stands for 1 hour 1 min 1 sec and 1 minisec. funcsig (func) Get the signature of a function Try to get the source first, if failed, try to get its name, otherwise return None params: func : The function returns: The signature map (func, vec) Python2 and Python3 compatible map params: func : The map function vec : The list to be maped returns: The maped list range (i, *args, **kwargs) Convert a range to list, because in python3, range is not a list params: r : the range data returns: The converted list reduce (func, vec) Python2 and Python3 compatible reduce params: func : The reduce function vec : The list to be reduced returns: The reduced value split (s, delimter, trim) Split a string using a single-character delimter params: s : the string delimter : the single-character delimter trim : whether to trim each part. Default: True examples: ret = split ( a,b ,c , , ) # ret == [ a,b , c ] # , inside quotes will be recognized. returns: The list of substrings uid (s, l, alphabet) Calculate a short uid based on a string. Safe enough, tested on 1000000 32-char strings, no repeated uid found. This is used to calcuate a uid for a process params: s : the base string l : the length of the uid alphabet : the charset used to generate the uid returns: The uid varname (maxline, incldot) Get the variable name for ini params: maxline : The max number of lines to retrive. Default: 20 incldot : Whether include dot in the variable name. Default: False returns: The variable name Module utils.box . class: Box Allow dot operation for OrderedDict Module utils.cmd . class: Cmd A command (subprocess) wapper class: Timeout run (cmd, bg, raiseExc, timeout, **kwargs) [@staticmethod] A shortcut of Command.run To chain another command, you can do: run('seq 1 3', bg = True).pipe('grep 1') params: cmd : The command, could be a string or a list bg : Run in background or not. Default: False If it is True , rc and stdout/stderr will be default (no value retrieved). raiseExc : raise the expcetion or not **kwargs : other arguments for Popen returns: The Command instance sleep (cmd, bg, raiseExc, timeout, **kwargs, **kwargs) sleep(seconds) Delay execution for a given number of seconds. The argument may be a floating point number for subsecond precision. time (cmd, bg, raiseExc, timeout, **kwargs, **kwargs, **kwargs) time() - floating point number Return the current time in seconds since the Epoch. Fractions of a second may be present if the system clock provides them. Module utils.parallel . class: Parallel A parallel runner run (func, args, nthread, backend, raiseExc) [@staticmethod] A shortcut of Parallel.run params: func : The function to run args : The arguments for the function, should be a list with tuple s nthread : Number of jobs to run simultaneously. Default: 1 backend : The backend, either process (default) or thread raiseExc : Whether raise exception or not. Default: True returns: The merged results from each job. Module utils.safefs . class: ChmodError OS system call failed. Lock () [@staticmethod] Returns a non-recursive lock object class: SafeFs A thread-safe file system @static variables: `TMPDIR`: The default temporary directory to store lock files # file types `FILETYPE_UNKNOWN` : Unknown file type `FILETYPE_NOENT` : File does not exist `FILETYPE_NOENTLINK`: A dead link (a link links to a non-existent file. `FILETYPE_FILE` : A regular file `FILETYPE_FILELINK` : A link to a regular file `FILETYPE_DIR` : A regular directory `FILETYPE_DIRLINK` : A link to a regular directory # relation of two files `FILES_DIFF_BOTHNOENT` : Two files are different and none of them exists `FILES_DIFF_NOENT1` : Two files are different but file1 does not exists `FILES_DIFF_NOENT2` : Two files are different but file2 does not exists `FILES_DIFF_BOTHENT` : Two files are different and both of them exist `FILES_SAME_STRNOENT` : Two files are the same string and it does not exist `FILES_SAME_STRENT` : Two files are the same string and it exists `FILES_SAME_BOTHLINKS` : Two files link to one file `FILES_SAME_BOTHLINKS1`: File1 links to file2, file2 links to a regular file `FILES_SAME_BOTHLINKS2`: File2 links to file1, file1 links to a regular file `FILES_SAME_REAL1` : File2 links to file1, which a regular file `FILES_SAME_REAL2` : File1 links to file2, which a regular file `LOCK`: A global lock ensures the locks are locked at the same time copy (file1, file2, overwrite, callback) [@staticmethod] A shortcut of SafeFs.copy params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False exists (filepath, callback) [@staticmethod] A shortcut of SafeFs.exists params: filepath : The filepath callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if the file exists else False gz (file1, file2, overwrite, callback) [@staticmethod] A shortcut of SafeFs.gz params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False link (file1, file2, overwrite, callback) [@staticmethod] A shortcut of SafeFs.link params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False moveWithLink (file1, file2, overwrite, callback) [@staticmethod] A shortcut of SafeFs.moveWithLink params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False osremove (file1, file2, overwrite, callback) remove(path) Remove a file (same as unlink(path)). readlink (file1, file2, overwrite, callback) readlink(path) - path Return a string representing the path to which the symbolic link points. shmove (src, dst) [@staticmethod] Recursively move a file or directory to another location. This is similar to the Unix \"mv\" command. If the destination is a directory or a symlink to a directory, the source is moved inside the directory. The destination path must not already exist. If the destination already exists but is not a directory, it may be overwritten depending on os.rename() semantics. If the destination is on our current filesystem, then rename() is used. Otherwise, src is copied to the destination and then removed. A lot more could be done here... A look at a mv.c shows a lot of the issues this implementation glosses over. ungz (file1, file2, overwrite, callback) [@staticmethod] A shortcut of SafeFs.ungz params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False Module proctree.ProcNode The node for processes to manage relations between each other __init__ (self, proc) Constructor params: proc : The Proc instance sameIdTag (self, proc) Check if the process has the same id and tag with me. params: proc : The Proc instance returns: True if it is. False if not. Module proctree.ProcTree . __init__ (self) Constructor, set the status of all ProcNode s check (proc) [@staticmethod] Check whether a process with the same id and tag exists params: proc : The Proc instance checkPath (self, proc) Check whether paths of a process can start from a start process params: proc : The process returns: True if all paths can pass The failed path otherwise getAllPaths (self) getEnds (self) Get the end processes returns: The end processes getNext (proc) [@staticmethod] Get next processes of process params: proc : The Proc instance returns: The processes depend on this process getNextStr (proc) [@staticmethod] Get the names of processes depend on a process params: proc : The Proc instance returns: The names getNextToRun (self) Get the process to run next returns: The process next to run getPaths (self, proc, proc0) Infer the path to a process params: proc : The process proc0 : The original process, because this function runs recursively. returns: p1 - p2 - p3 p4 _/ Paths for p3: [[p4], [p2, p1]] getPathsToStarts (self, proc) Filter the paths with start processes params: proc : The process returns: The filtered path getPrevStr (proc) [@staticmethod] Get the names of processes a process depends on params: proc : The Proc instance returns: The names getStarts (self) Get the start processes returns: The start processes register (proc) [@staticmethod] Register the process params: proc : The Proc instance reset () [@staticmethod] Reset the status of all ProcNode s setStarts (self, starts) Set the start processes params: starts : The start processes unranProcs (self) Module templates.TemplatePyPPL Built-in template wrapper. __init__ (self, source, **envs) Initiate the engine with source and envs params: source : The souce text envs : The env data _render (self, data) Render the template params: data : The data used for rendering returns: The rendered string Module templates.TemplateJinja2 Jinja2 template wrapper __init__ (self, source, **envs) Initiate the engine with source and envs params: source : The souce text envs : The env data _render (self, data) Render the template params: data : The data used for rendering returns: The rendered string Module runners.Runner The base runner class __init__ (self, job) Constructor params: job : The job object _flush (self, fout, ferr, lastout, lasterr, end) Flush stdout/stderr params: fout : The stdout file handler ferr : The stderr file handler lastout : The leftovers of previously readlines of stdout lasterr : The leftovers of previously readlines of stderr end : Whether this is the last time to flush finish (self) getpid (self) Get the job id isRunning (self) Try to tell whether the job is still running. returns: True if yes, otherwise False kill (self) Try to kill the running jobs if I am exiting retry (self) run (self) returns: True: success/fail False: needs retry submit (self) Try to submit the job Module runners.RunnerLocal Constructor @params: job : The job object config : The properties of the process __init__ (self, job) Module runners.RunnerSsh The ssh runner @static variables: `SERVERID`: The incremental number used to calculate which server should be used. - Don't touch unless you know what's going on! __init__ (self, job) Constructor params: job : The job object isServerAlive (server, key) [@staticmethod] Module runners.RunnerSge The sge runner __init__ (self, job) Constructor params: job : The job object config : The properties of the process Module runners.RunnerSlurm The slurm runner __init__ (self, job) Constructor params: job : The job object config : The properties of the process Module runners.RunnerDry The dry runner __init__ (self, job) Constructor params: job : The job object finish (self) Do some cleanup work when jobs finish","title":"API"},{"location":"api/#api","text":"","title":"API"},{"location":"api/#module-pyppl","text":"The PyPPL class @static variables: `TIPS`: The tips for users `RUNNERS`: Registered runners `DEFAULT_CFGFILES`: Default configuration file","title":"Module PyPPL"},{"location":"api/#__init__-self-config-cfgfile","text":"Constructor params: config : the configurations for the pipeline, default: {} cfgfile : the configuration file for the pipeline, default: ~/.PyPPL.json or ./.PyPPL","title":"__init__ (self, config, cfgfile)"},{"location":"api/#_any2procs-args-staticmethod","text":"Get procs from anything (aggr.starts, proc, procs, proc names) params: arg : anything returns: A set of procs","title":"_any2procs (*args) [@staticmethod]"},{"location":"api/#_checkproc-proc-staticmethod","text":"Check processes, whether 2 processes have the same id and tag params: proc : The process returns: If there are 2 processes with the same id and tag, raise ValueError .","title":"_checkProc (proc) [@staticmethod]"},{"location":"api/#_registerproc-proc-staticmethod","text":"Register the process params: proc : The process","title":"_registerProc (proc) [@staticmethod]"},{"location":"api/#_resume-self-args-kwargs","text":"Mark processes as to be resumed params: args : the processes to be marked. The last element is the mark for processes to be skipped.","title":"_resume (self, *args, **kwargs)"},{"location":"api/#flowchart-self-fcfile-dotfile","text":"Generate graph in dot language and visualize it. params: dotfile : Where to same the dot graph. Default: None ( path.splitext(sys.argv[0])[0] + \".pyppl.dot\" ) fcfile : The flowchart file. Default: None ( path.splitext(sys.argv[0])[0] + \".pyppl.svg\" ) For example: run python pipeline.py will save it to pipeline.pyppl.svg dot : The dot visulizer. Default: \"dot -Tsvg {{dotfile}} {{fcfile}}\" returns: The pipeline object itself.","title":"flowchart (self, fcfile, dotfile)"},{"location":"api/#registerrunner-runner-staticmethod","text":"Register a runner params: runner : The runner to be registered.","title":"registerRunner (runner) [@staticmethod]"},{"location":"api/#resume-self-args","text":"Mark processes as to be resumed params: args : the processes to be marked returns: The pipeline object itself.","title":"resume (self, *args)"},{"location":"api/#resume2-self-args","text":"Mark processes as to be resumed params: args : the processes to be marked returns: The pipeline object itself.","title":"resume2 (self, *args)"},{"location":"api/#run-self-profile","text":"Run the pipeline params: profile : the profile used to run, if not found, it'll be used as runner name. default: 'default' returns: The pipeline object itself.","title":"run (self, profile)"},{"location":"api/#showallroutes-self","text":"","title":"showAllRoutes (self)"},{"location":"api/#start-self-args","text":"Set the starting processes of the pipeline params: args : the starting processes returns: The pipeline object itself.","title":"start (self, *args)"},{"location":"api/#module-proc","text":"The Proc class defining a process @static variables: `RUNNERS`: The regiested runners `ALIAS`: The alias for the properties `LOG_NLINE`: The limit of lines of logging information of same type of messages @magic methods: `__getattr__(self, name)`: get the value of a property in `self.props` `__setattr__(self, name, value)`: set the value of a property in `self.config`","title":"Module Proc"},{"location":"api/#__init__-self-tag-desc-id-kwargs","text":"Constructor params: tag : The tag of the process desc : The description of the process id : The identify of the process config: id, input, output, ppldir, forks, cache, cclean, rc, echo, runner, script, depends, tag, desc, dirsig exdir, exhow, exow, errhow, errntry, lang, beforeCmd, afterCmd, workdir, args, aggr callfront, callback, expect, expart, template, tplenvs, resume, nthread props input, output, rc, echo, script, depends, beforeCmd, afterCmd, workdir, expect expart, template, channel, jobs, ncjobids, size, sets, procvars, suffix, logs","title":"__init__ (self, tag, desc, id, **kwargs)"},{"location":"api/#_buildinput-self","text":"Build the input data Input could be: 1. list: ['input', 'infile:file'] = ['input:var', 'infile:path'] 2. str : \"input, infile:file\" = input:var, infile:path 3. dict: {\"input\": channel1, \"infile:file\": channel2} or {\"input:var, input:file\" : channel3} for 1,2 channels will be the combined channel from dependents, if there is not dependents, it will be sys.argv[1:]","title":"_buildInput (self)"},{"location":"api/#_buildjobs-self","text":"Build the jobs.","title":"_buildJobs (self)"},{"location":"api/#_buildoutput-self","text":"Build the output data templates waiting to be rendered.","title":"_buildOutput (self)"},{"location":"api/#_buildprocvars-self","text":"Build proc attribute values for template rendering, and also echo some out.","title":"_buildProcVars (self)"},{"location":"api/#_buildprops-self","text":"Compute some properties","title":"_buildProps (self)"},{"location":"api/#_buildscript-self","text":"Build the script template waiting to be rendered.","title":"_buildScript (self)"},{"location":"api/#_checkcached-self","text":"Tell whether the jobs are cached returns: True if all jobs are cached, otherwise False","title":"_checkCached (self)"},{"location":"api/#_readconfig-self-profile-profiles","text":"Read the configuration params: config : The configuration","title":"_readConfig (self, profile, profiles)"},{"location":"api/#_runcmd-self-key","text":"Run the beforeCmd or afterCmd params: key : \"beforeCmd\" or \"afterCmd\" returns: The return code of the command","title":"_runCmd (self, key)"},{"location":"api/#_runjobs-self","text":"Submit and run the jobs","title":"_runJobs (self)"},{"location":"api/#_savesettings-self","text":"Save all settings in proc.settings, mostly for debug","title":"_saveSettings (self)"},{"location":"api/#_suffix-self","text":"Calcuate a uid for the process according to the configuration The philosophy: 1. procs from different script must have different suffix (sys.argv[0]) 2. procs from the same script: - procs with different id or tag have different suffix - procs with different input have different suffix (depends, input) returns: The uniq id of the process","title":"_suffix (self)"},{"location":"api/#_tidyafterrun-self","text":"Do some cleaning after running jobs self.resume can only be: - '': normal process - skip+: skipped process but required workdir and data exists - resume: resume pipeline from this process, no requirement - resume+: get data from workdir/proc.settings, and resume","title":"_tidyAfterRun (self)"},{"location":"api/#_tidybeforerun-self","text":"Do some preparation before running jobs","title":"_tidyBeforeRun (self)"},{"location":"api/#copy-self-tag-desc-id","text":"Copy a process params: id : The new id of the process, default: None (use the varname) tag : The tag of the new process, default: None (used the old one) desc : The desc of the new process, default: None (used the old one) returns: The new process","title":"copy (self, tag, desc, id)"},{"location":"api/#log-self-msg-level-key","text":"The log function with aggregation name, process id and tag integrated. params: msg : The message to log level : The log level key : The type of messages","title":"log (self, msg, level, key)"},{"location":"api/#name-self-aggr","text":"Get my name include aggr , id , tag returns: the name","title":"name (self, aggr)"},{"location":"api/#run-self-profile-profiles","text":"Run the jobs with a configuration params: config : The configuration","title":"run (self, profile, profiles)"},{"location":"api/#module-channel","text":"The channen class, extended from list","title":"Module Channel"},{"location":"api/#_tuplize-tu-staticmethod","text":"A private method, try to convert an element to tuple If it's a string, convert it to (tu, ) Else if it is iterable, convert it to tuple(tu) Otherwise, convert it to (tu, ) Notice that string is also iterable. params: tu : the element to be converted returns: The converted element","title":"_tuplize (tu) [@staticmethod]"},{"location":"api/#attach-self-names-kwargs","text":"Attach columns to names of Channel, so we can access each column by: ch.col0 == ch.colAt(0) params: names : The names. Have to be as length as channel's width. None of them should be Channel's property name flatten : Whether flatten the channel for the name being attached","title":"attach (self, *names, **kwargs)"},{"location":"api/#cbind-self-cols","text":"Add columns to the channel params: cols : The columns returns: The channel with the columns inserted.","title":"cbind (self, *cols)"},{"location":"api/#colat-self-index","text":"Fetch one column of a Channel params: index : which column to fetch returns: The Channel with that column","title":"colAt (self, index)"},{"location":"api/#collapse-self-col","text":"Do the reverse of expand length: N - 1 width: M - M params: col : the index of the column used to collapse returns: The collapsed Channel","title":"collapse (self, col)"},{"location":"api/#copy-self","text":"Copy a Channel using copy.copy returns: The copied Channel","title":"copy (self)"},{"location":"api/#create-l-staticmethod","text":"Create a Channel from a list params: l : The list, default: [] returns: The Channel created from the list","title":"create (l) [@staticmethod]"},{"location":"api/#expand-self-col-pattern-t-sortby-reverse","text":"expand the Channel according to the files in , other cols will keep the same [(dir1/dir2, 1)].expand (0, \"*\") will expand to [(dir1/dir2/file1, 1), (dir1/dir2/file2, 1), ...] length: 1 - N width: M - M params: col : the index of the column used to expand pattern : use a pattern to filter the files/dirs, default: * t : the type of the files/dirs to include 'dir', 'file', 'link' or 'any' (default) sortby : how the list is sorted 'name' (default), 'mtime', 'size' reverse : reverse sort. Default: False returns: The expanded Channel","title":"expand (self, col, pattern, t, sortby, reverse)"},{"location":"api/#filter-self-func","text":"Alias of python builtin filter params: func : the function. Default: None returns: The filtered Channel","title":"filter (self, func)"},{"location":"api/#filtercol-self-func-col","text":"Just filter on the first column params: func : the function col : the column to filter returns: The filtered Channel","title":"filterCol (self, func, col)"},{"location":"api/#flatten-self-col","text":"Convert a single-column Channel to a list (remove the tuple signs) [(a,), (b,)] to [a, b] params: col : The column to flat. None for all columns (default) returns: The list converted from the Channel.","title":"flatten (self, col)"},{"location":"api/#fold-self-n","text":"Fold a Channel. Make a row to n-length chunk rows a1 a2 a3 a4 b1 b2 b3 b4 if n==2, fold(2) will change it to: a1 a2 a3 a4 b1 b2 b3 b4 params: n : the size of the chunk returns The new Channel","title":"fold (self, n)"},{"location":"api/#fromargv-staticmethod","text":"Create a Channel from sys.argv[1:] \"python test.py a b c\" creates a width=1 Channel \"python test.py a,1 b,2 c,3\" creates a width=2 Channel returns: The Channel created from the command line arguments","title":"fromArgv () [@staticmethod]"},{"location":"api/#fromchannels-args-staticmethod","text":"Create a Channel from Channels params: args : The Channels returns: The Channel merged from other Channels","title":"fromChannels (*args) [@staticmethod]"},{"location":"api/#fromfile-fn-header-skip-delimit-staticmethod","text":"Create Channel from the file content It's like a matrix file, each row is a row for a Channel. And each column is a column for a Channel. params: fn : the file header : Whether the file contains header. If True, will attach the header So you can use channel. header to fetch the column skip : first lines to skip delimit : the delimit for columns returns: A Channel created from the file","title":"fromFile (fn, header, skip, delimit) [@staticmethod]"},{"location":"api/#frompairs-pattern-staticmethod","text":"Create a width = 2 Channel from a pattern params: pattern : the pattern returns: The Channel create from every 2 files match the pattern","title":"fromPairs (pattern) [@staticmethod]"},{"location":"api/#fromparams-pnames-staticmethod","text":"Create a Channel from params params: *pnames : The names of the option returns: The Channel","title":"fromParams (*pnames) [@staticmethod]"},{"location":"api/#frompattern-pattern-t-sortby-reverse-staticmethod","text":"Create a Channel from a path pattern params: pattern : the pattern with wild cards t : the type of the files/dirs to include 'dir', 'file', 'link' or 'any' (default) sortby : how the list is sorted 'name' (default), 'mtime', 'size' reverse : reverse sort. Default: False returns: The Channel created from the path","title":"fromPattern (pattern, t, sortby, reverse) [@staticmethod]"},{"location":"api/#get-self-idx","text":"Get the element of a flattened channel params: idx : The index of the element to get. Default: 0 return: The element","title":"get (self, idx)"},{"location":"api/#insert-self-cidx-cols","text":"Insert columns to a channel params: cidx : Insert into which index of column? cols : the columns to be bound to Channel returns: The combined Channel Note, self is also changed","title":"insert (self, cidx, *cols)"},{"location":"api/#length-self","text":"Get the length of a Channel It's just an alias of len(chan) returns: The length of the Channel","title":"length (self)"},{"location":"api/#map-self-func","text":"Alias of python builtin map params: func : the function returns: The transformed Channel","title":"map (self, func)"},{"location":"api/#mapcol-self-func-col","text":"Map for a column params: func : the function col : the index of the column. Default: 0 returns: The transformed Channel","title":"mapCol (self, func, col)"},{"location":"api/#nones-length-width-staticmethod","text":"Create a channel with None s params: length : The length of the channel width : The width of the channel returns: The created channel","title":"nones (length, width) [@staticmethod]"},{"location":"api/#rbind-self-rows","text":"The multiple-argument versoin of rbind params: rows : the rows to be bound to Channel returns: The combined Channel Note, self is also changed","title":"rbind (self, *rows)"},{"location":"api/#reduce-self-func","text":"Alias of python builtin reduce params: func : the function returns: The reduced value","title":"reduce (self, func)"},{"location":"api/#reducecol-self-func-col","text":"Reduce a column params: func : the function col : the column to reduce returns: The reduced value","title":"reduceCol (self, func, col)"},{"location":"api/#repcol-self-n","text":"Repeat column and return a new channel params: n : how many times to repeat. returns: The new channel with repeated columns","title":"repCol (self, n)"},{"location":"api/#reprow-self-n","text":"Repeat row and return a new channel params: n : how many times to repeat. returns: The new channel with repeated rows","title":"repRow (self, n)"},{"location":"api/#rowat-self-index","text":"Fetch one row of a Channel params: index : which row to fetch returns: The Channel with that row","title":"rowAt (self, index)"},{"location":"api/#slice-self-start-length","text":"Fetch some columns of a Channel params: start : from column to start length : how many columns to fetch, default: None (from start to the end) returns: The Channel with fetched columns","title":"slice (self, start, length)"},{"location":"api/#split-self-flatten","text":"Split a Channel to single-column Channels returns: The list of single-column Channels","title":"split (self, flatten)"},{"location":"api/#t-self","text":"Transpose a channel","title":"t (self)"},{"location":"api/#transpose-self","text":"Transpose a channel","title":"transpose (self)"},{"location":"api/#unfold-self-n","text":"Do the reverse thing as self.fold does params: n : How many rows to combind each time. default: 2 returns: The unfolded Channel","title":"unfold (self, n)"},{"location":"api/#unique-self","text":"Make the channel unique, remove duplicated rows Try to keep the order","title":"unique (self)"},{"location":"api/#width-self","text":"Get the width of a Channel returns: The width of the Channel","title":"width (self)"},{"location":"api/#module-job","text":"Job class, defining a job in a process","title":"Module Job"},{"location":"api/#__init__-self-index-proc","text":"Constructor params: index : The index of the job in a process proc : The process","title":"__init__ (self, index, proc)"},{"location":"api/#_indexindicator-self","text":"Get the index indicator in the log returns: The \"[001/100]\" like indicator","title":"_indexIndicator (self)"},{"location":"api/#_linkinfile-self-orgfile","text":"Create links for input files params: orgfile : The original input file returns: The link to the original file.","title":"_linkInfile (self, orgfile)"},{"location":"api/#_prepinput-self","text":"Prepare input, create link to input files and set other placeholders","title":"_prepInput (self)"},{"location":"api/#_prepoutput-self","text":"Build the output data. Output could be: 1. list: ['output:var:{{input}}', 'outfile:file:{{infile.bn}}.txt'] or you can ignore the name if you don't put it in script: ['var:{{input}}', 'path:{{infile.bn}}.txt'] or even (only var type can be ignored): ['{{input}}', 'file:{{infile.bn}}.txt'] 2. str : 'output:var:{{input}}, outfile:file:{{infile.bn}}.txt' 3. OrderedDict: {\"output:var:{{input}}\": channel1, \"outfile:file:{{infile.bn}}.txt\": channel2} or {\"output:var:{{input}}, output:file:{{infile.bn}}.txt\" : channel3} for 1,2 channels will be the property channel for this proc (i.e. p.channel)","title":"_prepOutput (self)"},{"location":"api/#_prepscript-self","text":"Build the script, interpret the placeholders","title":"_prepScript (self)"},{"location":"api/#_reportitem-self-key-maxlen-data-loglevel","text":"Report the item on logs params: key : The key of the item maxlen : The max length of the key data : The data of the item loglevel : The log level","title":"_reportItem (self, key, maxlen, data, loglevel)"},{"location":"api/#cache-self","text":"Truly cache the job (by signature)","title":"cache (self)"},{"location":"api/#checkoutfiles-self-expect","text":"Check whether output files are generated, if not, add - to rc.","title":"checkOutfiles (self, expect)"},{"location":"api/#done-self","text":"Do some cleanup when job finished","title":"done (self)"},{"location":"api/#export-self","text":"Export the output files","title":"export (self)"},{"location":"api/#init-self","text":"Initiate a job, make directory and prepare input, output and script.","title":"init (self)"},{"location":"api/#isexptcached-self","text":"Prepare to use export files as cached information True if succeed, otherwise False","title":"isExptCached (self)"},{"location":"api/#istrulycached-self","text":"Check whether a job is truly cached (by signature)","title":"isTrulyCached (self)"},{"location":"api/#pid-self-val","text":"Get/Set the job id (pid or the id from queue system) params: val : The id to be set","title":"pid (self, val)"},{"location":"api/#rc-self-val","text":"Get/Set the return code params: val : The return code to be set. If it is None, return the return code. Default: None If val == -1000: the return code will be negative of current one. 0 will be '-0' returns: The return code if val is None If rcfile does not exist or is empty, return 9999, otherwise return -rc A negative rc (including -0) means output files not generated","title":"rc (self, val)"},{"location":"api/#report-self","text":"Report the job information to logger","title":"report (self)"},{"location":"api/#reset-self-retry","text":"Clear the intermediate files and output files","title":"reset (self, retry)"},{"location":"api/#showerror-self-totalfailed","text":"Show the error message if the job failed.","title":"showError (self, totalfailed)"},{"location":"api/#signature-self","text":"Calculate the signature of the job based on the input/output and the script returns: The signature of the job","title":"signature (self)"},{"location":"api/#succeed-self","text":"Tell if the job is successful by return code, and output file expectations. returns: True if succeed else False","title":"succeed (self)"},{"location":"api/#module-jobmgr","text":"Job Manager","title":"Module Jobmgr"},{"location":"api/#__init__-self-proc-runner","text":"Job manager constructor params: proc : The process runner : The runner class","title":"__init__ (self, proc, runner)"},{"location":"api/#_exit-self","text":"","title":"_exit (self)"},{"location":"api/#alljobsdone-self","text":"Tell whether all jobs are done. No need to lock as it only runs in one process (the watcher process) returns: True if all jobs are done else False","title":"allJobsDone (self)"},{"location":"api/#cansubmit-self","text":"Tell whether we can submit jobs. returns: True if we can, otherwise False","title":"canSubmit (self)"},{"location":"api/#halt-self-halt_anyway","text":"Halt the pipeline if needed","title":"halt (self, halt_anyway)"},{"location":"api/#progressbar-self-jid-loglevel","text":"","title":"progressbar (self, jid, loglevel)"},{"location":"api/#run-self","text":"Start to run the jobs","title":"run (self)"},{"location":"api/#runpool-self-rq-sq","text":"The pool to run jobs (wait jobs to be done) params: rq : The run queue sq : The submit queue","title":"runPool (self, rq, sq)"},{"location":"api/#submitpool-self-sq","text":"The pool to submit jobs params: sq : The submit queue","title":"submitPool (self, sq)"},{"location":"api/#watchpool-self-rq-sq","text":"The watchdog, checking whether all jobs are done.","title":"watchPool (self, rq, sq)"},{"location":"api/#module-aggr","text":"The aggregation of a set of processes @magic methods: `__setattr__(self, name, value)`: Set property value of an aggregation. - if it's a common property, set it to all processes - if it is `input` set it to starting processes - if it is `depends` set it to the end processes - if it is related to `export` (startswith `ex`), set it to the end processes - if it is in ['starts', 'ends', 'id'], set it to the aggregation itself. - Otherwise a `ValueError` raised. - You can use `[aggr].[proc].[prop]` to set/get the properties of a processes in the aggregation.","title":"Module Aggr"},{"location":"api/#__init__-self-args-kwargs","text":"Constructor params: args : the set of processes depends : Whether auto deduce depends. Default: True id : The id of the aggr. Default: None (the variable name) tag : The tag of the processes. Default: None (a unique 4-char str according to the id)","title":"__init__ (self, *args, **kwargs)"},{"location":"api/#_select-self-key-forcelist-flatten","text":"Select processes # self._procs = OrderedDict([ # ( a , Proc(id = a )), # ( b , Proc(id = b )), # ( c , Proc(id = c )), # ( d , Proc(id = d )) # ]) self[ a ] # proc a self[0] # proc a self[1:2] # _Proxy of (proc b, proc c) self[1,3] # _Proxy of (proc b, proc d) self[ b , c ] # _Proxy of (proc b, proc c) self[ b,c ] # _Proxy of (proc b, proc c) self[Proc(id = d )] # proc d #### `addEnd (self, *procs) ` #### `addProc (self, p, tag, where, copy) ` Add a process to the aggregation. Note that you have to adjust the dependencies after you add processes. - **params:** `p`: The process `where`: Add to where: starts , ends , both or None (default) - **returns:** the aggregation itself #### `addStart (self, *procs) ` #### `copy (self, tag, depends, id, delegates, modules) ` Like `proc` s `copy` function, copy an aggregation. Each processes will be copied. - **params:** `tag`: The new tag of all copied processes `depends`: Whether to copy the dependencies or not. Default: True - dependences for processes in starts will not be copied `id`: Use a different id if you don t want to use the variant name `delegates`: Copy delegates? Default: `True` `configs`: Copy configs? Default: `True` - **returns:** The new aggregation #### `delEnd (self, *procs) ` #### `delStart (self, *procs) ` #### `delegate (self, attrs, procs) ` Delegate the procs to have the attributes set by: `aggr.args.a.b = 1` Instead of setting `args.a.b` of all processes, `args.a.b` of only delegated processes will be set. `procs` can be `starts`/`ends`, but it cannot be set with other procs, which means you can do: `aggr.delegate( args , starts )`, but not `aggr.delegate( args , [ starts , pXXX ])` #### `module (self, name, starts, depends, ends, starts_shared, depends_shared, ends_shared) ` Define a function for aggr. The shared parameters will be indicators not to remove those processes when the shared function is on. - **params:** `name` : The name of the function `starts` : A list of start processes. `depends` : A dict of dependences of the procs `ends` : A list of end processes `starts_shared` : A dict of functions that shares the same starts `depends_shared`: A dict of functions that shares the same depends `ends_shared` : A dict of functions that shares the same ends - For example: `{ procs : func }` #### `moduleFunc (self, name, on, off) ` #### `off (self, *names) ` #### `on (self, *names) ` ## Module `flowchart.Flowchart` Draw flowchart for pipelines @static variables: `THEMES`: predefined themes #### `__init__ (self, fcfile, dotfile) ` The constructor - **params:** `fcfile`: The flowchart file. Default: `path.splitext(sys.argv[0])[0] + .pyppl.svg ` `dotfile`: The dot file. Default: `path.splitext(sys.argv[0])[0] + .pyppl.dot ` #### `_assemble (self) ` Assemble the graph for printing and rendering #### `addLink (self, node1, node2) ` Add a link to the chart - **params:** `node1`: The first node. `node2`: The second node. #### `addNode (self, node, role) ` Add a node to the chart - **params:** `node`: The node `role`: Is it a starting node, an ending node or None. Default: None. #### `generate (self) ` Generate the dot file and graph file. #### `setTheme (self, theme, base) ` Set the theme to be used - **params:** `theme`: The theme, could be the key of Flowchart.THEMES or a dict of a theme definition. `base` : The base theme to be based on you pass custom theme ## Module `parameters.Parameter` The class for a single parameter #### `__init__ (self, name, value) ` Constructor - **params:** `name`: The name of the parameter `value`: The initial value of the parameter #### `_forceType (self) ` Coerce the value to the type specified TypeError will be raised if error happens #### `setDesc (self, d) ` Set the description of the parameter - **params:** `d`: The description #### `setName (self, n) ` Set the name of the parameter - **params:** `n`: The name #### `setRequired (self, r) ` Set whether this parameter is required - **params:** `r`: True if required else False. Default: True #### `setShow (self, s) ` Set whether this parameter should be shown in help information - **params:** `s`: True if it shows else False. Default: True #### `setType (self, t) ` Set the type of the parameter - **params:** `t`: The type of the value. Default: str - Note: str rather then str #### `setValue (self, v) ` Set the value of the parameter - **params:** `v`: The value ## Module `parameters.Parameters` A set of parameters #### `__init__ (self, command, theme) ` Constructor #### `_coerceValue (value, t) [@staticmethod]` #### `_getType (self, argname, argtype) ` #### `_parseName (self, argname) ` If `argname` is the name of an option - **params:** `argname`: The argname - **returns:** `an`: clean argument name `at`: normalized argument type `av`: the argument value, if `argname` is like: `-a=1` #### `_putValue (self, argname, argtype, argval) ` #### `_setDesc (self, desc) ` #### `_setHbald (self, hbald) ` #### `_setHopts (self, hopts) ` #### `_setPrefix (self, prefix) ` #### `_setTheme (self, theme) ` #### `_setUsage (self, usage) ` #### `_shouldPrintHelp (self, args) ` #### `asDict (self) ` Convert the parameters to Box object - **returns:** The Box object #### `help (self, error, printNexit) ` Calculate the help page - **params:** `error`: The error message to show before the help information. Default: ` ` `printNexit`: Print the help page and exit the program? Default: `False` (return the help information) - **return:** The help information #### `loadDict (self, dictVar, show) ` Load parameters from a dict - **params:** `dictVar`: The dict variable. - Properties are set by param .required , param .show , ... `show`: Whether these parameters should be shown in help information - Default: False (don t show parameter from config object in help page) - It ll be overwritten by the `show` property inside dict variable. - If it is None, will inherit the param s show value #### `loadFile (self, cfgfile, show) ` Load parameters from a json/config file If the file name ends with .json , `json.load` will be used, otherwise, `ConfigParser` will be used. For config file other than json, a section name is needed, whatever it is. - **params:** `cfgfile`: The config file `show`: Whether these parameters should be shown in help information - Default: False (don t show parameter from config file in help page) - It ll be overwritten by the `show` property inside the config file. #### `parse (self, args) ` Parse the arguments from `sys.argv` ## Module `logger` A customized logger for pyppl #### `class: Box` Allow dot operation for OrderedDict #### `class: LoggerThemeError` Theme errors for logger #### `class: PyPPLLogFilter` logging filter by levels (flags) #### `class: PyPPLLogFormatter` logging formatter for pyppl #### `class: PyPPLStreamHandler` #### `class: TemplatePyPPL` Built-in template wrapper. #### `Value (typecode_or_type, *args, **kwds) [@staticmethod]` Returns a synchronized shared object #### `_formatTheme (theme) [@staticmethod]` Make them in the standard form with bgcolor and fgcolor in raw terminal color strings If the theme is read from file, try to translate COLORS.xxx to terminal color strings - **params:** `theme`: The theme - **returns:** The formatted colors #### `_getColorFromTheme (level, theme) [@staticmethod]` Get colors from a them - **params:** `level`: Our own log record level `theme`: The theme - **returns:** The colors #### `_getLevel (record) [@staticmethod]` Get the flags of a record - **params:** `record`: The logging record #### `getLogger (levels, theme, logfile, lvldiff, pbar, name) [@staticmethod]` Get the default logger - **params:** `levels`: The log levels(tags), default: basic `theme`: The theme of the logs on terminal. Default: True (default theme will be used) - False to disable theme `logfile`:The log file. Default: None (don t white to log file) `lvldiff`:The diff levels for log - [ -depends , jobdone , +debug ]: show jobdone, hide depends and debug `name`: The name of the logger, default: PyPPL - **returns:** The logger ## Module `utils` A set of utitities for PyPPL #### `class: Box` Allow dot operation for OrderedDict #### `alwaysList (data) ` Convert a string or a list with element - **params:** `data`: the data to be converted - **examples:** ```python data = [ a, b, c , d ] ret = alwaysList (data) # ret == [ a , b , c , d ] returns: The split list","title":"_select (self, key, forceList, flatten)"},{"location":"api/#asstr-s-encoding","text":"Convert everything (str, unicode, bytes) to str with python2, python3 compatiblity","title":"asStr (s, encoding)"},{"location":"api/#brieflist-l","text":"Briefly show an integer list, combine the continuous numbers. params: l : The list returns: The string to show for the briefed list.","title":"briefList (l)"},{"location":"api/#dictupdate-origdict-newdict","text":"Update a dictionary recursively. params: origDict : The original dictionary newDict : The new dictionary examples: od1 = { a : { b : { c : 1 , d : 1 }}} od2 = { key : value for key : value in od1 . items ()} nd = { a : { b : { d : 2 }}} od1 . update ( nd ) # od1 == { a : { b : { d : 2}}}, od1[ a ][ b ] is lost dictUpdate ( od2 , nd ) # od2 == { a : { b : { c : 1, d : 2}}}","title":"dictUpdate (origDict, newDict)"},{"location":"api/#filter-func-vec","text":"Python2 and Python3 compatible filter params: func : The filter function vec : The list to be filtered returns: The filtered list","title":"filter (func, vec)"},{"location":"api/#formatsecs-seconds","text":"Format a time duration params: seconds : the time duration in seconds returns: The formated string. For example: \"01:01:01.001\" stands for 1 hour 1 min 1 sec and 1 minisec.","title":"formatSecs (seconds)"},{"location":"api/#funcsig-func","text":"Get the signature of a function Try to get the source first, if failed, try to get its name, otherwise return None params: func : The function returns: The signature","title":"funcsig (func)"},{"location":"api/#map-func-vec","text":"Python2 and Python3 compatible map params: func : The map function vec : The list to be maped returns: The maped list","title":"map (func, vec)"},{"location":"api/#range-i-args-kwargs","text":"Convert a range to list, because in python3, range is not a list params: r : the range data returns: The converted list","title":"range (i, *args, **kwargs)"},{"location":"api/#reduce-func-vec","text":"Python2 and Python3 compatible reduce params: func : The reduce function vec : The list to be reduced returns: The reduced value","title":"reduce (func, vec)"},{"location":"api/#split-s-delimter-trim","text":"Split a string using a single-character delimter params: s : the string delimter : the single-character delimter trim : whether to trim each part. Default: True examples: ret = split ( a,b ,c , , ) # ret == [ a,b , c ] # , inside quotes will be recognized. returns: The list of substrings","title":"split (s, delimter, trim)"},{"location":"api/#uid-s-l-alphabet","text":"Calculate a short uid based on a string. Safe enough, tested on 1000000 32-char strings, no repeated uid found. This is used to calcuate a uid for a process params: s : the base string l : the length of the uid alphabet : the charset used to generate the uid returns: The uid","title":"uid (s, l, alphabet)"},{"location":"api/#varname-maxline-incldot","text":"Get the variable name for ini params: maxline : The max number of lines to retrive. Default: 20 incldot : Whether include dot in the variable name. Default: False returns: The variable name","title":"varname (maxline, incldot)"},{"location":"api/#module-utilsbox","text":".","title":"Module utils.box"},{"location":"api/#class-box","text":"Allow dot operation for OrderedDict","title":"class: Box"},{"location":"api/#module-utilscmd","text":".","title":"Module utils.cmd"},{"location":"api/#class-cmd","text":"A command (subprocess) wapper","title":"class: Cmd"},{"location":"api/#class-timeout","text":"","title":"class: Timeout"},{"location":"api/#run-cmd-bg-raiseexc-timeout-kwargs-staticmethod","text":"A shortcut of Command.run To chain another command, you can do: run('seq 1 3', bg = True).pipe('grep 1') params: cmd : The command, could be a string or a list bg : Run in background or not. Default: False If it is True , rc and stdout/stderr will be default (no value retrieved). raiseExc : raise the expcetion or not **kwargs : other arguments for Popen returns: The Command instance","title":"run (cmd, bg, raiseExc, timeout, **kwargs) [@staticmethod]"},{"location":"api/#sleep-cmd-bg-raiseexc-timeout-kwargs-kwargs","text":"sleep(seconds) Delay execution for a given number of seconds. The argument may be a floating point number for subsecond precision.","title":"sleep (cmd, bg, raiseExc, timeout, **kwargs, **kwargs)"},{"location":"api/#time-cmd-bg-raiseexc-timeout-kwargs-kwargs-kwargs","text":"time() - floating point number Return the current time in seconds since the Epoch. Fractions of a second may be present if the system clock provides them.","title":"time (cmd, bg, raiseExc, timeout, **kwargs, **kwargs, **kwargs)"},{"location":"api/#module-utilsparallel","text":".","title":"Module utils.parallel"},{"location":"api/#class-parallel","text":"A parallel runner","title":"class: Parallel"},{"location":"api/#run-func-args-nthread-backend-raiseexc-staticmethod","text":"A shortcut of Parallel.run params: func : The function to run args : The arguments for the function, should be a list with tuple s nthread : Number of jobs to run simultaneously. Default: 1 backend : The backend, either process (default) or thread raiseExc : Whether raise exception or not. Default: True returns: The merged results from each job.","title":"run (func, args, nthread, backend, raiseExc) [@staticmethod]"},{"location":"api/#module-utilssafefs","text":".","title":"Module utils.safefs"},{"location":"api/#class-chmoderror","text":"OS system call failed.","title":"class: ChmodError"},{"location":"api/#lock-staticmethod","text":"Returns a non-recursive lock object","title":"Lock () [@staticmethod]"},{"location":"api/#class-safefs","text":"A thread-safe file system @static variables: `TMPDIR`: The default temporary directory to store lock files # file types `FILETYPE_UNKNOWN` : Unknown file type `FILETYPE_NOENT` : File does not exist `FILETYPE_NOENTLINK`: A dead link (a link links to a non-existent file. `FILETYPE_FILE` : A regular file `FILETYPE_FILELINK` : A link to a regular file `FILETYPE_DIR` : A regular directory `FILETYPE_DIRLINK` : A link to a regular directory # relation of two files `FILES_DIFF_BOTHNOENT` : Two files are different and none of them exists `FILES_DIFF_NOENT1` : Two files are different but file1 does not exists `FILES_DIFF_NOENT2` : Two files are different but file2 does not exists `FILES_DIFF_BOTHENT` : Two files are different and both of them exist `FILES_SAME_STRNOENT` : Two files are the same string and it does not exist `FILES_SAME_STRENT` : Two files are the same string and it exists `FILES_SAME_BOTHLINKS` : Two files link to one file `FILES_SAME_BOTHLINKS1`: File1 links to file2, file2 links to a regular file `FILES_SAME_BOTHLINKS2`: File2 links to file1, file1 links to a regular file `FILES_SAME_REAL1` : File2 links to file1, which a regular file `FILES_SAME_REAL2` : File1 links to file2, which a regular file `LOCK`: A global lock ensures the locks are locked at the same time","title":"class: SafeFs"},{"location":"api/#copy-file1-file2-overwrite-callback-staticmethod","text":"A shortcut of SafeFs.copy params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False","title":"copy (file1, file2, overwrite, callback) [@staticmethod]"},{"location":"api/#exists-filepath-callback-staticmethod","text":"A shortcut of SafeFs.exists params: filepath : The filepath callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if the file exists else False","title":"exists (filepath, callback) [@staticmethod]"},{"location":"api/#gz-file1-file2-overwrite-callback-staticmethod","text":"A shortcut of SafeFs.gz params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False","title":"gz (file1, file2, overwrite, callback) [@staticmethod]"},{"location":"api/#link-file1-file2-overwrite-callback-staticmethod","text":"A shortcut of SafeFs.link params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False","title":"link (file1, file2, overwrite, callback) [@staticmethod]"},{"location":"api/#movewithlink-file1-file2-overwrite-callback-staticmethod","text":"A shortcut of SafeFs.moveWithLink params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False","title":"moveWithLink (file1, file2, overwrite, callback) [@staticmethod]"},{"location":"api/#osremove-file1-file2-overwrite-callback","text":"remove(path) Remove a file (same as unlink(path)).","title":"osremove (file1, file2, overwrite, callback)"},{"location":"api/#readlink-file1-file2-overwrite-callback","text":"readlink(path) - path Return a string representing the path to which the symbolic link points.","title":"readlink (file1, file2, overwrite, callback)"},{"location":"api/#shmove-src-dst-staticmethod","text":"Recursively move a file or directory to another location. This is similar to the Unix \"mv\" command. If the destination is a directory or a symlink to a directory, the source is moved inside the directory. The destination path must not already exist. If the destination already exists but is not a directory, it may be overwritten depending on os.rename() semantics. If the destination is on our current filesystem, then rename() is used. Otherwise, src is copied to the destination and then removed. A lot more could be done here... A look at a mv.c shows a lot of the issues this implementation glosses over.","title":"shmove (src, dst) [@staticmethod]"},{"location":"api/#ungz-file1-file2-overwrite-callback-staticmethod","text":"A shortcut of SafeFs.ungz params: file1 : File 1 file2 : File 2 overwrite : Whether overwrite file 2. Default: True callback : The callback. arguments: r : Whether the file exists fs : This instance returns: True if succeed else False","title":"ungz (file1, file2, overwrite, callback) [@staticmethod]"},{"location":"api/#module-proctreeprocnode","text":"The node for processes to manage relations between each other","title":"Module proctree.ProcNode"},{"location":"api/#__init__-self-proc","text":"Constructor params: proc : The Proc instance","title":"__init__ (self, proc)"},{"location":"api/#sameidtag-self-proc","text":"Check if the process has the same id and tag with me. params: proc : The Proc instance returns: True if it is. False if not.","title":"sameIdTag (self, proc)"},{"location":"api/#module-proctreeproctree","text":".","title":"Module proctree.ProcTree"},{"location":"api/#__init__-self","text":"Constructor, set the status of all ProcNode s","title":"__init__ (self)"},{"location":"api/#check-proc-staticmethod","text":"Check whether a process with the same id and tag exists params: proc : The Proc instance","title":"check (proc) [@staticmethod]"},{"location":"api/#checkpath-self-proc","text":"Check whether paths of a process can start from a start process params: proc : The process returns: True if all paths can pass The failed path otherwise","title":"checkPath (self, proc)"},{"location":"api/#getallpaths-self","text":"","title":"getAllPaths (self)"},{"location":"api/#getends-self","text":"Get the end processes returns: The end processes","title":"getEnds (self)"},{"location":"api/#getnext-proc-staticmethod","text":"Get next processes of process params: proc : The Proc instance returns: The processes depend on this process","title":"getNext (proc) [@staticmethod]"},{"location":"api/#getnextstr-proc-staticmethod","text":"Get the names of processes depend on a process params: proc : The Proc instance returns: The names","title":"getNextStr (proc) [@staticmethod]"},{"location":"api/#getnexttorun-self","text":"Get the process to run next returns: The process next to run","title":"getNextToRun (self)"},{"location":"api/#getpaths-self-proc-proc0","text":"Infer the path to a process params: proc : The process proc0 : The original process, because this function runs recursively. returns: p1 - p2 - p3 p4 _/ Paths for p3: [[p4], [p2, p1]]","title":"getPaths (self, proc, proc0)"},{"location":"api/#getpathstostarts-self-proc","text":"Filter the paths with start processes params: proc : The process returns: The filtered path","title":"getPathsToStarts (self, proc)"},{"location":"api/#getprevstr-proc-staticmethod","text":"Get the names of processes a process depends on params: proc : The Proc instance returns: The names","title":"getPrevStr (proc) [@staticmethod]"},{"location":"api/#getstarts-self","text":"Get the start processes returns: The start processes","title":"getStarts (self)"},{"location":"api/#register-proc-staticmethod","text":"Register the process params: proc : The Proc instance","title":"register (proc) [@staticmethod]"},{"location":"api/#reset-staticmethod","text":"Reset the status of all ProcNode s","title":"reset () [@staticmethod]"},{"location":"api/#setstarts-self-starts","text":"Set the start processes params: starts : The start processes","title":"setStarts (self, starts)"},{"location":"api/#unranprocs-self","text":"","title":"unranProcs (self)"},{"location":"api/#module-templatestemplatepyppl","text":"Built-in template wrapper.","title":"Module templates.TemplatePyPPL"},{"location":"api/#__init__-self-source-envs","text":"Initiate the engine with source and envs params: source : The souce text envs : The env data","title":"__init__ (self, source, **envs)"},{"location":"api/#_render-self-data","text":"Render the template params: data : The data used for rendering returns: The rendered string","title":"_render (self, data)"},{"location":"api/#module-templatestemplatejinja2","text":"Jinja2 template wrapper","title":"Module templates.TemplateJinja2"},{"location":"api/#__init__-self-source-envs_1","text":"Initiate the engine with source and envs params: source : The souce text envs : The env data","title":"__init__ (self, source, **envs)"},{"location":"api/#_render-self-data_1","text":"Render the template params: data : The data used for rendering returns: The rendered string","title":"_render (self, data)"},{"location":"api/#module-runnersrunner","text":"The base runner class","title":"Module runners.Runner"},{"location":"api/#__init__-self-job","text":"Constructor params: job : The job object","title":"__init__ (self, job)"},{"location":"api/#_flush-self-fout-ferr-lastout-lasterr-end","text":"Flush stdout/stderr params: fout : The stdout file handler ferr : The stderr file handler lastout : The leftovers of previously readlines of stdout lasterr : The leftovers of previously readlines of stderr end : Whether this is the last time to flush","title":"_flush (self, fout, ferr, lastout, lasterr, end)"},{"location":"api/#finish-self","text":"","title":"finish (self)"},{"location":"api/#getpid-self","text":"Get the job id","title":"getpid (self)"},{"location":"api/#isrunning-self","text":"Try to tell whether the job is still running. returns: True if yes, otherwise False","title":"isRunning (self)"},{"location":"api/#kill-self","text":"Try to kill the running jobs if I am exiting","title":"kill (self)"},{"location":"api/#retry-self","text":"","title":"retry (self)"},{"location":"api/#run-self_1","text":"returns: True: success/fail False: needs retry","title":"run (self)"},{"location":"api/#submit-self","text":"Try to submit the job","title":"submit (self)"},{"location":"api/#module-runnersrunnerlocal","text":"Constructor @params: job : The job object config : The properties of the process","title":"Module runners.RunnerLocal"},{"location":"api/#__init__-self-job_1","text":"","title":"__init__ (self, job)"},{"location":"api/#module-runnersrunnerssh","text":"The ssh runner @static variables: `SERVERID`: The incremental number used to calculate which server should be used. - Don't touch unless you know what's going on!","title":"Module runners.RunnerSsh"},{"location":"api/#__init__-self-job_2","text":"Constructor params: job : The job object","title":"__init__ (self, job)"},{"location":"api/#isserveralive-server-key-staticmethod","text":"","title":"isServerAlive (server, key) [@staticmethod]"},{"location":"api/#module-runnersrunnersge","text":"The sge runner","title":"Module runners.RunnerSge"},{"location":"api/#__init__-self-job_3","text":"Constructor params: job : The job object config : The properties of the process","title":"__init__ (self, job)"},{"location":"api/#module-runnersrunnerslurm","text":"The slurm runner","title":"Module runners.RunnerSlurm"},{"location":"api/#__init__-self-job_4","text":"Constructor params: job : The job object config : The properties of the process","title":"__init__ (self, job)"},{"location":"api/#module-runnersrunnerdry","text":"The dry runner","title":"Module runners.RunnerDry"},{"location":"api/#__init__-self-job_5","text":"Constructor params: job : The job object","title":"__init__ (self, job)"},{"location":"api/#finish-self_1","text":"Do some cleanup work when jobs finish","title":"finish (self)"},{"location":"basic-concepts-and-directory-structure/","text":"Basic concepts and folder structure Layers of a pipeline The pipeline consists of channels and processes. A process may have many jobs. Each job uses the corresponding elements from the input channel of the process, and generates values for output channel. Actually, what you need to do is just specify the first input channel, and then tell PyPPL the dependencies of the processes. The later processes will use the output channel of the processes they depend on. Of course, you can interfere by using functions in the input specification. Folder structure ./ |-- pipeline.py `-- workdir/ `-- PyPPL. id . tag . suffix / |-- lock |-- proc.settings `-- job.index / |-- input/ |-- output/ |-- job.cache |-- job.script |-- job.pid |-- job.rc |-- job.stdout |-- job.stderr |-- [job.script.local] |-- [job.script.ssh] `-- [job.script.sge] Path Content Memo workdir/ Where the pipeline directories of all processes of current pipeline are located. Can be set by p.ppldir PyPPL. id . tag . suffix / The work directory of current process. The suffix is a unique identify of the process according to its configuration. You may set it by p.workdir proc.settings/ The settings of the process A copy of proc settings job.index / The job directory Starts with 1 job.index /input/ Where you can find the links to all the input files job.index /output/ Where you can find all the output files job.index /job.cache The file containing the signature of the job job.index /job.script To script file to be running job.index /job.pid The id of the job of its running system. Mostly used to tell whether the process is still running. job.index /job.rc To file containing the return code job.index /job.stdout The STDOUT of the script job.index /job.stderr The STDERR of the script job.index /job.script.local The wrapper for local jobs to save return code, stdout and stderr job.index /job.script.ssh The script file for ssh runner job.index /job.script.sge The script file for sge runner Note You are encouraged to set p.ppldir BUT NOT p.workdir , as it contains a unique suffix that is automatically computed. Symbols used in this documentation workdir refers to ./workdir/PyPPL. id . tag . suffix / , indir refers to ./workdir/PyPPL. id . tag . suffix / job.index /input/ outdir refers to ./workdir/PyPPL. id . tag . suffix / job.index /output/ pXXX or p refers to a process instantiated from pyppl.Proc class.","title":"Basics and folder structure"},{"location":"basic-concepts-and-directory-structure/#basic-concepts-and-folder-structure","text":"","title":"Basic concepts and folder structure"},{"location":"basic-concepts-and-directory-structure/#layers-of-a-pipeline","text":"The pipeline consists of channels and processes. A process may have many jobs. Each job uses the corresponding elements from the input channel of the process, and generates values for output channel. Actually, what you need to do is just specify the first input channel, and then tell PyPPL the dependencies of the processes. The later processes will use the output channel of the processes they depend on. Of course, you can interfere by using functions in the input specification.","title":"Layers of a pipeline"},{"location":"basic-concepts-and-directory-structure/#folder-structure","text":"./ |-- pipeline.py `-- workdir/ `-- PyPPL. id . tag . suffix / |-- lock |-- proc.settings `-- job.index / |-- input/ |-- output/ |-- job.cache |-- job.script |-- job.pid |-- job.rc |-- job.stdout |-- job.stderr |-- [job.script.local] |-- [job.script.ssh] `-- [job.script.sge] Path Content Memo workdir/ Where the pipeline directories of all processes of current pipeline are located. Can be set by p.ppldir PyPPL. id . tag . suffix / The work directory of current process. The suffix is a unique identify of the process according to its configuration. You may set it by p.workdir proc.settings/ The settings of the process A copy of proc settings job.index / The job directory Starts with 1 job.index /input/ Where you can find the links to all the input files job.index /output/ Where you can find all the output files job.index /job.cache The file containing the signature of the job job.index /job.script To script file to be running job.index /job.pid The id of the job of its running system. Mostly used to tell whether the process is still running. job.index /job.rc To file containing the return code job.index /job.stdout The STDOUT of the script job.index /job.stderr The STDERR of the script job.index /job.script.local The wrapper for local jobs to save return code, stdout and stderr job.index /job.script.ssh The script file for ssh runner job.index /job.script.sge The script file for sge runner Note You are encouraged to set p.ppldir BUT NOT p.workdir , as it contains a unique suffix that is automatically computed.","title":"Folder structure"},{"location":"basic-concepts-and-directory-structure/#symbols-used-in-this-documentation","text":"workdir refers to ./workdir/PyPPL. id . tag . suffix / , indir refers to ./workdir/PyPPL. id . tag . suffix / job.index /input/ outdir refers to ./workdir/PyPPL. id . tag . suffix / job.index /output/ pXXX or p refers to a process instantiated from pyppl.Proc class.","title":"Symbols used in this documentation"},{"location":"caching/","text":"Caching and resuming processes Process caching Once a job is cached, PyPPL will skip running this job. But you have to tell a process how to cache its jobs by setting pXXX.cache with a valid caching method: Caching method ( p.cache=? ) How True A signature * of input files, script and output files of a job is cached in workdir / job.index /job.cache , compare the signature before a job starts to run. False Disable caching, always run jobs. \"export\" First try to find the signatures, if failed, try to restore the files existed (or exported previously in p.exdir ). Hint : p.cache = \"export\" is extremely useful for a process that you only want it to run successfully once, export the result files and never run the process again. You can even delete the workdir of the process, but PyPPL will find the exported files and use them as the input for processes depending on it, so that you don't need to modify the pipeline. One scenario is that you can use it to download some files and never need to download them again. Resuming from processes Sometimes, you may not want to start at the very begining of a pipeline. Then you can resume it from some intermediate processes. To resume pipeline from a process, you have to make sure that the output files of the processes that this process depends on are already generated. Then you can do: PyPPL () . start ( ... ) . resume ( pXXX ) . run () Or if the process uses the data from other processes, especially the output channel, you may need PyPPL to infer (not neccessary run the script) the output data for processes that this process depends on. Then you can do: PyPPL () . start ( ... ) . resume2 ( pXXX ) . run () You may also use a common id to set a set of processes: p1 = Proc ( newid = p , tag = 1st ) p2 = Proc ( newid = p , tag = 2nd ) p3 = Proc ( newid = p , tag = 3rd ) # pipeline will be resumed from p1, p2, p3 PyPPL () . start ( ... ) . resume ( p ) . run () Calculating signatures for caching By default, PyPPL uses the last modified time to generate signatures for files and directories. However, for large directories, it may take notably long time to walk over all the files in those directories. If not necessary, you may simply as PyPPL to get the last modified time for the directories themselves instead of the infiles inside them by setting p.dirsig = False","title":"Caching and resuming processes"},{"location":"caching/#caching-and-resuming-processes","text":"","title":"Caching and resuming processes"},{"location":"caching/#process-caching","text":"Once a job is cached, PyPPL will skip running this job. But you have to tell a process how to cache its jobs by setting pXXX.cache with a valid caching method: Caching method ( p.cache=? ) How True A signature * of input files, script and output files of a job is cached in workdir / job.index /job.cache , compare the signature before a job starts to run. False Disable caching, always run jobs. \"export\" First try to find the signatures, if failed, try to restore the files existed (or exported previously in p.exdir ). Hint : p.cache = \"export\" is extremely useful for a process that you only want it to run successfully once, export the result files and never run the process again. You can even delete the workdir of the process, but PyPPL will find the exported files and use them as the input for processes depending on it, so that you don't need to modify the pipeline. One scenario is that you can use it to download some files and never need to download them again.","title":"Process caching"},{"location":"caching/#resuming-from-processes","text":"Sometimes, you may not want to start at the very begining of a pipeline. Then you can resume it from some intermediate processes. To resume pipeline from a process, you have to make sure that the output files of the processes that this process depends on are already generated. Then you can do: PyPPL () . start ( ... ) . resume ( pXXX ) . run () Or if the process uses the data from other processes, especially the output channel, you may need PyPPL to infer (not neccessary run the script) the output data for processes that this process depends on. Then you can do: PyPPL () . start ( ... ) . resume2 ( pXXX ) . run () You may also use a common id to set a set of processes: p1 = Proc ( newid = p , tag = 1st ) p2 = Proc ( newid = p , tag = 2nd ) p3 = Proc ( newid = p , tag = 3rd ) # pipeline will be resumed from p1, p2, p3 PyPPL () . start ( ... ) . resume ( p ) . run ()","title":"Resuming from processes"},{"location":"caching/#calculating-signatures-for-caching","text":"By default, PyPPL uses the last modified time to generate signatures for files and directories. However, for large directories, it may take notably long time to walk over all the files in those directories. If not necessary, you may simply as PyPPL to get the last modified time for the directories themselves instead of the infiles inside them by setting p.dirsig = False","title":"Calculating signatures for caching"},{"location":"change-log/","text":"Change log Aug 30, 2018: 1.1.1 Allow progress bar to be collapsed in stream log. Remove loky dependency, so PyPPL could run on cygwin/msys2. Add ~/pyppl.yml , ~/pyppl.yaml , ~/pyppl and ~/pyppl.json to default configuration files. Update docs. Fix other bugs. Aug 20, 2018: 1.1.0 Let pipeline halt if any job fails (#33). Add KeyboardInterupt handling. Add Warning message when a process is locked (another instance is running) Clean up utils and runners source codes Support and add tests to travis for python2.7 and python3.3+ for both OSX and Linux. Allow global functions/values to be used in builtin templates. Add shortcut for lambda function in builtin templates. Add nones and transpose method for Channel . July 31, 2018: 1.0.1 Change the default path of flowchart and log from script directory to current directory. Rewrite the way of attribute setting for aggregations. Introduce modules for aggregations. Allow setting attributes from Proc constructor. Implement #33, Ctrl-c now also halts the pipeline, and hides the exception details. July 10, 2018: 1.0.0 ! Fix runner name issue #31. Use mkdocs to generate documentations and host them on GitHub pages. Keep stdout and stderr when a job is cached: #30. Allow command line arguments to overwrite Parameter's type. Host the testing procedures with Travis. Fix other bugs. June 8, 2018: 0.9.6 Auto-delegate common proc config names to aggr Add proc.origin to save the original proc id for copied procs Remove brings, add proc.infile to swith '{{in.(infile)}}' to job.indir path, original path or realpath of the input file Add process lock to avoid the same processes run simultaneously Use built-in Box instead of box.Box Merge template function Rvec and Rlist into R, add repr Fix #29 and #31, and fix other bugs Mar 6, 2018: 0.9.5 Add proc.dirsig to disable/enable calculating signatures from deep directories Add Jobmgr class to handle job distribution Allow channel.rowAt and colAt to return multiple rows and columns, respectively Allow empty channel as input (process will be skipped) Refine all tests Rewrite thread-safe file system helper functions Add specific exception classes Report line # when error happens in template Add progress bar for jobs Allow stdout and stderr file as output Dec 27, 2017: 0.9.4 Add yaml support for config file (#26). Allow empty list for input files. Merge continuous job ids in log (Make the list shorter). More details when internal template failed to render (#25) Ignore .yaml config files if yaml module is not installed. sleep before testing isRunning to avoid all jobs running it at the same time. Use repr to output p.args and p.props. Merge Proc attributes profile and runner. Profile is now an alias of runner, and will be removed finally. Nov 20, 2017: 0.9.3 Beautify parameters help page. Enable multithreading for job construction and cache checking (set by proc.nthread). Uniform multiprocessing/threading. Fix Aggr delegate problems. Add ProcTree to manage process relations. Report processes will not run due to prior processes not ran. Add cclean option for enable/disable cleanup (output check/export) if a job is cached. Add tooltip for flowchart svg. Fix job id not saved for runner_sge. Fix resume assignment issue. Rewrite proc.log function so that logs from jobs do not mess up when they are multithreaded. Fix params loaded from file get overwriten. Add coverage report. Oct 23, 2017: 0.9.2 Add profile for Proc so different procs can run with different profiles. Add delegate for Aggr. Add get, repCol, repRow, rowAt method for Channel. Dont't sleep for batch interval if jobs are cached or running. Add header argument for Channel.fromFile. Fix a bunch of bugs. Oct 6, 2017: 0.9.1 Fix issues reported by codacy Fix an issue checking whether output files generated Deepcopy args and tplenvs when copy a process Refer relative path in p.script (with \"file:\" prefix) where p.script is defined Fix a utils.dictUpdate bug Template function 'Rlist' now can deal with python list Add log for export using move method (previously missed) Allow Aggr instance to set input directly Switch default showing of parameters loaded from object or file to False Optimize utils.varname Add warning if more input data columns than number of input keys Fix output channel key sequence does not keep Use object id instead of name as key for PyPPL nexts/paths in case tag is set in pipeline configrations Sept 22, 2017: 0.9.0 Change class name with first letter capitalized Add resuming from processes (#20) Fix #19 Group log configuration Make flowchart themeable and configurable Make attributes of Proc clearer Add tutorials Make tests more robust Enhancer templating, support Jinja2 Set attributes of processes in aggregation with set Aug 4, 2017: 0.8.1 Add partial echo Add interactive log from the script Add partial export Add log themes and filters Add compare to command line tool Fix bugs Aug 1, 2017: 0.8.0 Add slurm and dry runner Fix bugs when more than 2 input files have same basename Add indent mark for script, specially useful for python Make stdout/stderr flushes out for instant runners when p.echo = True Add sortby , reverse for channel.fromPath Add command line argument parse Fix a bug that threads do not exit after process is done July 18, 2017: 0.7.4 Docs updated (thanks @marchon for some grammer corrections) Some shortcut functions for placeholders added Check running during polling removed Logfile added p.args['key'] can be set also by p.args.key now Bug fixes July 3, 2017: 0.7.3 Config file defaults to ~/.pyppl.json ( ~/.pyppl also works) Callfront added Empty input allowed Same basename name allowed for input files of a job Description of a proc added Aggr Optimized Bug #9 Fixed Private key supported for ssh runner Feature #7 Implemented June 20, 2017: 0.7.2 Optimize isRunning function (using specific job id) Support python3 now Test on OSX More debug information for caching Bug fixes June 15, 2017: 0.7.1 Move pyppl-cli to bin/pyppl channel.collapse now return the most common directory of paths Report oringinal file of input and bring files Show number of omitted logs Bug fixes June 13, 2017: 0.7.0 Add colored log Put jobs in different directories (files with same basename can be used as input files, otherwise it will be overwritten). Add configuration checkrun for pyppl allow runner.isRunning to be disabled (save resources on local machine). Add built-in functions for placeholders; lambda functions do not need to call (just define) File placeholders (.fn, .bn, .prefix, etc) removed, please use built-in functions instead. Add an alias p.ppldir for p.tmpdir to avoid confusion. Update command line tool accordingly Split base runner class into two. May 30, 2017: 0.6.2 Update docs and fix compilation errors from gitbook Change pyppl.dot to pyppl.pyppl.dot; Add channel.fromFile method; Add aggr.addProc method; Fix proc/aggr copy bugs; Fix utils.varname bugs; Fix bugs: channel._tuplize does not change list to tuple any more. Add fold/unfold to channel; cache job immediately after it's done; remove proc in nexts of its depends when its depends are reset; add dir for input files, prefix for output files; Fix utilfs.dirmtime if file not exists; add pyppl-cli; Change rc code, make it consistent with real rc code. Apr 27, 2017: 0.6.1 Overwrite input file if it exists and not the same file; fix varname bug when there are dots in the function name; Add brings feature; Add features to README, and brings to docs Apr 26, 2017: 0.6.0 Set job signature to False if any of the item is False (that means expected files not exists); - Do cache by job itself; Make it possible to cache and export successful jobs even when some jobs failed Host docs in gitbook Init job with log func from proc; Add docstring for API generation; Redefine return code for outfile not generated; Error ignore works now; Rewrite runner_local so it fits other runners to extend; Fix proc depends on mixed list of procs and aggrs Apr 18, 2017: 0.5.0 Fix local runner not waiting (continuiously submitting jobs); Add property alias for aggr; Output cleared if job not cached Fix bugs when export if outfiles are links; change default export method to move; add id and tag to calculate suffix for proc; add timer; add isRunning for job so that even if the main thread quit, we can still retrieve the job status; Apr 13, 2017: 0.4.0 Add files (array) support for input; Recursive update for configuration; Add aggregations; Move functions to utils; Separate run for runners to submit and wait; Add use job class for jobs in a proc; Use \"1,2 3,4\" for channel.fromArgs for multi-width channels; Add rbind, cbind, slice for channel; Add alias for some proc properties; Remove callfront for proc; Add export cache mode; Add gzip export support (#1); Unify loggers; Use job cache instead of proc cache so that a proc can be partly cached; Rewrite buildInput and buildOutput; Use job to construct runners; Mar 14, 2017: 0.2.0 Basic functions Jan 27, 2017: Initiate","title":"Change log"},{"location":"change-log/#change-log","text":"","title":"Change log"},{"location":"change-log/#aug-30-2018-111","text":"Allow progress bar to be collapsed in stream log. Remove loky dependency, so PyPPL could run on cygwin/msys2. Add ~/pyppl.yml , ~/pyppl.yaml , ~/pyppl and ~/pyppl.json to default configuration files. Update docs. Fix other bugs.","title":"Aug 30, 2018: 1.1.1"},{"location":"change-log/#aug-20-2018-110","text":"Let pipeline halt if any job fails (#33). Add KeyboardInterupt handling. Add Warning message when a process is locked (another instance is running) Clean up utils and runners source codes Support and add tests to travis for python2.7 and python3.3+ for both OSX and Linux. Allow global functions/values to be used in builtin templates. Add shortcut for lambda function in builtin templates. Add nones and transpose method for Channel .","title":"Aug 20, 2018: 1.1.0"},{"location":"change-log/#july-31-2018-101","text":"Change the default path of flowchart and log from script directory to current directory. Rewrite the way of attribute setting for aggregations. Introduce modules for aggregations. Allow setting attributes from Proc constructor. Implement #33, Ctrl-c now also halts the pipeline, and hides the exception details.","title":"July 31, 2018: 1.0.1"},{"location":"change-log/#july-10-2018-100","text":"Fix runner name issue #31. Use mkdocs to generate documentations and host them on GitHub pages. Keep stdout and stderr when a job is cached: #30. Allow command line arguments to overwrite Parameter's type. Host the testing procedures with Travis. Fix other bugs.","title":"July 10, 2018: 1.0.0 !"},{"location":"change-log/#june-8-2018-096","text":"Auto-delegate common proc config names to aggr Add proc.origin to save the original proc id for copied procs Remove brings, add proc.infile to swith '{{in.(infile)}}' to job.indir path, original path or realpath of the input file Add process lock to avoid the same processes run simultaneously Use built-in Box instead of box.Box Merge template function Rvec and Rlist into R, add repr Fix #29 and #31, and fix other bugs","title":"June 8, 2018: 0.9.6"},{"location":"change-log/#mar-6-2018-095","text":"Add proc.dirsig to disable/enable calculating signatures from deep directories Add Jobmgr class to handle job distribution Allow channel.rowAt and colAt to return multiple rows and columns, respectively Allow empty channel as input (process will be skipped) Refine all tests Rewrite thread-safe file system helper functions Add specific exception classes Report line # when error happens in template Add progress bar for jobs Allow stdout and stderr file as output","title":"Mar 6, 2018: 0.9.5"},{"location":"change-log/#dec-27-2017-094","text":"Add yaml support for config file (#26). Allow empty list for input files. Merge continuous job ids in log (Make the list shorter). More details when internal template failed to render (#25) Ignore .yaml config files if yaml module is not installed. sleep before testing isRunning to avoid all jobs running it at the same time. Use repr to output p.args and p.props. Merge Proc attributes profile and runner. Profile is now an alias of runner, and will be removed finally.","title":"Dec 27, 2017: 0.9.4"},{"location":"change-log/#nov-20-2017-093","text":"Beautify parameters help page. Enable multithreading for job construction and cache checking (set by proc.nthread). Uniform multiprocessing/threading. Fix Aggr delegate problems. Add ProcTree to manage process relations. Report processes will not run due to prior processes not ran. Add cclean option for enable/disable cleanup (output check/export) if a job is cached. Add tooltip for flowchart svg. Fix job id not saved for runner_sge. Fix resume assignment issue. Rewrite proc.log function so that logs from jobs do not mess up when they are multithreaded. Fix params loaded from file get overwriten. Add coverage report.","title":"Nov 20, 2017: 0.9.3"},{"location":"change-log/#oct-23-2017-092","text":"Add profile for Proc so different procs can run with different profiles. Add delegate for Aggr. Add get, repCol, repRow, rowAt method for Channel. Dont't sleep for batch interval if jobs are cached or running. Add header argument for Channel.fromFile. Fix a bunch of bugs.","title":"Oct 23, 2017: 0.9.2"},{"location":"change-log/#oct-6-2017-091","text":"Fix issues reported by codacy Fix an issue checking whether output files generated Deepcopy args and tplenvs when copy a process Refer relative path in p.script (with \"file:\" prefix) where p.script is defined Fix a utils.dictUpdate bug Template function 'Rlist' now can deal with python list Add log for export using move method (previously missed) Allow Aggr instance to set input directly Switch default showing of parameters loaded from object or file to False Optimize utils.varname Add warning if more input data columns than number of input keys Fix output channel key sequence does not keep Use object id instead of name as key for PyPPL nexts/paths in case tag is set in pipeline configrations","title":"Oct 6, 2017: 0.9.1"},{"location":"change-log/#sept-22-2017-090","text":"Change class name with first letter capitalized Add resuming from processes (#20) Fix #19 Group log configuration Make flowchart themeable and configurable Make attributes of Proc clearer Add tutorials Make tests more robust Enhancer templating, support Jinja2 Set attributes of processes in aggregation with set","title":"Sept 22, 2017: 0.9.0"},{"location":"change-log/#aug-4-2017-081","text":"Add partial echo Add interactive log from the script Add partial export Add log themes and filters Add compare to command line tool Fix bugs","title":"Aug 4, 2017: 0.8.1"},{"location":"change-log/#aug-1-2017-080","text":"Add slurm and dry runner Fix bugs when more than 2 input files have same basename Add indent mark for script, specially useful for python Make stdout/stderr flushes out for instant runners when p.echo = True Add sortby , reverse for channel.fromPath Add command line argument parse Fix a bug that threads do not exit after process is done","title":"Aug 1, 2017: 0.8.0"},{"location":"change-log/#july-18-2017-074","text":"Docs updated (thanks @marchon for some grammer corrections) Some shortcut functions for placeholders added Check running during polling removed Logfile added p.args['key'] can be set also by p.args.key now Bug fixes","title":"July 18, 2017: 0.7.4"},{"location":"change-log/#july-3-2017-073","text":"Config file defaults to ~/.pyppl.json ( ~/.pyppl also works) Callfront added Empty input allowed Same basename name allowed for input files of a job Description of a proc added Aggr Optimized Bug #9 Fixed Private key supported for ssh runner Feature #7 Implemented","title":"July 3, 2017: 0.7.3"},{"location":"change-log/#june-20-2017-072","text":"Optimize isRunning function (using specific job id) Support python3 now Test on OSX More debug information for caching Bug fixes","title":"June 20, 2017: 0.7.2"},{"location":"change-log/#june-15-2017-071","text":"Move pyppl-cli to bin/pyppl channel.collapse now return the most common directory of paths Report oringinal file of input and bring files Show number of omitted logs Bug fixes","title":"June 15, 2017: 0.7.1"},{"location":"change-log/#june-13-2017-070","text":"Add colored log Put jobs in different directories (files with same basename can be used as input files, otherwise it will be overwritten). Add configuration checkrun for pyppl allow runner.isRunning to be disabled (save resources on local machine). Add built-in functions for placeholders; lambda functions do not need to call (just define) File placeholders (.fn, .bn, .prefix, etc) removed, please use built-in functions instead. Add an alias p.ppldir for p.tmpdir to avoid confusion. Update command line tool accordingly Split base runner class into two.","title":"June 13, 2017: 0.7.0"},{"location":"change-log/#may-30-2017-062","text":"Update docs and fix compilation errors from gitbook Change pyppl.dot to pyppl.pyppl.dot; Add channel.fromFile method; Add aggr.addProc method; Fix proc/aggr copy bugs; Fix utils.varname bugs; Fix bugs: channel._tuplize does not change list to tuple any more. Add fold/unfold to channel; cache job immediately after it's done; remove proc in nexts of its depends when its depends are reset; add dir for input files, prefix for output files; Fix utilfs.dirmtime if file not exists; add pyppl-cli; Change rc code, make it consistent with real rc code.","title":"May 30, 2017: 0.6.2"},{"location":"change-log/#apr-27-2017-061","text":"Overwrite input file if it exists and not the same file; fix varname bug when there are dots in the function name; Add brings feature; Add features to README, and brings to docs","title":"Apr 27, 2017: 0.6.1"},{"location":"change-log/#apr-26-2017-060","text":"Set job signature to False if any of the item is False (that means expected files not exists); - Do cache by job itself; Make it possible to cache and export successful jobs even when some jobs failed Host docs in gitbook Init job with log func from proc; Add docstring for API generation; Redefine return code for outfile not generated; Error ignore works now; Rewrite runner_local so it fits other runners to extend; Fix proc depends on mixed list of procs and aggrs","title":"Apr 26, 2017: 0.6.0"},{"location":"change-log/#apr-18-2017-050","text":"Fix local runner not waiting (continuiously submitting jobs); Add property alias for aggr; Output cleared if job not cached Fix bugs when export if outfiles are links; change default export method to move; add id and tag to calculate suffix for proc; add timer; add isRunning for job so that even if the main thread quit, we can still retrieve the job status;","title":"Apr 18, 2017: 0.5.0"},{"location":"change-log/#apr-13-2017-040","text":"Add files (array) support for input; Recursive update for configuration; Add aggregations; Move functions to utils; Separate run for runners to submit and wait; Add use job class for jobs in a proc; Use \"1,2 3,4\" for channel.fromArgs for multi-width channels; Add rbind, cbind, slice for channel; Add alias for some proc properties; Remove callfront for proc; Add export cache mode; Add gzip export support (#1); Unify loggers; Use job cache instead of proc cache so that a proc can be partly cached; Rewrite buildInput and buildOutput; Use job to construct runners;","title":"Apr 13, 2017: 0.4.0"},{"location":"change-log/#mar-14-2017-020","text":"Basic functions","title":"Mar 14, 2017: 0.2.0"},{"location":"change-log/#jan-27-2017-initiate","text":"","title":"Jan 27, 2017: Initiate"},{"location":"channels/","text":"Channels Channels are used to pass data from one process (an instance of Proc ) to another. It is derived from a list , where each element is a tuple . So all python functions/methods that apply on list will also apply on Channel . The length a the tuple corresponds to the number of variables of the input or output of a proc . # v1 v2 v3 c = [ ( a1 , b1 , c1 ), # data for job #0 ( a2 , b2 , c2 ), # data for job #1 # ... ] If we specify this channel to the input of a proc : p = proc () p . input = { v1,v2,v3 : c } Then the values for different variables in different jobs wil be: Job Index v1 v2 v3 0 a1 b1 c1 1 a2 b2 c2 ... ... ... ... Initialize a channel There are several ways to initialize a channel: Note Please use Channel.create(...) instead of Channel(...) unless each element is 'tuplized' properly. Channel . create ([ 1 , 2 , 3 ]) != Channel ([ 1 , 2 , 3 ]) Channel . create ([ 1 , 2 , 3 ]) == Channel ([( 1 ,), ( 2 ,), ( 3 ,)]) From other channels: ch1 = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) ch2 = Channel . create ( a ) ch3 = Channel . create ([ 5 , 6 ]) ch = Channel . fromChannels ( ch1 , ch2 , ch3 ) # channels are column-bound # ch == [(1, 2, a , 5), (3, 4, a , 6)] From a file path pattern: Use glob.glob to grab files by the pattern, you may use different arguments for filter, sort or reverse the list: filter the files with type ( t ): dir , file , link or any (default), sort them by ( sortby ): size , mtime or name (default) reverse the list ( reverse ): False (default, don't reverse) c = Channel . fromPattern ( /a/b/*.txt , t = any , sortby = size , reverse = False ) From file pairs: c = Channel . fromPairs ( /a/b/*.txt ) # the files will be sorted by names and then split into pairs # c == [( /a/b/a1.txt , /a/b/a2.txt ), ( /a/b/b1.txt , /a/b/b2.txt )] From file content: Channel.fromFile(fn, header=False, skip=0, delimit=\"\\t\") For example, we have a file \"chan.txt\" with content: A tab B tab C a1 tab b1 tab c1 a2 tab b2 tab c2 Read the file as a channel: c = Channel . fromFile ( chan.txt ) # c == [( A , B , C ), ( a1 , b1 , c1 ), ( a2 , b2 , c2 )] c = Channel . fromFile ( chan.txt , header = True ) # c == [( a1 , b1 , c1 ), ( a2 , b2 , c2 )] # c.A == [( a1 , ), ( a2 , )] # c.B == [( b1 , ), ( b2 , )] # c.C == [( c1 , ), ( c2 , )] c = Channel . fromFile ( chan.txt , skip = 1 ) # c == [( a1 , b1 , c1 ), ( a2 , b2 , c2 )] From sys.argv (command line arguments): c == channel . fromArgv () # python whatever.py /a/b/*.txt # c == [( /a/b/1.txt ,), ( /a/b/2.txt ,), ( /a/b/3.txt ,), ( /a/b/4.txt ,)] # Make a multple-variable channel: # python whatever.py /a/b/1.txt,/a/b/2.txt /a/b/3.txt,/a/b/4.txt # c == [( /a/b/1.txt , /a/b/2.txt ), ( /a/b/3.txt , /a/b/4.txt )] From command line argument parser: See command line argument parser for details. from PyPPL import Channel , params params . a = a params . b = 2 params . b . type = int params . c = [ 1 , 2 ] params . c . type = list params . d = [ a , b ] params . d . type = list params . e = [] params . e . type = list ch = Channel . fromParams ( c , e ) # Raises ValueError, non-equal length ch = Channel . fromParams ( c , d ) # ch == [(1, a ), (2, b )] ch = Channel . fromParams ( a , b ) # ch == [( a , 2)] Methods for channels Get the length and width of a channel chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) #chan.length() == 2 == len(chan) #chan.width() == 3 Get value from a channel chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) # chan.get() == 1 # chan.get(0) == 1 # chan.get(1) == 2 # chan.get(2) == 3 # chan.get(3) == 4 # chan.get(4) == 5 # chan.get(5) == 6 Repeat rows and columns chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) chan2 = chan . repCol () chan3 = chan . repCol ( n = 3 ) # chan2 == [(1,2,3,1,2,3), (4,5,6,4,5,6)] # chan3 == [(1,2,3,1,2,3,1,2,3), (4,5,6,4,5,6,4,5,6)] chan4 = chan . repRow () chan5 = chan . repRow ( n = 3 ) # chan4 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6)] # chan5 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6), (1,2,3), (4,5,6)] Expand a channel by directory Channel.expand (col= 0, pattern = '*', t='any', sortby='name', reverse=False) Sometimes we prepare files in one process (for example, split a big file into small ones in a directory), then handle these files by different jobs in another process, so that they can be processed simultaneously. Caution expand only works for original channels with length is 1, which will expand to N (number of files included). If original channel has more than 1 element, only first element will be used, and other elements will be ignored. Only the value of the column to be expanded will be changed, values of other columns remain the same. Collapse a channel by files in a common ancestor directory Channel.collapse(col=0) It's basically the reverse process of expand . It applies when you deal with different files and in next process you need them all involved (i.e. combine the results): Caution os.path.dirname(os.path.commonprefix(...)) is used to detect the common ancestor directory, so the files could be ['/a/1/1.file', '/a/2/1.file'] . In this case /a/ will be returned. values at other columns should be the same, PyPPL will NOT check it, the first value at the column will be used. Fetch rows from a channel Channel.rowAt(index) chan1 = Channel . create ([( 1 , 2 , 3 , 4 ), ( 4 , 5 , 6 , 7 )]) chan2 = chan1 . rowAt ( 1 ) # chan2 == [(4,5,6,7)] # Now you can also fetch multiple columus as a channel: chan3 = chan1 . rowAt ([: 2 ]) chan3 == chan1 Fetch columns from a channel Channel.slice(start, length=None) chan1 = Channel . create ([( 1 , 2 , 3 , 4 ), ( 4 , 5 , 6 , 7 )]) chan2 = chan1 . slice ( 1 , 2 ) # chan2 == [(2,3), (5,6)] chan3 = chan1 . slice ( 2 ) # chan3 == [(3,4), (6,7)] chan4 = chan1 . slice ( - 1 ) # chan4 == [(4,), (7,)] Channel.colAt(index) chan . colAt ( index ) == chan . slice ( index , 1 ) # Now you may also fetch multiple columns: chan . colAt ([ 1 , 2 ]) == chan . slice ( 1 , 2 ) Flatten a channel Channel.flatten(col = None) Flatten a channel, make it into a list. chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) f1 = chan . flatten () # f1 == [1,2,3,4,5,6] f2 = chan . flatten ( 1 ) # f1 == [2,5] Split a channel to single-width channels Channel.split(flatten = False) chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) chans = chan . split () # isinstance (chans, list) == True # isinstance (chans, Channel) == False # chans == [ # [(1,), (4,)], # isinstance (chans[0], Channel) == True # [(2,), (5,)], # [(3,), (6,)], # ] chans2 = chan . split ( True ) # chans2 == [ # [1, 4], # isinstance (chans2[0], Channel) == False # [2, 5], # [3, 6], # ] Attach column names Channel.attach(*names) We can attach the column names and then use them to access the columns. ch = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) ch . attach ( col1 , col2 , col3 ) # ch.col1 == [(1,), (4,)] # ch.col2 == [(2,), (5,)] # ch.col3 == [(3,), (6,)] # isinstance(ch.col1, Channel) == True # flatten the columns ch . attach ( col1 , col2 , col3 , True ) # ch.col1 == [1,4] # ch.col2 == [2.5] # ch.col3 == [3,6] # isinstance(ch.col1, Channel) == False Map, filter, reduce Channel.map(func) Channel.mapCol(func, col=0) ch1 = Channel . create () ch2 = Channel . create ([ 1 , 2 , 3 , 4 , 5 ]) ch3 = Channel . create ([( a , 1 ), ( b , 2 )]) # ch1.map(lambda x: (x[0]*x[0],)) == [] # ch2.map(lambda x: (x[0]*x[0],)) == [(1,),(4,),(9,),(16,),(25,)] # ch3.map(lambda x: (x[0], x[1]*x[1])) == [( a , 1), ( b , 4)] # ch1.mapCol(lambda x: x*x) == [] # ch2.mapCol(lambda x: x*x) == [(1,),(4,),(9,),(16,),(25,)] # ch3.mapCol(lambda x: x*x, 1) == [( a , 1), ( b , 4)] # map mapCol return an instance of Channel Channel.filter(func) Channel.filterCol(func, col=0) ch1 = Channel . create ([ ( 1 , 0 , 0 , 1 ), ( a , , b , 0 ), ( True , False , 0 , 1 ), ([], [ 1 ], [ 2 ], [ 0 ]), ]) # Filter by the first column, only first three rows remained ch1 . filterCol () == ch1 [: 3 ] # Filter by the second column, only the last row remained ch1 . filterCol ( col = 1 ) == ch1 [ 3 : 4 ] # Filter by the third column, the 2nd and 4th row remained ch1 . filterCol ( col = 2 ) == [ ch1 [ 1 ], ch1 [ 3 ]] # Filter by the fourth column, all rows remained ch1 . filterCol ( col = 3 ) == ch1 # Filter with a function: ch1 . filter ( lambda x : isinstance ( x [ 2 ], int )) == [ ch1 [ 0 ], ch1 [ 2 ]] # filter filterCol return an instance of Channel Channel.reduce(func) Channel.reduceCol(func, col=0) ch1 = Channel . create () # Raises TypeError, no elements ch1 . reduce ( lambda x , y : x + y ) ch1 = Channel . create ([ 1 , 2 , 3 , 4 , 5 ]) # Notice the different ch1 . reduce ( lambda x , y : x + y ) == ( 1 , 2 , 3 , 4 , 5 ) # x and y are tuples ch1 . reduceCol ( lambda x , y : x + y ) == 15 # x and y are numbers Add rows/columns to a channel Channel.rbind(*rows) Each row can be either a channel, a tuple, a list or a non-iterable element(including string) ch1 = Channel . create () ch2 = Channel . create (( 1 , 2 , 3 )) row1 = Channel . create ( 1 ) row2 = Channel . create (( 2 , 2 , 2 )) row3 = [ 3 ] row4 = ( 3 ,) row5 = ( 4 , 4 , 4 ) row6 = [ 4 , 4 , 4 ] row7 = 5 ch1 . rbind ( row1 ) == [( 1 , )] ch2 . rbind ( row1 ) == [( 1 , 2 , 3 ),( 1 , 1 , 1 )], ch1 . rbind ( row2 ) == [( 2 , 2 , 2 )] ch2 . rbind ( row2 ) == [( 1 , 2 , 3 ), ( 2 , 2 , 2 )] ch1 . rbind ( row3 ) == [( 3 ,)] ch2 . rbind ( row3 ) == [( 1 , 2 , 3 ),( 3 , 3 , 3 )] ch1 . rbind ( row4 ) == [( 3 ,)] ch2 . rbind ( row4 ) == [( 1 , 2 , 3 ),( 3 , 3 , 3 )] ch1 . rbind ( row5 ) == [( 4 , 4 , 4 )] ch2 . rbind ( row5 ) == [( 1 , 2 , 3 ),( 4 , 4 , 4 )] ch1 . rbind ( row6 ) == [( 4 , 4 , 4 )] ch2 . rbind ( row6 ) == [( 1 , 2 , 3 ),( 4 , 4 , 4 )] ch1 . rbind ( row7 ) == [( 5 ,)] ch2 . rbind ( row7 ) == [( 1 , 2 , 3 ),( 5 , 5 , 5 )] Channel.cbind(*cols) ch1 = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) ch2 = Channel . create ([ 5 , 6 ]) ch1 . cbind ( ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch2 = Channel . create ( 5 ) ch1 . cbind ( ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 5 )] ch1 . cbind ([ 5 , 6 ]) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . cbind (( 5 , 6 )) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . cbind ( a ) == [( 1 , 2 , a ), ( 3 , 4 , a )] ch1 = Channel . create () ch2 = Channel . create ([ 21 , 22 ]) ch3 = 3 ch4 = [ 41 , 42 ] ch5 = ( 51 , 52 ) ch6 = a ch1 . cbind ( ch2 , ch3 , ch4 , ch5 , ch6 ) == [( 21 , 3 , 41 , 51 , a ), ( 22 , 3 , 42 , 52 , a )] ch1 . cbind ( ch3 ) . cbind ( ch6 ) == [( 3 , a )] Channel.insert(index, col) ch1 = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) ch2 = Channel . create ([ 5 , 6 ]) ch1 . insert ( 0 , ch2 ) == [( 5 , 1 , 2 ), ( 6 , 3 , 4 )] ch1 . insert ( 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( - 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( None , ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch2 = Channel . create ( 5 ) ch1 . insert ( 0 , ch2 ) == [( 5 , 1 , 2 ), ( 5 , 3 , 4 )] ch1 . insert ( 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 5 , 4 )] ch1 . insert ( - 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 5 , 4 )] ch1 . insert ( None , ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 5 )] ch1 . insert ( 0 , [ 5 , 6 ]) == [( 5 , 1 , 2 ), ( 6 , 3 , 4 )] ch1 . insert ( 1 , [ 5 , 6 ]) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( - 1 , [ 5 , 6 ]) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( None , [ 5 , 6 ]) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . insert ( 0 , ( 5 , 6 )) == [( 5 , 1 , 2 ), ( 6 , 3 , 4 )] ch1 . insert ( 1 , ( 5 , 6 )) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( - 1 , ( 5 , 6 )) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( None , ( 5 , 6 )) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . insert ( 0 , a ) == [( a , 1 , 2 ), ( a , 3 , 4 )] ch1 . insert ( 1 , a ) == [( 1 , a , 2 ), ( 3 , a , 4 )] ch1 . insert ( - 1 , a ) == [( 1 , a , 2 ), ( 3 , a , 4 )] ch1 . insert ( None , a ) == [( 1 , 2 , a ), ( 3 , 4 , a )] self . assertEqual ( ch1 , [( 1 , 2 ), ( 3 , 4 )]) ch1 = Channel . create () ch2 = Channel . create ([ 21 , 22 ]) ch3 = 3 ch4 = [ 41 , 42 ] ch5 = ( 51 , 52 ) ch6 = a # Raises ValueError, when 1 is inserted, it is a 1-width channel, # then you can t insert a 2-width to it. ch1 . insert ( 1 , ch2 ) ch1 . insert ( 0 , ch2 , ch3 , ch4 , ch5 , ch6 ) == [( 21 , 3 , 41 , 51 , a ), ( 22 , 3 , 42 , 52 , a )] Fold a channel Channel.fold(n = 1) Fold a channel , Make a row to n-length chunk rows For example, you have the following channel: a1 a2 a3 a4 b1 b2 b3 b4 After apply chan.fold(2) you will get: a1 a2 a3 a4 b1 b2 b3 b4 Unfold a channel Channel.unfold(n=2) Combine n-rows into one row; do the reverse thing as Channel.fold . But note that the different meaning of n . In fold , n means the length of the chunk that a row is cut to; will in unfold , it means how many rows to combine. Copy a channel Channel.copy()","title":"Channels"},{"location":"channels/#channels","text":"Channels are used to pass data from one process (an instance of Proc ) to another. It is derived from a list , where each element is a tuple . So all python functions/methods that apply on list will also apply on Channel . The length a the tuple corresponds to the number of variables of the input or output of a proc . # v1 v2 v3 c = [ ( a1 , b1 , c1 ), # data for job #0 ( a2 , b2 , c2 ), # data for job #1 # ... ] If we specify this channel to the input of a proc : p = proc () p . input = { v1,v2,v3 : c } Then the values for different variables in different jobs wil be: Job Index v1 v2 v3 0 a1 b1 c1 1 a2 b2 c2 ... ... ... ...","title":"Channels"},{"location":"channels/#initialize-a-channel","text":"There are several ways to initialize a channel: Note Please use Channel.create(...) instead of Channel(...) unless each element is 'tuplized' properly. Channel . create ([ 1 , 2 , 3 ]) != Channel ([ 1 , 2 , 3 ]) Channel . create ([ 1 , 2 , 3 ]) == Channel ([( 1 ,), ( 2 ,), ( 3 ,)]) From other channels: ch1 = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) ch2 = Channel . create ( a ) ch3 = Channel . create ([ 5 , 6 ]) ch = Channel . fromChannels ( ch1 , ch2 , ch3 ) # channels are column-bound # ch == [(1, 2, a , 5), (3, 4, a , 6)] From a file path pattern: Use glob.glob to grab files by the pattern, you may use different arguments for filter, sort or reverse the list: filter the files with type ( t ): dir , file , link or any (default), sort them by ( sortby ): size , mtime or name (default) reverse the list ( reverse ): False (default, don't reverse) c = Channel . fromPattern ( /a/b/*.txt , t = any , sortby = size , reverse = False ) From file pairs: c = Channel . fromPairs ( /a/b/*.txt ) # the files will be sorted by names and then split into pairs # c == [( /a/b/a1.txt , /a/b/a2.txt ), ( /a/b/b1.txt , /a/b/b2.txt )] From file content: Channel.fromFile(fn, header=False, skip=0, delimit=\"\\t\") For example, we have a file \"chan.txt\" with content: A tab B tab C a1 tab b1 tab c1 a2 tab b2 tab c2 Read the file as a channel: c = Channel . fromFile ( chan.txt ) # c == [( A , B , C ), ( a1 , b1 , c1 ), ( a2 , b2 , c2 )] c = Channel . fromFile ( chan.txt , header = True ) # c == [( a1 , b1 , c1 ), ( a2 , b2 , c2 )] # c.A == [( a1 , ), ( a2 , )] # c.B == [( b1 , ), ( b2 , )] # c.C == [( c1 , ), ( c2 , )] c = Channel . fromFile ( chan.txt , skip = 1 ) # c == [( a1 , b1 , c1 ), ( a2 , b2 , c2 )] From sys.argv (command line arguments): c == channel . fromArgv () # python whatever.py /a/b/*.txt # c == [( /a/b/1.txt ,), ( /a/b/2.txt ,), ( /a/b/3.txt ,), ( /a/b/4.txt ,)] # Make a multple-variable channel: # python whatever.py /a/b/1.txt,/a/b/2.txt /a/b/3.txt,/a/b/4.txt # c == [( /a/b/1.txt , /a/b/2.txt ), ( /a/b/3.txt , /a/b/4.txt )] From command line argument parser: See command line argument parser for details. from PyPPL import Channel , params params . a = a params . b = 2 params . b . type = int params . c = [ 1 , 2 ] params . c . type = list params . d = [ a , b ] params . d . type = list params . e = [] params . e . type = list ch = Channel . fromParams ( c , e ) # Raises ValueError, non-equal length ch = Channel . fromParams ( c , d ) # ch == [(1, a ), (2, b )] ch = Channel . fromParams ( a , b ) # ch == [( a , 2)]","title":"Initialize a channel"},{"location":"channels/#methods-for-channels","text":"","title":"Methods for channels"},{"location":"channels/#get-the-length-and-width-of-a-channel","text":"chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) #chan.length() == 2 == len(chan) #chan.width() == 3","title":"Get the length and width of a channel"},{"location":"channels/#get-value-from-a-channel","text":"chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) # chan.get() == 1 # chan.get(0) == 1 # chan.get(1) == 2 # chan.get(2) == 3 # chan.get(3) == 4 # chan.get(4) == 5 # chan.get(5) == 6","title":"Get value from a channel"},{"location":"channels/#repeat-rows-and-columns","text":"chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) chan2 = chan . repCol () chan3 = chan . repCol ( n = 3 ) # chan2 == [(1,2,3,1,2,3), (4,5,6,4,5,6)] # chan3 == [(1,2,3,1,2,3,1,2,3), (4,5,6,4,5,6,4,5,6)] chan4 = chan . repRow () chan5 = chan . repRow ( n = 3 ) # chan4 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6)] # chan5 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6), (1,2,3), (4,5,6)]","title":"Repeat rows and columns"},{"location":"channels/#expand-a-channel-by-directory","text":"Channel.expand (col= 0, pattern = '*', t='any', sortby='name', reverse=False) Sometimes we prepare files in one process (for example, split a big file into small ones in a directory), then handle these files by different jobs in another process, so that they can be processed simultaneously. Caution expand only works for original channels with length is 1, which will expand to N (number of files included). If original channel has more than 1 element, only first element will be used, and other elements will be ignored. Only the value of the column to be expanded will be changed, values of other columns remain the same.","title":"Expand a channel by directory"},{"location":"channels/#collapse-a-channel-by-files-in-a-common-ancestor-directory","text":"Channel.collapse(col=0) It's basically the reverse process of expand . It applies when you deal with different files and in next process you need them all involved (i.e. combine the results): Caution os.path.dirname(os.path.commonprefix(...)) is used to detect the common ancestor directory, so the files could be ['/a/1/1.file', '/a/2/1.file'] . In this case /a/ will be returned. values at other columns should be the same, PyPPL will NOT check it, the first value at the column will be used.","title":"Collapse a channel by files in a common ancestor directory"},{"location":"channels/#fetch-rows-from-a-channel","text":"Channel.rowAt(index) chan1 = Channel . create ([( 1 , 2 , 3 , 4 ), ( 4 , 5 , 6 , 7 )]) chan2 = chan1 . rowAt ( 1 ) # chan2 == [(4,5,6,7)] # Now you can also fetch multiple columus as a channel: chan3 = chan1 . rowAt ([: 2 ]) chan3 == chan1","title":"Fetch rows from a channel"},{"location":"channels/#fetch-columns-from-a-channel","text":"Channel.slice(start, length=None) chan1 = Channel . create ([( 1 , 2 , 3 , 4 ), ( 4 , 5 , 6 , 7 )]) chan2 = chan1 . slice ( 1 , 2 ) # chan2 == [(2,3), (5,6)] chan3 = chan1 . slice ( 2 ) # chan3 == [(3,4), (6,7)] chan4 = chan1 . slice ( - 1 ) # chan4 == [(4,), (7,)] Channel.colAt(index) chan . colAt ( index ) == chan . slice ( index , 1 ) # Now you may also fetch multiple columns: chan . colAt ([ 1 , 2 ]) == chan . slice ( 1 , 2 )","title":"Fetch columns from a channel"},{"location":"channels/#flatten-a-channel","text":"Channel.flatten(col = None) Flatten a channel, make it into a list. chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) f1 = chan . flatten () # f1 == [1,2,3,4,5,6] f2 = chan . flatten ( 1 ) # f1 == [2,5]","title":"Flatten a channel"},{"location":"channels/#split-a-channel-to-single-width-channels","text":"Channel.split(flatten = False) chan = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) chans = chan . split () # isinstance (chans, list) == True # isinstance (chans, Channel) == False # chans == [ # [(1,), (4,)], # isinstance (chans[0], Channel) == True # [(2,), (5,)], # [(3,), (6,)], # ] chans2 = chan . split ( True ) # chans2 == [ # [1, 4], # isinstance (chans2[0], Channel) == False # [2, 5], # [3, 6], # ]","title":"Split a channel to single-width channels"},{"location":"channels/#attach-column-names","text":"Channel.attach(*names) We can attach the column names and then use them to access the columns. ch = Channel . create ([( 1 , 2 , 3 ), ( 4 , 5 , 6 )]) ch . attach ( col1 , col2 , col3 ) # ch.col1 == [(1,), (4,)] # ch.col2 == [(2,), (5,)] # ch.col3 == [(3,), (6,)] # isinstance(ch.col1, Channel) == True # flatten the columns ch . attach ( col1 , col2 , col3 , True ) # ch.col1 == [1,4] # ch.col2 == [2.5] # ch.col3 == [3,6] # isinstance(ch.col1, Channel) == False","title":"Attach column names"},{"location":"channels/#map-filter-reduce","text":"Channel.map(func) Channel.mapCol(func, col=0) ch1 = Channel . create () ch2 = Channel . create ([ 1 , 2 , 3 , 4 , 5 ]) ch3 = Channel . create ([( a , 1 ), ( b , 2 )]) # ch1.map(lambda x: (x[0]*x[0],)) == [] # ch2.map(lambda x: (x[0]*x[0],)) == [(1,),(4,),(9,),(16,),(25,)] # ch3.map(lambda x: (x[0], x[1]*x[1])) == [( a , 1), ( b , 4)] # ch1.mapCol(lambda x: x*x) == [] # ch2.mapCol(lambda x: x*x) == [(1,),(4,),(9,),(16,),(25,)] # ch3.mapCol(lambda x: x*x, 1) == [( a , 1), ( b , 4)] # map mapCol return an instance of Channel Channel.filter(func) Channel.filterCol(func, col=0) ch1 = Channel . create ([ ( 1 , 0 , 0 , 1 ), ( a , , b , 0 ), ( True , False , 0 , 1 ), ([], [ 1 ], [ 2 ], [ 0 ]), ]) # Filter by the first column, only first three rows remained ch1 . filterCol () == ch1 [: 3 ] # Filter by the second column, only the last row remained ch1 . filterCol ( col = 1 ) == ch1 [ 3 : 4 ] # Filter by the third column, the 2nd and 4th row remained ch1 . filterCol ( col = 2 ) == [ ch1 [ 1 ], ch1 [ 3 ]] # Filter by the fourth column, all rows remained ch1 . filterCol ( col = 3 ) == ch1 # Filter with a function: ch1 . filter ( lambda x : isinstance ( x [ 2 ], int )) == [ ch1 [ 0 ], ch1 [ 2 ]] # filter filterCol return an instance of Channel Channel.reduce(func) Channel.reduceCol(func, col=0) ch1 = Channel . create () # Raises TypeError, no elements ch1 . reduce ( lambda x , y : x + y ) ch1 = Channel . create ([ 1 , 2 , 3 , 4 , 5 ]) # Notice the different ch1 . reduce ( lambda x , y : x + y ) == ( 1 , 2 , 3 , 4 , 5 ) # x and y are tuples ch1 . reduceCol ( lambda x , y : x + y ) == 15 # x and y are numbers","title":"Map, filter, reduce"},{"location":"channels/#add-rowscolumns-to-a-channel","text":"Channel.rbind(*rows) Each row can be either a channel, a tuple, a list or a non-iterable element(including string) ch1 = Channel . create () ch2 = Channel . create (( 1 , 2 , 3 )) row1 = Channel . create ( 1 ) row2 = Channel . create (( 2 , 2 , 2 )) row3 = [ 3 ] row4 = ( 3 ,) row5 = ( 4 , 4 , 4 ) row6 = [ 4 , 4 , 4 ] row7 = 5 ch1 . rbind ( row1 ) == [( 1 , )] ch2 . rbind ( row1 ) == [( 1 , 2 , 3 ),( 1 , 1 , 1 )], ch1 . rbind ( row2 ) == [( 2 , 2 , 2 )] ch2 . rbind ( row2 ) == [( 1 , 2 , 3 ), ( 2 , 2 , 2 )] ch1 . rbind ( row3 ) == [( 3 ,)] ch2 . rbind ( row3 ) == [( 1 , 2 , 3 ),( 3 , 3 , 3 )] ch1 . rbind ( row4 ) == [( 3 ,)] ch2 . rbind ( row4 ) == [( 1 , 2 , 3 ),( 3 , 3 , 3 )] ch1 . rbind ( row5 ) == [( 4 , 4 , 4 )] ch2 . rbind ( row5 ) == [( 1 , 2 , 3 ),( 4 , 4 , 4 )] ch1 . rbind ( row6 ) == [( 4 , 4 , 4 )] ch2 . rbind ( row6 ) == [( 1 , 2 , 3 ),( 4 , 4 , 4 )] ch1 . rbind ( row7 ) == [( 5 ,)] ch2 . rbind ( row7 ) == [( 1 , 2 , 3 ),( 5 , 5 , 5 )] Channel.cbind(*cols) ch1 = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) ch2 = Channel . create ([ 5 , 6 ]) ch1 . cbind ( ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch2 = Channel . create ( 5 ) ch1 . cbind ( ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 5 )] ch1 . cbind ([ 5 , 6 ]) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . cbind (( 5 , 6 )) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . cbind ( a ) == [( 1 , 2 , a ), ( 3 , 4 , a )] ch1 = Channel . create () ch2 = Channel . create ([ 21 , 22 ]) ch3 = 3 ch4 = [ 41 , 42 ] ch5 = ( 51 , 52 ) ch6 = a ch1 . cbind ( ch2 , ch3 , ch4 , ch5 , ch6 ) == [( 21 , 3 , 41 , 51 , a ), ( 22 , 3 , 42 , 52 , a )] ch1 . cbind ( ch3 ) . cbind ( ch6 ) == [( 3 , a )] Channel.insert(index, col) ch1 = Channel . create ([( 1 , 2 ), ( 3 , 4 )]) ch2 = Channel . create ([ 5 , 6 ]) ch1 . insert ( 0 , ch2 ) == [( 5 , 1 , 2 ), ( 6 , 3 , 4 )] ch1 . insert ( 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( - 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( None , ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch2 = Channel . create ( 5 ) ch1 . insert ( 0 , ch2 ) == [( 5 , 1 , 2 ), ( 5 , 3 , 4 )] ch1 . insert ( 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 5 , 4 )] ch1 . insert ( - 1 , ch2 ) == [( 1 , 5 , 2 ), ( 3 , 5 , 4 )] ch1 . insert ( None , ch2 ) == [( 1 , 2 , 5 ), ( 3 , 4 , 5 )] ch1 . insert ( 0 , [ 5 , 6 ]) == [( 5 , 1 , 2 ), ( 6 , 3 , 4 )] ch1 . insert ( 1 , [ 5 , 6 ]) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( - 1 , [ 5 , 6 ]) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( None , [ 5 , 6 ]) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . insert ( 0 , ( 5 , 6 )) == [( 5 , 1 , 2 ), ( 6 , 3 , 4 )] ch1 . insert ( 1 , ( 5 , 6 )) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( - 1 , ( 5 , 6 )) == [( 1 , 5 , 2 ), ( 3 , 6 , 4 )] ch1 . insert ( None , ( 5 , 6 )) == [( 1 , 2 , 5 ), ( 3 , 4 , 6 )] ch1 . insert ( 0 , a ) == [( a , 1 , 2 ), ( a , 3 , 4 )] ch1 . insert ( 1 , a ) == [( 1 , a , 2 ), ( 3 , a , 4 )] ch1 . insert ( - 1 , a ) == [( 1 , a , 2 ), ( 3 , a , 4 )] ch1 . insert ( None , a ) == [( 1 , 2 , a ), ( 3 , 4 , a )] self . assertEqual ( ch1 , [( 1 , 2 ), ( 3 , 4 )]) ch1 = Channel . create () ch2 = Channel . create ([ 21 , 22 ]) ch3 = 3 ch4 = [ 41 , 42 ] ch5 = ( 51 , 52 ) ch6 = a # Raises ValueError, when 1 is inserted, it is a 1-width channel, # then you can t insert a 2-width to it. ch1 . insert ( 1 , ch2 ) ch1 . insert ( 0 , ch2 , ch3 , ch4 , ch5 , ch6 ) == [( 21 , 3 , 41 , 51 , a ), ( 22 , 3 , 42 , 52 , a )]","title":"Add rows/columns to a channel"},{"location":"channels/#fold-a-channel","text":"Channel.fold(n = 1) Fold a channel , Make a row to n-length chunk rows For example, you have the following channel: a1 a2 a3 a4 b1 b2 b3 b4 After apply chan.fold(2) you will get: a1 a2 a3 a4 b1 b2 b3 b4","title":"Fold a channel"},{"location":"channels/#unfold-a-channel","text":"Channel.unfold(n=2) Combine n-rows into one row; do the reverse thing as Channel.fold . But note that the different meaning of n . In fold , n means the length of the chunk that a row is cut to; will in unfold , it means how many rows to combine.","title":"Unfold a channel"},{"location":"channels/#copy-a-channel","text":"Channel.copy()","title":"Copy a channel"},{"location":"command-line-argument-parser/","text":"Command line argument parser This module is just for your convenience. You are free to use Python's built-in argparse module or just use sys.argv . To start with, just import it from PyPPL : from pyppl import params Add an option params . opt = a # or # params.opt.setValue( a ) params . opt2 . setType ( list ) . setRequired () # then don t forget to parse the command line arguments: params . parse () Then python pipeline.py -opt b -opt2 1 2 3 will overwrite the value. list option can also be specified as -opt2 1 -opt2 2 -opt2 3 To use the value: var = params . opt . value + 2 # var is b2 var2 = params . opt2 . value + [ 4 ] # var2 is [1, 2, 3, 4] If you feel annoying using params.xxx.value to have the value of the option, you can convert the params object to a Box object (inherited from OrderedDict ): params . parse () ps = params . asDict () # or just # ps = params.parse() Then you can use params.xxx to get the value directly: var = ps . opt + 2 var2 = ps . opt2 + [ 4 ] Set attributes of an option An option has server properties: - desc : The description of the option, shows in the help page. Default: [] - required : Whether the option is required. Default: False - show : Whether this option should be shown in the help page. Default: True - type : The type of the option value. Default: str - name : The name of the option, then in command line, - name refers to the option. - value : The default value of the option. You can either set the value of an option by params . opt . required = True or params . opt . setRequired ( True ) # params.opt.setDesc( Desctipion ) # params.opt.setShow(False) # params.opt.setType(list) # params.opt.setName( opt1 ) # params.opt.setValue([1,2,3]) # or they can be chained: # params.opt.setDesc( Desctipion ) \\ # .setShow(False) \\ # .setType(list) \\ # .setName( opt1 ) \\ # .setValue([1,2,3]) About type Declear the type of an option You can use either the type itself or the type name: params.opt.type = int or params.opt.type = 'int' Infer type when option initialized When you initialize an option: # with nothing specified params . opt # with a default value params . opt = 1 The type will be inferred from the value. In the first case, the type is None , will in the second, it's 'int' Note Let it initialize an option first: params.opt , then when the value is set explictly: params.opt.value = \"a\" Or when the value replaced (set value after initialization): params.opt = \"a\" The type will not be changed (it's None in this case). Allowed types Literally, allowed types are str , int , float , bool and list . But we allow subtypes (types of elements) for list . By default, the value of list elements will be recognized automatically. For example, '1' will be recognized as an integer 1 , and \"True\" will be recognized as bool value True . You can avoid this by specified the subtypes explictly: params.opt.type = 'list:str' , then '1' will be kept as '1' rather than 1 , and \"True\" will be \"True\" instead of True . You can use shortcuts for the types: i - int f - float b - bool s - str l - list array - list For subtypes, you can also do params.opt.type = 'l:s' indicating list:str . Or you can even mix them: params.opt.type = 'l:str' or params.opt.type = 'list:s' Overwrite the type from command line Even though we may declear the type of an option by params.opt.type = 'list:str' , the users are able to overwrite it by pass the type through command argument: program -opt:list:int 1 2 3 Then we will get: params.opt.value == [1,2,3] instead of params.opt.value == ['1', '2', '3'] when we do: program -opt 1 2 3 Flexibly, we can have mixed types of elements in a list option: program -opt:list:bool 1 -opt:list:int 2 -opt:list:str 3 We will get: params.opt.value == [True, 2, '3'] Load params from a dict You can define a dict , and then load it to params : d2load = { p1 : 1 , p1.required : True , p2 : [ 1 , 2 , 3 ], p2.show : True , p3 : 2.3 , p3.desc : The p3 params , p3.required : True , p3.show : True } params . loadDict ( d2load ) # params.p1.value == 1 # params.p1.required == True # params.p1.show == False # params.p1.type == int # params.p2.value == [1,2,3] # params.p2.required == False # params.p2.show == True # params.p2.type == list # params.p3.value == 2.3 # params.p3.required == True # params.p3.show == False # params.p3.type == float Note that by default, the options from the dict will be hidden from the help page. If you want to show some of them, just set p2.show = True , or you want to show them all: params.loadDict(d2load, show = True) Load params from a configuration file If the configuration file has a name ending with '.json', then json.load will be used to load the file into a dict , and params.loadDict will be used to load it to params Else python's ConfigParse will be used. All params in the configuration file should be under one section with whatever name: [Config] p1: 1 p1.required: True p2: [1,2,3] p2.show: True p3: 2.3 p3.desc = The p3 params p3.required: True p3.show: True Similarly, you can set the default value for show property by: params.loadCfgfile(\"params.config\", show=True) Hint params is a singleton of Parameters . It's a combination of configuration and command line arguments. So you are able to load the configurations from files, which will be used as default values, before you parse the command line arguments. You are also able to choose some of the options for the users to pass value to, and some hidden from the users. Preseved option names We have a certain convention of the option names used with params : - Make sure it's only composed of alphabetics, underscores and hyphens. - Make sure it starts with alphabetics. - Make sure it's not one of these words ( help , parse , loadDict , loadFile and asDict ) Set other attributes of params Show the usages/example/description of the program params._setUsage([\"{prog} -h\", \"{prog} -infile infile [options]\"]) params._setDesc([\"This program does this.\", \"This program also does that.\"]) params._setHbald(True) # print help if no arguments passed Or params._usage = [\"{prog} -h\", \"{prog} -infile infile [options]\"] params._desc = [\"This program does this.\", \"This program also does that.\"] params._hbald = True Notice the leading underscore of the attribute name, this makes sure usage , desc and hbald still avaiable to be used as option names. Set the prefix of option names By default, the option names are recognized from the command line if they follow the convention and start with - . You may change it, let's say you want the option names to start with -- : params('prefix', '--') The only prog --opt 1 will be parsed, instead of prog -opt 1 to get the value by params.opt.value Note You cannot use an empty prefix. Set the default help options By default, the help options are ['-h', '--help', '-H', '-?'] . params._hopts = '-h, --help, -H, -?' or params.setHopts(['-h', '--help', '-H', '-?']) Hint These statements can also be chained: params._setUsage([\"{prog} -h\", \"{prog} -infile infile [options]\"]) \\ ._setPrefix('--') \\ ._setHopts(['-H', '-?']) Positional options Positional options are activated when you set the descriptions of them: params . _ . desc = The positional option If you want to use a different variable to represent positional option: from pyppl import Parameters Parameters . POSITIONAL = POSITIONAL params . POSITIONAL . desc = The positional option Note The default type of positional option is list . The values can be appeared in the middle of the command line: prog -a 1 pos1 -b 2 pos2 , then params._ == ['pos1', 'pos2'] Support of subcommands Add a subcommand from pyppl import commands commands . list = list work directories under wdir commands.list becomes an instance of Parameters , you may do anything like what we did for params . For example, add an option for the command: commands . list . wdir = ./workdir commands . list . wdir . desc = The ppldir containing process work directories. To parse the arguments: command , params = commands . parse () # command: The command # params : The parsed arguments, a Box object # To get the value of an option: params.wdir # if you want to get the Parameter objects: # params.wdir == commands[command].wdir.value Use a different 'help' command By default, you may use prog help command to print the help information for the command. Alternatively, you can change it by: commands . _hcmd = h # prog h command Use a different theme for help page There are three built-in themes: default , blue and plain , to switch them: from pyppl import Commands commands = Commands ( theme = blue ) # if you don t like the colors, just use the plain theme You may also use different theme for params : from pyppl import Parameters params = Parameters ( theme = blue ) Use you own theme: Instead of passing a theme name to Commands/Parameters , you may also pass the entire theme: from pyppl.logger import COLORS commands = Commands ( theme = { # the color for errors error = COLORS . red , # the color for warnings warning = COLORS . yellow , # the color for the title of each section title = COLORS . bold + COLORS . underline + COLORS . cyan , # the color for the programe name in any sections prog = COLORS . bold + COLORS . green , # the color for default values default = COLORS . magenta , # the color for option names optname = COLORS . bold + COLORS . green , # the color for option types or placeholders opttype = COLORS . blue , # the color for option descriptions optdesc = # leave it as default })","title":"Command line argument parser"},{"location":"command-line-argument-parser/#command-line-argument-parser","text":"This module is just for your convenience. You are free to use Python's built-in argparse module or just use sys.argv . To start with, just import it from PyPPL : from pyppl import params","title":"Command line argument parser"},{"location":"command-line-argument-parser/#add-an-option","text":"params . opt = a # or # params.opt.setValue( a ) params . opt2 . setType ( list ) . setRequired () # then don t forget to parse the command line arguments: params . parse () Then python pipeline.py -opt b -opt2 1 2 3 will overwrite the value. list option can also be specified as -opt2 1 -opt2 2 -opt2 3 To use the value: var = params . opt . value + 2 # var is b2 var2 = params . opt2 . value + [ 4 ] # var2 is [1, 2, 3, 4] If you feel annoying using params.xxx.value to have the value of the option, you can convert the params object to a Box object (inherited from OrderedDict ): params . parse () ps = params . asDict () # or just # ps = params.parse() Then you can use params.xxx to get the value directly: var = ps . opt + 2 var2 = ps . opt2 + [ 4 ]","title":"Add an option"},{"location":"command-line-argument-parser/#set-attributes-of-an-option","text":"An option has server properties: - desc : The description of the option, shows in the help page. Default: [] - required : Whether the option is required. Default: False - show : Whether this option should be shown in the help page. Default: True - type : The type of the option value. Default: str - name : The name of the option, then in command line, - name refers to the option. - value : The default value of the option. You can either set the value of an option by params . opt . required = True or params . opt . setRequired ( True ) # params.opt.setDesc( Desctipion ) # params.opt.setShow(False) # params.opt.setType(list) # params.opt.setName( opt1 ) # params.opt.setValue([1,2,3]) # or they can be chained: # params.opt.setDesc( Desctipion ) \\ # .setShow(False) \\ # .setType(list) \\ # .setName( opt1 ) \\ # .setValue([1,2,3])","title":"Set attributes of an option"},{"location":"command-line-argument-parser/#about-type","text":"","title":"About type"},{"location":"command-line-argument-parser/#declear-the-type-of-an-option","text":"You can use either the type itself or the type name: params.opt.type = int or params.opt.type = 'int'","title":"Declear the type of an option"},{"location":"command-line-argument-parser/#infer-type-when-option-initialized","text":"When you initialize an option: # with nothing specified params . opt # with a default value params . opt = 1 The type will be inferred from the value. In the first case, the type is None , will in the second, it's 'int' Note Let it initialize an option first: params.opt , then when the value is set explictly: params.opt.value = \"a\" Or when the value replaced (set value after initialization): params.opt = \"a\" The type will not be changed (it's None in this case).","title":"Infer type when option initialized"},{"location":"command-line-argument-parser/#allowed-types","text":"Literally, allowed types are str , int , float , bool and list . But we allow subtypes (types of elements) for list . By default, the value of list elements will be recognized automatically. For example, '1' will be recognized as an integer 1 , and \"True\" will be recognized as bool value True . You can avoid this by specified the subtypes explictly: params.opt.type = 'list:str' , then '1' will be kept as '1' rather than 1 , and \"True\" will be \"True\" instead of True . You can use shortcuts for the types: i - int f - float b - bool s - str l - list array - list For subtypes, you can also do params.opt.type = 'l:s' indicating list:str . Or you can even mix them: params.opt.type = 'l:str' or params.opt.type = 'list:s'","title":"Allowed types"},{"location":"command-line-argument-parser/#overwrite-the-type-from-command-line","text":"Even though we may declear the type of an option by params.opt.type = 'list:str' , the users are able to overwrite it by pass the type through command argument: program -opt:list:int 1 2 3 Then we will get: params.opt.value == [1,2,3] instead of params.opt.value == ['1', '2', '3'] when we do: program -opt 1 2 3 Flexibly, we can have mixed types of elements in a list option: program -opt:list:bool 1 -opt:list:int 2 -opt:list:str 3 We will get: params.opt.value == [True, 2, '3']","title":"Overwrite the type from command line"},{"location":"command-line-argument-parser/#load-params-from-a-dict","text":"You can define a dict , and then load it to params : d2load = { p1 : 1 , p1.required : True , p2 : [ 1 , 2 , 3 ], p2.show : True , p3 : 2.3 , p3.desc : The p3 params , p3.required : True , p3.show : True } params . loadDict ( d2load ) # params.p1.value == 1 # params.p1.required == True # params.p1.show == False # params.p1.type == int # params.p2.value == [1,2,3] # params.p2.required == False # params.p2.show == True # params.p2.type == list # params.p3.value == 2.3 # params.p3.required == True # params.p3.show == False # params.p3.type == float Note that by default, the options from the dict will be hidden from the help page. If you want to show some of them, just set p2.show = True , or you want to show them all: params.loadDict(d2load, show = True)","title":"Load params from a dict"},{"location":"command-line-argument-parser/#load-params-from-a-configuration-file","text":"If the configuration file has a name ending with '.json', then json.load will be used to load the file into a dict , and params.loadDict will be used to load it to params Else python's ConfigParse will be used. All params in the configuration file should be under one section with whatever name: [Config] p1: 1 p1.required: True p2: [1,2,3] p2.show: True p3: 2.3 p3.desc = The p3 params p3.required: True p3.show: True Similarly, you can set the default value for show property by: params.loadCfgfile(\"params.config\", show=True) Hint params is a singleton of Parameters . It's a combination of configuration and command line arguments. So you are able to load the configurations from files, which will be used as default values, before you parse the command line arguments. You are also able to choose some of the options for the users to pass value to, and some hidden from the users.","title":"Load params from a configuration file"},{"location":"command-line-argument-parser/#preseved-option-names","text":"We have a certain convention of the option names used with params : - Make sure it's only composed of alphabetics, underscores and hyphens. - Make sure it starts with alphabetics. - Make sure it's not one of these words ( help , parse , loadDict , loadFile and asDict )","title":"Preseved option names"},{"location":"command-line-argument-parser/#set-other-attributes-of-params","text":"","title":"Set other attributes of params"},{"location":"command-line-argument-parser/#show-the-usagesexampledescription-of-the-program","text":"params._setUsage([\"{prog} -h\", \"{prog} -infile infile [options]\"]) params._setDesc([\"This program does this.\", \"This program also does that.\"]) params._setHbald(True) # print help if no arguments passed Or params._usage = [\"{prog} -h\", \"{prog} -infile infile [options]\"] params._desc = [\"This program does this.\", \"This program also does that.\"] params._hbald = True Notice the leading underscore of the attribute name, this makes sure usage , desc and hbald still avaiable to be used as option names.","title":"Show the usages/example/description of the program"},{"location":"command-line-argument-parser/#set-the-prefix-of-option-names","text":"By default, the option names are recognized from the command line if they follow the convention and start with - . You may change it, let's say you want the option names to start with -- : params('prefix', '--') The only prog --opt 1 will be parsed, instead of prog -opt 1 to get the value by params.opt.value Note You cannot use an empty prefix.","title":"Set the prefix of option names"},{"location":"command-line-argument-parser/#set-the-default-help-options","text":"By default, the help options are ['-h', '--help', '-H', '-?'] . params._hopts = '-h, --help, -H, -?' or params.setHopts(['-h', '--help', '-H', '-?']) Hint These statements can also be chained: params._setUsage([\"{prog} -h\", \"{prog} -infile infile [options]\"]) \\ ._setPrefix('--') \\ ._setHopts(['-H', '-?'])","title":"Set the default help options"},{"location":"command-line-argument-parser/#positional-options","text":"Positional options are activated when you set the descriptions of them: params . _ . desc = The positional option If you want to use a different variable to represent positional option: from pyppl import Parameters Parameters . POSITIONAL = POSITIONAL params . POSITIONAL . desc = The positional option Note The default type of positional option is list . The values can be appeared in the middle of the command line: prog -a 1 pos1 -b 2 pos2 , then params._ == ['pos1', 'pos2']","title":"Positional options"},{"location":"command-line-argument-parser/#support-of-subcommands","text":"","title":"Support of subcommands"},{"location":"command-line-argument-parser/#add-a-subcommand","text":"from pyppl import commands commands . list = list work directories under wdir commands.list becomes an instance of Parameters , you may do anything like what we did for params . For example, add an option for the command: commands . list . wdir = ./workdir commands . list . wdir . desc = The ppldir containing process work directories. To parse the arguments: command , params = commands . parse () # command: The command # params : The parsed arguments, a Box object # To get the value of an option: params.wdir # if you want to get the Parameter objects: # params.wdir == commands[command].wdir.value","title":"Add a subcommand"},{"location":"command-line-argument-parser/#use-a-different-help-command","text":"By default, you may use prog help command to print the help information for the command. Alternatively, you can change it by: commands . _hcmd = h # prog h command","title":"Use a different 'help' command"},{"location":"command-line-argument-parser/#use-a-different-theme-for-help-page","text":"There are three built-in themes: default , blue and plain , to switch them: from pyppl import Commands commands = Commands ( theme = blue ) # if you don t like the colors, just use the plain theme You may also use different theme for params : from pyppl import Parameters params = Parameters ( theme = blue )","title":"Use a different theme for help page"},{"location":"command-line-argument-parser/#use-you-own-theme","text":"Instead of passing a theme name to Commands/Parameters , you may also pass the entire theme: from pyppl.logger import COLORS commands = Commands ( theme = { # the color for errors error = COLORS . red , # the color for warnings warning = COLORS . yellow , # the color for the title of each section title = COLORS . bold + COLORS . underline + COLORS . cyan , # the color for the programe name in any sections prog = COLORS . bold + COLORS . green , # the color for default values default = COLORS . magenta , # the color for option names optname = COLORS . bold + COLORS . green , # the color for option types or placeholders opttype = COLORS . blue , # the color for option descriptions optdesc = # leave it as default })","title":"Use you own theme:"},{"location":"command-line-tool/","text":"Command line tool When you are debuggin a processes, specially when you are modify input, output and script, the suffix a process will change. Then there will be several workdir created in ppldir . The command line tool helps to maintain and clean up the workdir s bin/pyppl DESCRIPTION: PyPPL command line tool COMMANDS: list - list work directories under wdir clean - remove some work directories compare - compare two processes from different directories help COMMAND - Print help information for the command List processes pyppl list command will list the processes in ./workdir . It will group the processes with same id and tag , and compare their time start to run. The latest one will show at the first place, follows the second latest, ... If a proc.settings cannot be found in the process directory, it will be shown in red. Clean processes pyppl clean command will ask whether you want to remove the process directory for the older processes with the same id and tag . You can remove all those older process directories without confirmation by pyppl clean -force Caution Be careful when you are using pyppl clean -force , which will remove all workdirs in wdir Compare the settings of two pipeines pyppl compare uses python's difflib to compare the proc.settings files in the directories of two processes. it can take a process group name (i.e. -pro pSort.notag , in this case, actually, the tag can be omitted if it is notag , so you can use -pro pSort ) to compare the top 2 latest processes or two process names with suffices (i.e. -proc1 pSort.notag.4HIhyVbp -proc2 pSort.notag.7hNBe2uT .","title":"Command line tool"},{"location":"command-line-tool/#command-line-tool","text":"When you are debuggin a processes, specially when you are modify input, output and script, the suffix a process will change. Then there will be several workdir created in ppldir . The command line tool helps to maintain and clean up the workdir s bin/pyppl DESCRIPTION: PyPPL command line tool COMMANDS: list - list work directories under wdir clean - remove some work directories compare - compare two processes from different directories help COMMAND - Print help information for the command","title":"Command line tool"},{"location":"command-line-tool/#list-processes","text":"pyppl list command will list the processes in ./workdir . It will group the processes with same id and tag , and compare their time start to run. The latest one will show at the first place, follows the second latest, ... If a proc.settings cannot be found in the process directory, it will be shown in red.","title":"List processes"},{"location":"command-line-tool/#clean-processes","text":"pyppl clean command will ask whether you want to remove the process directory for the older processes with the same id and tag . You can remove all those older process directories without confirmation by pyppl clean -force Caution Be careful when you are using pyppl clean -force , which will remove all workdirs in wdir","title":"Clean processes"},{"location":"command-line-tool/#compare-the-settings-of-two-pipeines","text":"pyppl compare uses python's difflib to compare the proc.settings files in the directories of two processes. it can take a process group name (i.e. -pro pSort.notag , in this case, actually, the tag can be omitted if it is notag , so you can use -pro pSort ) to compare the top 2 latest processes or two process names with suffices (i.e. -proc1 pSort.notag.4HIhyVbp -proc2 pSort.notag.7hNBe2uT .","title":"Compare the settings of two pipeines"},{"location":"configure-a-pipeline/","text":"Full pipeline configuration To configure your pipeline, you just pass the configurations (a dict ) to the constructor: ppl = PyPPL ( config ) Here is the full structure of the configurations ( yaml configuration file is also supported since 0.9.4 ): { _log : { levels : basic , // the log levels theme : true , // use colored log information lvldiff : [ +DEBUG ], // modify the loglevels group file : false , // disable logfile, or specify a different logfile }, _flowchart : { theme : default , dot : dot -Tsvg {{dotfile}} -o {{fcfile}} }, proc : { // shared configuration of processes forks : 10 , runner : sge , sgeRunner : { // sge options } }, profile1 : { forks : 20 , runner : ssh , sshRunner : { // ssh options } }, profile2 : { ... }, profile3 : { ... }, ... } - For log configuration please refer to configure your logs - For flowchart configuration please refer to pipeline flowchart - proc defines the base running profile for processes in this pipeline. All the properties of a process can be set here, but just some common one are recommended. Obviously, input is not suitable to be set here, except some extreme cases. - profiles defines some profiles that may be shared by the processes. To use a profile, just specify the profile name to run : PyPPL(config).start(process).run( profile ) . Note You may also use the runner name as a profile. That means, following profiles are implied in the configuration: { sge : { runner : sge }, ssh : { runner : ssh }, slurm : { runner : slurm }, local : { runner : local }, dry : { runner : dry }, } Caution You cannot define profiles with names _flowchart and _log Priority of configuration options See here for use of configuration files. Now you have 3 ways to set attributes for a process: - directly set the process attributes (1) , - set in the first argument ( config ) of PyPPL constructor (2) , - set in a configuration file /a/b/pyppl.config.json (3) , - set in configuration file ~/.PyPPL.json (4) , and - set in configuration file ~/.PyPPL (5) The priority is: (1) (2) (3) (4) (5). Once you set the property of the process, it will never be changed by PyPPL constructor or the configuration file. But the first argument can overwrite the options in configuration files. Here are an examples to illustrate the priority: Example 1: # ~/.PyPPL.json { proc : { forks : 5} } p = Proc () p . forks = 1 ppl = PyPPL ({ proc : { forks : 10 }}) # p.forks == 1 Example 2: # we also have ~/.PyPPL.json as previous example p = Proc () ppl = PyPPL ({ proc : { forks : 10 }}) # p.forks == 10 Example 3: # we also have ~/.PyPPL.json as previous example p = Proc () ppl = PyPPL () # p.forks == 5 Caution If a process is depending on other processes, you are not supposed to set it as starting process. Of course you can, but make sure the input channel can be normally constructed. If a process is not depending on any other processes, you have to set it as starting process. Otherwise, it won't start to run.","title":"Pipeline configuration"},{"location":"configure-a-pipeline/#full-pipeline-configuration","text":"To configure your pipeline, you just pass the configurations (a dict ) to the constructor: ppl = PyPPL ( config ) Here is the full structure of the configurations ( yaml configuration file is also supported since 0.9.4 ): { _log : { levels : basic , // the log levels theme : true , // use colored log information lvldiff : [ +DEBUG ], // modify the loglevels group file : false , // disable logfile, or specify a different logfile }, _flowchart : { theme : default , dot : dot -Tsvg {{dotfile}} -o {{fcfile}} }, proc : { // shared configuration of processes forks : 10 , runner : sge , sgeRunner : { // sge options } }, profile1 : { forks : 20 , runner : ssh , sshRunner : { // ssh options } }, profile2 : { ... }, profile3 : { ... }, ... } - For log configuration please refer to configure your logs - For flowchart configuration please refer to pipeline flowchart - proc defines the base running profile for processes in this pipeline. All the properties of a process can be set here, but just some common one are recommended. Obviously, input is not suitable to be set here, except some extreme cases. - profiles defines some profiles that may be shared by the processes. To use a profile, just specify the profile name to run : PyPPL(config).start(process).run( profile ) . Note You may also use the runner name as a profile. That means, following profiles are implied in the configuration: { sge : { runner : sge }, ssh : { runner : ssh }, slurm : { runner : slurm }, local : { runner : local }, dry : { runner : dry }, } Caution You cannot define profiles with names _flowchart and _log","title":"Full pipeline configuration"},{"location":"configure-a-pipeline/#priority-of-configuration-options","text":"See here for use of configuration files. Now you have 3 ways to set attributes for a process: - directly set the process attributes (1) , - set in the first argument ( config ) of PyPPL constructor (2) , - set in a configuration file /a/b/pyppl.config.json (3) , - set in configuration file ~/.PyPPL.json (4) , and - set in configuration file ~/.PyPPL (5) The priority is: (1) (2) (3) (4) (5). Once you set the property of the process, it will never be changed by PyPPL constructor or the configuration file. But the first argument can overwrite the options in configuration files. Here are an examples to illustrate the priority: Example 1: # ~/.PyPPL.json { proc : { forks : 5} } p = Proc () p . forks = 1 ppl = PyPPL ({ proc : { forks : 10 }}) # p.forks == 1 Example 2: # we also have ~/.PyPPL.json as previous example p = Proc () ppl = PyPPL ({ proc : { forks : 10 }}) # p.forks == 10 Example 3: # we also have ~/.PyPPL.json as previous example p = Proc () ppl = PyPPL () # p.forks == 5 Caution If a process is depending on other processes, you are not supposed to set it as starting process. Of course you can, but make sure the input channel can be normally constructed. If a process is not depending on any other processes, you have to set it as starting process. Otherwise, it won't start to run.","title":"Priority of configuration options"},{"location":"configure-your-logs/","text":"Configure your logs PyPPL has fancy logs. You can define how they look like (theme) and what messages to show (levels). Built-in log themes We have some built-in themes: greenOnBlack (default): greenOnWhite: blueOnBlack: blueOnWhite: magentaOnBlack: magentaOnWhite: If you don't like them, you can also disable them: To use them, just specify the name in your pipeline configuration file: { _log : { theme : magentaOnWhite } } Or when you initialize a pipeline: PyPPL ({ _log : { theme : magentaOnWhite }}) . start ( ... ) . run () If you want to disable the theme, just set \"theme\" to False ( false for json ) If you set theme to True , then default theme greenOnBlack is used. Levels of pyppl logs Please note that the levels are different from those of python's logging module. For logging module has 6 levels , with different int values. However, pyppl's log has many levels, or more suitable, flags, which don't have corresponding values. They are somehow equal, but some of them always print out unless you ask them not to. Note The log levels are a little bit different from here, please see debug your script . You may also specify the group name in your pipeline configuration file: { _log : { levels : nodebug }, // running profiles ... } Or when you initialize a pipeline: PyPPL ({ _log : { levels : nodebug }}) . start ( ... ) . run () You can also explicitly define a set of messages with different levels to show in the logs: { _log : { levels : [ PROCESS , RUNNING , CACHED ]} } Even you can modify the base groups: { _log : { levels : normal , lvldiff : [ +DEBUG , P.ARGS , -SUBMIT ] } } Then the DEBUG , P.ARGS messages will show, and SUBMIT will hide. Define your theme Let's see how the built-in theme looks like first: in pyppl/logger.py : themes = { greenOnBlack : { PROCESS : [ colors . bold + colors . cyan , colors . bold + colors . underline + colors . cyan ], DONE : colors . bold + colors . green , DEBUG : colors . bold + colors . black , DEPENDS : colors . magenta , PROCESS : [ colors . bold + colors . cyan , colors . bold + colors . underline + colors . cyan ], in:SUBMIT,JOBDONE,INFO,P.PROPS,OUTPUT,EXPORT,INPUT,P.ARGS,BRINGS : colors . green , has:ERR : colors . red , in:WARNING,RETRY : colors . bold + colors . yellow , in:CACHED,RUNNING : colors . yellow , : colors . white }, # other themes } For the keys, you may either directly use the level name or have some prefix to define how to match the level names: - in: matches the messages with level name in the following list, which is separated by comma ( , ). - has: matches the messages with level name containing the following string. - starts: matches the messages with level name starting with the following string. - re : uses the following string as regular expression to match Then empty string key ( '' ) defines the colors to use for the messages that can not match any of the above rules. For the values, basically it's a 2-element list, where the first one defines the color to show the level name; and the second is the color to render the message. If only one color offered, it will be used for both level name and message. If you just want to modify the built-in themes, you can do it before you specify it to the pyppl constructor: from PyPPL import logger , PyPPL logger . themes [ greenOnBlack ][ DONE ] = logger . colors . cyan # ... define some procs PyPPL ({ _log :{ theme : greenOnBlack }}) . start ( ... ) . run () Yes, of course, you can also define a completely new theme: from pyppl import logger , PyPPL # ... define procs PyPPL ({ _log : { theme : { DONE : logger . colors . green , DEBUG : logger . colors . black , starts:LOG : logger . colors . bgwhite + logger . colors . black , # ... }} }) . start ( ... ) . run () Available colors in logger.colors : Key Color Key Color Key Color Key Color Key Color none '' 1 black red green yellow end 2 blue magenta cyan white bold A 3 bgblack bgred bggreen bgyellow underline _ 4 bgblue bgmagenta bgcyan bgwhite An empty string; 2. End of coloring; 3. Show bold characters; 4. Show underline characters. You can also use the directly terminal escape sequences, like \\033[30m for black (check here ). If you define a theme in a configuration file, you may use the escape sequences or also use the color names: { _log : { theme : { DONE : {{colors.green}} , DEBUG : {{colors.black}} , starts:LOG : {{colors.bgwhite}}{{colors.black}} , # ... }} } Log to file By default, pyppl will not log to a file until you set a file path to {\"_log\": {\"file\": \"/path/to/logfile\"}} in the configuration. Or you can specfiy False to it to disable logging to file. If you set it to True , a default log file will be used, which is: \"./pipeline.pyppl.log\" if your pipeline is from file: ./pipeline.py Note Filters and themes are not applied to handler to log to file. So you can always find all logs in the log file if your have it enabled. Progress bar Job status and progress are indicated in the log with progress bar: [==============================XXXXX!!!!! -----] The bar length defaults to 50 . You can change it in your code: from pyppl import Jobmgr Jobmgr . PBAR_SIZE = 80 Here is an explanation about how a cell (one sign or element of the bar) corresponds to job(s): Let's say Jobmgr.PBAR_SIZE = 9 and we have 5 jobs, then every two cells represent 1 job for first 8 cells, and last one represents job #5. The rule is trying to equally distributed the cells to jobs: 1 2 3 4 5 1 2 345 1 2345 [==XX!! -] NOT [===XXX! -] OR [=====X! -] While if you have more jobs than cells, we try to equally distribute jobs to cells: Say we have 9 jobs but Jobmgr.PBAR_SIZE = 5 : 1357 24689 [=X! -] First cell represents job #1, #2; second job #3, #4; ...; the last one represents job #9. The meaning of each sign in the cell: - - : Job initiated - ! : Job failed to submit - : Job running - X : Job failed - = : Job done Note that if a cell represents multiple jobs, it has a priority as above listed. For example, in the second situation, if job #1 is done, however, job #2 is running, then the sign should be . But if the progress bar belongs to a job (shown when a job is submitted or done), the status of the job has the highest priority. So in the above example, if the progress bar belongs to job #1: [JOBDONE] [1/9] [=----] Done: x.x% | Running: x ^ Indicating current job So even job #2 belongs to the first cell and it's running, the sign is still = .","title":"Log configuration"},{"location":"configure-your-logs/#configure-your-logs","text":"PyPPL has fancy logs. You can define how they look like (theme) and what messages to show (levels).","title":"Configure your logs"},{"location":"configure-your-logs/#built-in-log-themes","text":"We have some built-in themes: greenOnBlack (default): greenOnWhite: blueOnBlack: blueOnWhite: magentaOnBlack: magentaOnWhite: If you don't like them, you can also disable them: To use them, just specify the name in your pipeline configuration file: { _log : { theme : magentaOnWhite } } Or when you initialize a pipeline: PyPPL ({ _log : { theme : magentaOnWhite }}) . start ( ... ) . run () If you want to disable the theme, just set \"theme\" to False ( false for json ) If you set theme to True , then default theme greenOnBlack is used.","title":"Built-in log themes"},{"location":"configure-your-logs/#levels-of-pyppl-logs","text":"Please note that the levels are different from those of python's logging module. For logging module has 6 levels , with different int values. However, pyppl's log has many levels, or more suitable, flags, which don't have corresponding values. They are somehow equal, but some of them always print out unless you ask them not to. Note The log levels are a little bit different from here, please see debug your script . You may also specify the group name in your pipeline configuration file: { _log : { levels : nodebug }, // running profiles ... } Or when you initialize a pipeline: PyPPL ({ _log : { levels : nodebug }}) . start ( ... ) . run () You can also explicitly define a set of messages with different levels to show in the logs: { _log : { levels : [ PROCESS , RUNNING , CACHED ]} } Even you can modify the base groups: { _log : { levels : normal , lvldiff : [ +DEBUG , P.ARGS , -SUBMIT ] } } Then the DEBUG , P.ARGS messages will show, and SUBMIT will hide.","title":"Levels of pyppl logs"},{"location":"configure-your-logs/#define-your-theme","text":"Let's see how the built-in theme looks like first: in pyppl/logger.py : themes = { greenOnBlack : { PROCESS : [ colors . bold + colors . cyan , colors . bold + colors . underline + colors . cyan ], DONE : colors . bold + colors . green , DEBUG : colors . bold + colors . black , DEPENDS : colors . magenta , PROCESS : [ colors . bold + colors . cyan , colors . bold + colors . underline + colors . cyan ], in:SUBMIT,JOBDONE,INFO,P.PROPS,OUTPUT,EXPORT,INPUT,P.ARGS,BRINGS : colors . green , has:ERR : colors . red , in:WARNING,RETRY : colors . bold + colors . yellow , in:CACHED,RUNNING : colors . yellow , : colors . white }, # other themes } For the keys, you may either directly use the level name or have some prefix to define how to match the level names: - in: matches the messages with level name in the following list, which is separated by comma ( , ). - has: matches the messages with level name containing the following string. - starts: matches the messages with level name starting with the following string. - re : uses the following string as regular expression to match Then empty string key ( '' ) defines the colors to use for the messages that can not match any of the above rules. For the values, basically it's a 2-element list, where the first one defines the color to show the level name; and the second is the color to render the message. If only one color offered, it will be used for both level name and message. If you just want to modify the built-in themes, you can do it before you specify it to the pyppl constructor: from PyPPL import logger , PyPPL logger . themes [ greenOnBlack ][ DONE ] = logger . colors . cyan # ... define some procs PyPPL ({ _log :{ theme : greenOnBlack }}) . start ( ... ) . run () Yes, of course, you can also define a completely new theme: from pyppl import logger , PyPPL # ... define procs PyPPL ({ _log : { theme : { DONE : logger . colors . green , DEBUG : logger . colors . black , starts:LOG : logger . colors . bgwhite + logger . colors . black , # ... }} }) . start ( ... ) . run () Available colors in logger.colors : Key Color Key Color Key Color Key Color Key Color none '' 1 black red green yellow end 2 blue magenta cyan white bold A 3 bgblack bgred bggreen bgyellow underline _ 4 bgblue bgmagenta bgcyan bgwhite An empty string; 2. End of coloring; 3. Show bold characters; 4. Show underline characters. You can also use the directly terminal escape sequences, like \\033[30m for black (check here ). If you define a theme in a configuration file, you may use the escape sequences or also use the color names: { _log : { theme : { DONE : {{colors.green}} , DEBUG : {{colors.black}} , starts:LOG : {{colors.bgwhite}}{{colors.black}} , # ... }} }","title":"Define your theme"},{"location":"configure-your-logs/#log-to-file","text":"By default, pyppl will not log to a file until you set a file path to {\"_log\": {\"file\": \"/path/to/logfile\"}} in the configuration. Or you can specfiy False to it to disable logging to file. If you set it to True , a default log file will be used, which is: \"./pipeline.pyppl.log\" if your pipeline is from file: ./pipeline.py Note Filters and themes are not applied to handler to log to file. So you can always find all logs in the log file if your have it enabled.","title":"Log to file"},{"location":"configure-your-logs/#progress-bar","text":"Job status and progress are indicated in the log with progress bar: [==============================XXXXX!!!!! -----] The bar length defaults to 50 . You can change it in your code: from pyppl import Jobmgr Jobmgr . PBAR_SIZE = 80 Here is an explanation about how a cell (one sign or element of the bar) corresponds to job(s): Let's say Jobmgr.PBAR_SIZE = 9 and we have 5 jobs, then every two cells represent 1 job for first 8 cells, and last one represents job #5. The rule is trying to equally distributed the cells to jobs: 1 2 3 4 5 1 2 345 1 2345 [==XX!! -] NOT [===XXX! -] OR [=====X! -] While if you have more jobs than cells, we try to equally distribute jobs to cells: Say we have 9 jobs but Jobmgr.PBAR_SIZE = 5 : 1357 24689 [=X! -] First cell represents job #1, #2; second job #3, #4; ...; the last one represents job #9. The meaning of each sign in the cell: - - : Job initiated - ! : Job failed to submit - : Job running - X : Job failed - = : Job done Note that if a cell represents multiple jobs, it has a priority as above listed. For example, in the second situation, if job #1 is done, however, job #2 is running, then the sign should be . But if the progress bar belongs to a job (shown when a job is submitted or done), the status of the job has the highest priority. So in the above example, if the progress bar belongs to job #1: [JOBDONE] [1/9] [=----] Done: x.x% | Running: x ^ Indicating current job So even job #2 belongs to the first cell and it's running, the sign is still = .","title":"Progress bar"},{"location":"draw-flowchart-of-a-pipeline/","text":"Draw flowchart of a pipeline PyPPL will generate a graph in DOT language , according to the process dependencies. You can have multiple renderers to visualize to graph. A typical one is Graphviz . With its python port graphviz installed, you can output the flowchart to an svg figure. Generate the flowchart For example, if we have a pipeline written in pipeline.py : from pyppl import PyPPL , Proc p1 = Proc () p2 = Proc () p3 = Proc () p4 = Proc () p5 = Proc () p6 = Proc () p7 = Proc () p8 = Proc () p9 = Proc () p1 p8 / \\ / p2 p3 \\ / p4 p9 / \\ / p5 p6 (export) \\ / p7 (export) p2 . depends = p1 p3 . depends = p1 , p8 p4 . depends = p2 , p3 p4 . exdir = ./export p5 . depends = p4 p6 . depends = p4 , p9 p6 . exdir = ./export p7 . depends = p5 , p6 p7 . exdir = ./export # make sure at least one job is created. p1 . input = { in : [ 0 ]} p8 . input = { in : [ 0 ]} p9 . input = { in : [ 0 ]} PyPPL () . star ( p1 , p8 , p9 ) . flowchart () . run () You can specify different files to store the dot and svg file: pyppl () . starts ( p1 , p8 , p9 ) . flowchart ( /another/dot/file , /another/svg/file ) Note The svg file will be only generated if you specify the right command to do it. For example, if you have Graphviz installed, you will have dot available to convert the dot file to svg file: PyPPL () . start ( p1 , p8 , p9 ) . flowchart ( /another/dot/file , /another/svg/file ) The graph ( svgfile ) will be like: The green processes are the starting processes; ones with purple text are processes that will export the output files; and nodes in red are the end processes of the pipeline. Use the dark theme PyPPL ({ _flowchart : { theme : dark } }) . star ( p1 , p8 , p9 ) . flowchart () . run () Define your own theme You just need to define the style for each type of nodes (refer DOT node shapes for detailed styles): You may also put the definition in the default configuration file ( ~/.PyPPL.json ) PyPPL ({ _flowchart : { theme : { base : { shape : box , style : rounded,filled , fillcolor : #555555 , color : #ffffff , fontcolor : #ffffff , }, start : { style : filled , color : #59b95d , # green penwidth : 2 , }, end : { style : filled , color : #ea7d75 , # red penwidth : 2 , }, export : { fontcolor : #db95e6 , # purple }, skip : { fillcolor : #eaeaea , # gray }, skip+ : { fillcolor : #e9e9e9 , # gray }, resume : { fillcolor : #1b5a2d , # light green }, aggr : { style : filled , color : #eeeeee , # almost white } } } }) . star ( p1 , p8 , p9 ) . flowchart () . run () Explanations of node types: base : The base node style start : The style for starting processes end : The style for starting processes export : The style for processes have output file to be exported skip : The style for processes to be skiped skip+ : The style for processes to be skiped but ouput channel will be computed resume : The style for the processes to be resumed aggr : The style for the group, where all processes belong to the same aggregation","title":"Pipeline flowchart"},{"location":"draw-flowchart-of-a-pipeline/#draw-flowchart-of-a-pipeline","text":"PyPPL will generate a graph in DOT language , according to the process dependencies. You can have multiple renderers to visualize to graph. A typical one is Graphviz . With its python port graphviz installed, you can output the flowchart to an svg figure.","title":"Draw flowchart of a pipeline"},{"location":"draw-flowchart-of-a-pipeline/#generate-the-flowchart","text":"For example, if we have a pipeline written in pipeline.py : from pyppl import PyPPL , Proc p1 = Proc () p2 = Proc () p3 = Proc () p4 = Proc () p5 = Proc () p6 = Proc () p7 = Proc () p8 = Proc () p9 = Proc () p1 p8 / \\ / p2 p3 \\ / p4 p9 / \\ / p5 p6 (export) \\ / p7 (export) p2 . depends = p1 p3 . depends = p1 , p8 p4 . depends = p2 , p3 p4 . exdir = ./export p5 . depends = p4 p6 . depends = p4 , p9 p6 . exdir = ./export p7 . depends = p5 , p6 p7 . exdir = ./export # make sure at least one job is created. p1 . input = { in : [ 0 ]} p8 . input = { in : [ 0 ]} p9 . input = { in : [ 0 ]} PyPPL () . star ( p1 , p8 , p9 ) . flowchart () . run () You can specify different files to store the dot and svg file: pyppl () . starts ( p1 , p8 , p9 ) . flowchart ( /another/dot/file , /another/svg/file ) Note The svg file will be only generated if you specify the right command to do it. For example, if you have Graphviz installed, you will have dot available to convert the dot file to svg file: PyPPL () . start ( p1 , p8 , p9 ) . flowchart ( /another/dot/file , /another/svg/file ) The graph ( svgfile ) will be like: The green processes are the starting processes; ones with purple text are processes that will export the output files; and nodes in red are the end processes of the pipeline.","title":"Generate the flowchart"},{"location":"draw-flowchart-of-a-pipeline/#use-the-dark-theme","text":"PyPPL ({ _flowchart : { theme : dark } }) . star ( p1 , p8 , p9 ) . flowchart () . run ()","title":"Use the dark theme"},{"location":"draw-flowchart-of-a-pipeline/#define-your-own-theme","text":"You just need to define the style for each type of nodes (refer DOT node shapes for detailed styles): You may also put the definition in the default configuration file ( ~/.PyPPL.json ) PyPPL ({ _flowchart : { theme : { base : { shape : box , style : rounded,filled , fillcolor : #555555 , color : #ffffff , fontcolor : #ffffff , }, start : { style : filled , color : #59b95d , # green penwidth : 2 , }, end : { style : filled , color : #ea7d75 , # red penwidth : 2 , }, export : { fontcolor : #db95e6 , # purple }, skip : { fillcolor : #eaeaea , # gray }, skip+ : { fillcolor : #e9e9e9 , # gray }, resume : { fillcolor : #1b5a2d , # light green }, aggr : { style : filled , color : #eeeeee , # almost white } } } }) . star ( p1 , p8 , p9 ) . flowchart () . run () Explanations of node types: base : The base node style start : The style for starting processes end : The style for starting processes export : The style for processes have output file to be exported skip : The style for processes to be skiped skip+ : The style for processes to be skiped but ouput channel will be computed resume : The style for the processes to be resumed aggr : The style for the group, where all processes belong to the same aggregation","title":"Define your own theme"},{"location":"error-handling/","text":"Error handling of processes You can ask PyPPL to terminate, retry or ignore a job that fails to run. When a job finishes, it should generate a job.rc file containing the return code. When compare with the valid return codes pXXX.rc , the error triggered if it is not in pXXX.rc . pXXX.errhow determines what's next if errors happen. \"terminate\" : when errors happen, terminate the entire pipeline \"ignore\" : ignore the errors, continuing run the next process \"retry\" : re-submit and run the job again. pXXX.errntry defines how many time to retry. Set expectations of a process results You can use commands to check whether you have expected output. For example: p = Proc () p . input = { input : 1 } p . script = echo {{in.input}} # check the stdout p . expect = grep 1 {{job.outfile}}","title":"Error handling of processes"},{"location":"error-handling/#error-handling-of-processes","text":"You can ask PyPPL to terminate, retry or ignore a job that fails to run. When a job finishes, it should generate a job.rc file containing the return code. When compare with the valid return codes pXXX.rc , the error triggered if it is not in pXXX.rc . pXXX.errhow determines what's next if errors happen. \"terminate\" : when errors happen, terminate the entire pipeline \"ignore\" : ignore the errors, continuing run the next process \"retry\" : re-submit and run the job again. pXXX.errntry defines how many time to retry.","title":"Error handling of processes"},{"location":"error-handling/#set-expectations-of-a-process-results","text":"You can use commands to check whether you have expected output. For example: p = Proc () p . input = { input : 1 } p . script = echo {{in.input}} # check the stdout p . expect = grep 1 {{job.outfile}}","title":"Set expectations of a process results"},{"location":"export-output-files/","text":"Export output files Output files are generated in outdir ( job.index /output ) if you specify the basename for a file/path/dir for process output. You can export them to a specific directory by specify the directory to exdir of a process: pXXX.exdir = exdir . You can use different ways (specify it to exporthow (alias: exhow ) of a process) to export output files: Ways to export ( p.exhow=? ) Aliases What happens move (default) mv Move output files to the directory, leave links to them in outdir (make sure processes depend on this one can run normally). copy cp Copy output files to the directory symlink link , symbol Create symbolic links to the output files in the directory gzip gz If output is a file, will do gzip of the file and save the gzipped file in the export directory; if output is a directory, will do tar -zcvf of the output directory and save the result file in the export directory. You can export the output files of any process. Note that even though the export directory is specific to a process, the minimum unit is a job , whose output files are ready to be exported once it finishes successfully. You can ask PyPPL whether to overwrite the existing files in the export directory by set exow as True (overwrite) or False (do not overwrite). Note if the directory you specified to pXXX.exdir does not exist, it will be created automatically, including those intermediate directories if necessary. Partial export You can also partially export the output files by set value to pXXX.expart . You have 2 ways to select the files: - Output key. For example, for p.output = \"outfile1:file:a.txt1, outfile2:file:b.txt2\" , you can export only outfile1 by: p.expart = \"outfile1\" - Glob patterns. In the above example, you can also do: p.expart = \"*.txt1\" You can have multiple selectors: p.expart = [\"*.txt1\", \"outfile2\"] to export all files. Note Export-caching will not be allowed when using partial export. Templating is applied for this option. expart will first match output keys and then be used as a glob pattern. So if you have # ... p . output = outfile1:file:outfile2, outfile2:file:b.txt2 # ... p . expart = [ outfile2 ] then b.txt2 will be exported instead of job.outdir /outfile2 Control of export of cached jobs By default, if a job is cached, then it will not try to export the output files again (assuming that you have already successfully run the job and exported the output files). But you can force to export them anyway by setting p.cclean = True","title":"Output file exporting"},{"location":"export-output-files/#export-output-files","text":"Output files are generated in outdir ( job.index /output ) if you specify the basename for a file/path/dir for process output. You can export them to a specific directory by specify the directory to exdir of a process: pXXX.exdir = exdir . You can use different ways (specify it to exporthow (alias: exhow ) of a process) to export output files: Ways to export ( p.exhow=? ) Aliases What happens move (default) mv Move output files to the directory, leave links to them in outdir (make sure processes depend on this one can run normally). copy cp Copy output files to the directory symlink link , symbol Create symbolic links to the output files in the directory gzip gz If output is a file, will do gzip of the file and save the gzipped file in the export directory; if output is a directory, will do tar -zcvf of the output directory and save the result file in the export directory. You can export the output files of any process. Note that even though the export directory is specific to a process, the minimum unit is a job , whose output files are ready to be exported once it finishes successfully. You can ask PyPPL whether to overwrite the existing files in the export directory by set exow as True (overwrite) or False (do not overwrite). Note if the directory you specified to pXXX.exdir does not exist, it will be created automatically, including those intermediate directories if necessary.","title":"Export output files"},{"location":"export-output-files/#partial-export","text":"You can also partially export the output files by set value to pXXX.expart . You have 2 ways to select the files: - Output key. For example, for p.output = \"outfile1:file:a.txt1, outfile2:file:b.txt2\" , you can export only outfile1 by: p.expart = \"outfile1\" - Glob patterns. In the above example, you can also do: p.expart = \"*.txt1\" You can have multiple selectors: p.expart = [\"*.txt1\", \"outfile2\"] to export all files. Note Export-caching will not be allowed when using partial export. Templating is applied for this option. expart will first match output keys and then be used as a glob pattern. So if you have # ... p . output = outfile1:file:outfile2, outfile2:file:b.txt2 # ... p . expart = [ outfile2 ] then b.txt2 will be exported instead of job.outdir /outfile2","title":"Partial export"},{"location":"export-output-files/#control-of-export-of-cached-jobs","text":"By default, if a job is cached, then it will not try to export the output files again (assuming that you have already successfully run the job and exported the output files). But you can force to export them anyway by setting p.cclean = True","title":"Control of export of cached jobs"},{"location":"faq/","text":"FAQ Q: How should I migrate from 0.8.x? First letters of class names are capitalized (i.e. proc - Proc , aggr - Aggr ). Note that previous class pyppl was changed to PyPPL . Default configuration files were changed to ~/.PyPPL.json and ~/.PyPPL Log configurations were grouped to {\"log\": {...}} instead of {\"logtheme\": ..., \"loglevels\": ...} Flowchart is themeable now: in configuration file: {\"flowchart\": {...}} Templating enhanced from previous placeholders (Jinja2 supported). See templating Input and output placeholders are now under in and out namespaces, respectively. updateArgs is merged into set for Aggr . Module doct removed, python-box is used instead. Q: Do I have to use the variable name as the process id? A: No, you can use a different one by pWhatever = Proc (id=pYourId) , or pWhatever = Proc () , and then change the id by pWhatever.id = 'pYourId' Q: What's the difference between input and args ? A: Basically, args are supposed to be arguments shared among all jobs in the process. Files in args are not linked in the job.indir folder. Q: Does a Proc remain the same after it's used to construct an Aggr ? A: No, it will be a copy of the original one. So the original be used somewhere else. Q: Can I dry-run a process? A: Yes, just use the dry runner: p.runner = \"dry\" . The runner will just create empty files/directories for output, and skip to run the script. Q: Can I disable the logs on the terminal? A: Yes, just set {\"log\": {\"levels\": None}} in pipeline configurations.","title":"FAQ"},{"location":"faq/#faq","text":"Q: How should I migrate from 0.8.x? First letters of class names are capitalized (i.e. proc - Proc , aggr - Aggr ). Note that previous class pyppl was changed to PyPPL . Default configuration files were changed to ~/.PyPPL.json and ~/.PyPPL Log configurations were grouped to {\"log\": {...}} instead of {\"logtheme\": ..., \"loglevels\": ...} Flowchart is themeable now: in configuration file: {\"flowchart\": {...}} Templating enhanced from previous placeholders (Jinja2 supported). See templating Input and output placeholders are now under in and out namespaces, respectively. updateArgs is merged into set for Aggr . Module doct removed, python-box is used instead. Q: Do I have to use the variable name as the process id? A: No, you can use a different one by pWhatever = Proc (id=pYourId) , or pWhatever = Proc () , and then change the id by pWhatever.id = 'pYourId' Q: What's the difference between input and args ? A: Basically, args are supposed to be arguments shared among all jobs in the process. Files in args are not linked in the job.indir folder. Q: Does a Proc remain the same after it's used to construct an Aggr ? A: No, it will be a copy of the original one. So the original be used somewhere else. Q: Can I dry-run a process? A: Yes, just use the dry runner: p.runner = \"dry\" . The runner will just create empty files/directories for output, and skip to run the script. Q: Can I disable the logs on the terminal? A: Yes, just set {\"log\": {\"levels\": None}} in pipeline configurations.","title":"FAQ"},{"location":"placeholders/","text":"Templating PyPPL has its own template engine, which is derived from a 500-line-or-less template engine . It also supports Jinja2 if you have it installed and specify \"Jinja2\" to pXXX.template . The built-in template engine is enabled by default. Common data avaible for rendering When rendering a template, following data are fed to the render function. So that you can use those values in the template. Some attribute values of a process are shared for all templates that are applied: proc.aggr : The aggregation name of the process proc.args : A dict of process arguments. To access an item of it: {{args. item }} proc.cache : The cache option proc.cclean : Whether clean (error check and export) the job when it's cached? proc.desc : The description of the process proc.echo : The echo option proc.errhow : What to do if error happens proc.errntry : If errorhow == 'retry' , how many times to re-try if a job fails proc.exdir : The export directory proc.exhow : How to export output files proc.exow : Whether to overwrite existing files when exporting output files proc.forks : How many jobs to run concurrently proc.id : The id of the process. proc.infile : Where does {{in.infile}} refer to? (including other input files) proc.lang : The interpreter for the script proc.ppldir : Where the workdirs are located proc.procvars : The dict of all avaiable attributes of a process, can be accessed directly by {{proc. var }} proc.rc : The rc option proc.resume : The resume option proc.runner : The runner proc.sets : A list of attribute names that has been set explictly proc.size : Number of jobs proc.suffix : The unique suffix of the process proc.tag : The tag of the process proc.workdir : The workdir of the process Other data for rendering For each job, we also have some value available for rendering: job.index : The index of the job job.indir : The input directory of the job job.outdir : The output directory of the job job.dir : The directory of the job job.outfile : The stdout file of the job job.errfile : The stderr file of the job job.pidfile : The file stores the PID of the job or the identity from a queue runner. Input and output data are under namespace in and out , respectively. For example, you have following definition: pXXX . input = { a : [ hello ], b : [ /path/to/file ]} pXXX . output = a:{{in.a}} world! Now you can access them by: {{in.a}} , {{in.b}} and {{out.a}} The scope of data Attribute Data available Meaning pXXX.beforeCmd {{proc.*}} Command to run before job starts pXXX.afterCmd {{proc.*}} Command to run after job finishes pXXX.brings {{proc.*}} , {{job.*}} , {{in.*}} The bring-in files pXXX.output {{proc.*}} , {{job.*}} , {{in.*}} , {{bring.*}} The output of the process pXXX.expect All above-mentioned data Command to check output pXXX.expart All above-mentioned data Partial export pXXX.script All above-mentioned data The script to run Built-in functions Sometimes we need to transform the data in a template. We have some built-in functions available for the transformation. For built-in template engine, you may use pipe, for example: {{in.file | basename}} ; for Jinja2 , you have to use functions as \"functions\", for example: {{basename(in.file)}} . Here we give the examples with built-in template engine syntax. R : Transform a python value to R value. For example: Usage Data Result {{v R}} {'v': True} TRUE {'v': 'TRUE'} TRUE {'v': 'NA'} NA {'v': 'NULL'} NULL {'v': 1} 1 {'v': 'r:c(1,2,3)'} c(1,2,3) {'v': 'plainstring'} \"plainstring\" Rvec : Transform a python list to a R vector. For example: {{v | Rvec}} with {'v': [1,2,3]} results in c(1,2,3) Rlist : Transform a python dict to a R list. For example: {{v | Rlist}} with {'v': {'a':1, 'b':2}} results in list(a=1, b=2) realpath : Alias of os.path.realpath readlink : Alias of os.readlink dirname : Alias of os.path.dirname basename : Get the basename of a file. If a file is renamed by PyPPL in case of input files with the same basename, it tries to get the original basename. For example: Usage Data Result {{v basename}} {'v': '/path/to/file.txt'} file.txt {'v': '/path/to/file[1].txt'} file.txt {{v, orig basename}} {'v': '/path/to/file[1].txt', 'orig': True} file[1].txt bn : Alias of basename filename : Similar as basename but without extension. fn : Alias of filename filename2 : Get the filename without dot. fn2 : Alias of filename2 . (i.e: /a/b/c.d.e.txt - c ) ext : Get extension of a file. Alias of os.path.splitext(x)[1] Dot is included. To remove the dot: {{v | ext | [1:]}} prefix : Get the prefix of a path, without extension. It acts like {{v | dirname}}/{{v | filename}} prefix2 : Get the prefix of a path without dot in filename. (i.e: /a/b/c.d.e.txt - /a/b/c ) quote : Double-quote a string. asquote : Double quote items in a list and join them by space. For example: {{v | asquote}} with {'v': [1,2,3]} results in \"1\" \"2\" \"3\" acquote : Double quote items in a list and join them by comma. squote : Single-quote a string. json : Dumps a python object to a json string. Alias of json.dumps read : Read the content of a file. readlines : Read the lines of a file. Empty lines are skipped by default. To return the empty lines for {'v': '/path/to/file', 'skipEmptyLines': False} : {{v, skipEmptyLines | readlines}} repr : Alias of python repr built-in function. Usage of built-in template engine Basic usage: Usage Data Result {{v}} {'v': 1} 1 {{v.a}} , {{v['a']}} {'v': {'a': 1}} 1 {{v.0}} , {{v[0]}} {'v': [1]} 1 {{v.upper()}} {'v': \"a\"} A {{v.0.upper()}} {'v': [\"a\"]} A Applying functions: Usage Data Result {{v R}} {'v': True} TRUE {{v1, v2 paste}} {'v1': 'Hello', 'v2': 'world!', 'paste': lambda x, y: x + ' ' + y} Hello world! {{v1, v2 lambda x, y: x + ' ' + y}} {'v1': 'Hello', 'v2': 'world!'} Hello world! Note If you want to pass a literal value (for example: 1 , True ), you CANNOT do this: {{v, False | readlines}} . Instead, you can either: specify the value in data: {'v': '/path/to/file', 'skipEmptyLines': False} , then {{v, skipEmptyLines | readlines}} ; or use lambda function: {{v, readlines | lambda x, func: func(x, False)}} If-else/elif statements: Usage Data Result {% if v %} 1 {% else %} 2 {% endif %} {'v': True} 1 {% if v notExists %} Path not exists. {% elif v isDir %} Path is a directory. {% else %} Path exists but is not a directory. {% endif %} { 'v': '/path/to/file', 'notExists': lambda x: not __import__('os').path.exists(x), 'isDir': __import__('os').path.isdir } Path exists but is not a directory. Loops: Usage Data Result {% for var in varlist %}{{var R}}{% endfor %} {'varlist': ['abc', 'True', 1, False]} \"abc\"TRUE1FALSE {% for k , v in data.items() %}{{k}}:{{v}}{% endfor %} {'data': {'a':1, 'b':2}} a:1b:2 Set environment of template engine You can define you own functions/data for template rendering: pXXX . envs . data = { v1 : a , v2 : b , b : True } pXXX . envs . os = __import__ ( os ) pXXX . envs . paste = lambda x , y : x + + y # use them pXXX . script = { % i f data.b %} print {{data.v1, data.v2 | paste}} { % e lse %} print {{data.v1, data.v2 | os.path.join}} { % e ndif %} Then if pXXX.envs.data['b'] is True , it prints a b ; otherwise it prints a/b . Use Jinja2 All the data and environment definition mentioned above are all applicable when you use Jinja2 as your template engine. For usage of Jinja2 , you may refer to its official documentation .","title":"Templating"},{"location":"placeholders/#templating","text":"PyPPL has its own template engine, which is derived from a 500-line-or-less template engine . It also supports Jinja2 if you have it installed and specify \"Jinja2\" to pXXX.template . The built-in template engine is enabled by default.","title":"Templating"},{"location":"placeholders/#common-data-avaible-for-rendering","text":"When rendering a template, following data are fed to the render function. So that you can use those values in the template. Some attribute values of a process are shared for all templates that are applied: proc.aggr : The aggregation name of the process proc.args : A dict of process arguments. To access an item of it: {{args. item }} proc.cache : The cache option proc.cclean : Whether clean (error check and export) the job when it's cached? proc.desc : The description of the process proc.echo : The echo option proc.errhow : What to do if error happens proc.errntry : If errorhow == 'retry' , how many times to re-try if a job fails proc.exdir : The export directory proc.exhow : How to export output files proc.exow : Whether to overwrite existing files when exporting output files proc.forks : How many jobs to run concurrently proc.id : The id of the process. proc.infile : Where does {{in.infile}} refer to? (including other input files) proc.lang : The interpreter for the script proc.ppldir : Where the workdirs are located proc.procvars : The dict of all avaiable attributes of a process, can be accessed directly by {{proc. var }} proc.rc : The rc option proc.resume : The resume option proc.runner : The runner proc.sets : A list of attribute names that has been set explictly proc.size : Number of jobs proc.suffix : The unique suffix of the process proc.tag : The tag of the process proc.workdir : The workdir of the process","title":"Common data avaible for rendering"},{"location":"placeholders/#other-data-for-rendering","text":"For each job, we also have some value available for rendering: job.index : The index of the job job.indir : The input directory of the job job.outdir : The output directory of the job job.dir : The directory of the job job.outfile : The stdout file of the job job.errfile : The stderr file of the job job.pidfile : The file stores the PID of the job or the identity from a queue runner. Input and output data are under namespace in and out , respectively. For example, you have following definition: pXXX . input = { a : [ hello ], b : [ /path/to/file ]} pXXX . output = a:{{in.a}} world! Now you can access them by: {{in.a}} , {{in.b}} and {{out.a}}","title":"Other data for rendering"},{"location":"placeholders/#the-scope-of-data","text":"Attribute Data available Meaning pXXX.beforeCmd {{proc.*}} Command to run before job starts pXXX.afterCmd {{proc.*}} Command to run after job finishes pXXX.brings {{proc.*}} , {{job.*}} , {{in.*}} The bring-in files pXXX.output {{proc.*}} , {{job.*}} , {{in.*}} , {{bring.*}} The output of the process pXXX.expect All above-mentioned data Command to check output pXXX.expart All above-mentioned data Partial export pXXX.script All above-mentioned data The script to run","title":"The scope of data"},{"location":"placeholders/#built-in-functions","text":"Sometimes we need to transform the data in a template. We have some built-in functions available for the transformation. For built-in template engine, you may use pipe, for example: {{in.file | basename}} ; for Jinja2 , you have to use functions as \"functions\", for example: {{basename(in.file)}} . Here we give the examples with built-in template engine syntax. R : Transform a python value to R value. For example: Usage Data Result {{v R}} {'v': True} TRUE {'v': 'TRUE'} TRUE {'v': 'NA'} NA {'v': 'NULL'} NULL {'v': 1} 1 {'v': 'r:c(1,2,3)'} c(1,2,3) {'v': 'plainstring'} \"plainstring\" Rvec : Transform a python list to a R vector. For example: {{v | Rvec}} with {'v': [1,2,3]} results in c(1,2,3) Rlist : Transform a python dict to a R list. For example: {{v | Rlist}} with {'v': {'a':1, 'b':2}} results in list(a=1, b=2) realpath : Alias of os.path.realpath readlink : Alias of os.readlink dirname : Alias of os.path.dirname basename : Get the basename of a file. If a file is renamed by PyPPL in case of input files with the same basename, it tries to get the original basename. For example: Usage Data Result {{v basename}} {'v': '/path/to/file.txt'} file.txt {'v': '/path/to/file[1].txt'} file.txt {{v, orig basename}} {'v': '/path/to/file[1].txt', 'orig': True} file[1].txt bn : Alias of basename filename : Similar as basename but without extension. fn : Alias of filename filename2 : Get the filename without dot. fn2 : Alias of filename2 . (i.e: /a/b/c.d.e.txt - c ) ext : Get extension of a file. Alias of os.path.splitext(x)[1] Dot is included. To remove the dot: {{v | ext | [1:]}} prefix : Get the prefix of a path, without extension. It acts like {{v | dirname}}/{{v | filename}} prefix2 : Get the prefix of a path without dot in filename. (i.e: /a/b/c.d.e.txt - /a/b/c ) quote : Double-quote a string. asquote : Double quote items in a list and join them by space. For example: {{v | asquote}} with {'v': [1,2,3]} results in \"1\" \"2\" \"3\" acquote : Double quote items in a list and join them by comma. squote : Single-quote a string. json : Dumps a python object to a json string. Alias of json.dumps read : Read the content of a file. readlines : Read the lines of a file. Empty lines are skipped by default. To return the empty lines for {'v': '/path/to/file', 'skipEmptyLines': False} : {{v, skipEmptyLines | readlines}} repr : Alias of python repr built-in function.","title":"Built-in functions"},{"location":"placeholders/#usage-of-built-in-template-engine","text":"Basic usage: Usage Data Result {{v}} {'v': 1} 1 {{v.a}} , {{v['a']}} {'v': {'a': 1}} 1 {{v.0}} , {{v[0]}} {'v': [1]} 1 {{v.upper()}} {'v': \"a\"} A {{v.0.upper()}} {'v': [\"a\"]} A Applying functions: Usage Data Result {{v R}} {'v': True} TRUE {{v1, v2 paste}} {'v1': 'Hello', 'v2': 'world!', 'paste': lambda x, y: x + ' ' + y} Hello world! {{v1, v2 lambda x, y: x + ' ' + y}} {'v1': 'Hello', 'v2': 'world!'} Hello world! Note If you want to pass a literal value (for example: 1 , True ), you CANNOT do this: {{v, False | readlines}} . Instead, you can either: specify the value in data: {'v': '/path/to/file', 'skipEmptyLines': False} , then {{v, skipEmptyLines | readlines}} ; or use lambda function: {{v, readlines | lambda x, func: func(x, False)}} If-else/elif statements: Usage Data Result {% if v %} 1 {% else %} 2 {% endif %} {'v': True} 1 {% if v notExists %} Path not exists. {% elif v isDir %} Path is a directory. {% else %} Path exists but is not a directory. {% endif %} { 'v': '/path/to/file', 'notExists': lambda x: not __import__('os').path.exists(x), 'isDir': __import__('os').path.isdir } Path exists but is not a directory. Loops: Usage Data Result {% for var in varlist %}{{var R}}{% endfor %} {'varlist': ['abc', 'True', 1, False]} \"abc\"TRUE1FALSE {% for k , v in data.items() %}{{k}}:{{v}}{% endfor %} {'data': {'a':1, 'b':2}} a:1b:2","title":"Usage of built-in template engine"},{"location":"placeholders/#set-environment-of-template-engine","text":"You can define you own functions/data for template rendering: pXXX . envs . data = { v1 : a , v2 : b , b : True } pXXX . envs . os = __import__ ( os ) pXXX . envs . paste = lambda x , y : x + + y # use them pXXX . script = { % i f data.b %} print {{data.v1, data.v2 | paste}} { % e lse %} print {{data.v1, data.v2 | os.path.join}} { % e ndif %} Then if pXXX.envs.data['b'] is True , it prints a b ; otherwise it prints a/b .","title":"Set environment of template engine"},{"location":"placeholders/#use-jinja2","text":"All the data and environment definition mentioned above are all applicable when you use Jinja2 as your template engine. For usage of Jinja2 , you may refer to its official documentation .","title":"Use Jinja2"},{"location":"runners/","text":"Runners and running profiles Running profile A running profile defines the parameters that needed for a pipeline to run. Generally it contains the runner, the parameters for the runner and the common settings for the processes. A typical running profile is as follows: { runner : sge , sgeRunner : { queue : 1-day }, forks : 32 } Caution You may also put other settings of processes into a running profile, but keep in mind: 1. The value will not be overridden if the attribute is set explicitly (i.e: p.forks = 10 ) 2. Only set common attributes for all processes in a pipeline to avoid unexprected behavior. For example, you probably don't want this in general cases to set the same script for all processes: { script : file:/path/to/script } Defining running profiles You may pre-define some profiles so that you can easily swith them by: PyPPL () . start ( pXXX ) . run ( profile1 ) PyPPL () . start ( pXXX ) . run ( profile2 ) You can define profiles in PyPPL 's default configuration files: $HOME/.PyPPL.yaml , $HOME/.PyPPL and/or $HOME/.PyPPL.json . The latter ones have high priorities. $HOME/.PyPPL should also be in JSON format. Take $HOME/.PyPPL.yaml (requiring pyyaml ) for example, the content is like: default : runner : local forks : 1 echo : stderr profile1 : runner : sge sgeRunner : queue : 1-day profile2 : runner : sge sgeRunner : queue : 7-days Note If a key is not in a profile, then it will be inherited from default . You may also define some profiles in a file somewhere else, say /path/to/myprofiles.yaml . Just pass the file to PyPPL constructor: PyPPL ( cfgfile = /path/to/myprofiles.yaml ) . start ( pXXX ) . run ( profile1 ) Note This has higher priority than default configuration files. You can also pass the profiles to PyPPL constructor directly: PyPPL ({ default : { runner : local , forks : 1 , echo : stderr }, profile1 : { runner : sge , sgeRunner : { queue : 1-day } }, profile2 : { runner : sge , sgeRunner : { queue : 7-days } } }) . start ( pXXX ) . run ( profile1 ) Note In this way, the profiles have higher priorities than the ones defined in configuration files. Or even, you can also specify a profile to run function to ask the pipeline run with the profile directly: PyPPL () . start ( pXXX ) . run ({ runner : sge , sgeRunner : { queue : 1-day } }) Note This has the highest priority. Built-in runners We have 5 built-in runners ( RunnerLocal , RunnerSsh , RunnerSge , RunnerSlurm , runnerDry ), you can also define you own runners. You can either tell one process to use a runner, or even, you can tell the pipeline to use one runner for all the processes. That means each process can have the same runner or a different one. To tell a process which runner to use, just specify the runner name to pXXX.runner (for example, pXXX.runner = \"sge\" to use the sge runner). Each process may use different configuration for the runner ( pXXX.sgeRunner ) or the same one by configuring the pipeline . Configurations for ssh runner Ssh runner takes the advantage to use the computing resources from other servers that can be connected via ssh . The ssh command allows us to pass the command to the server and execute it: ssh [options] [command] Caution ssh runner only works when the servers share the same file system. you have to configure so that you don't need a password to log onto the servers, or use a private key to connect to the ssh servers. The jobs will be distributed equally to the servers. To tell a process the available ssh servers: pXXX . sshRunner = { servers : [ server1 , server2 , ... ], keys : [ /path/to/keyfile1 , /path/to/keyfile2 , ... ] } You can have complicated ssh configurations which can be set by the system ssh config subsystem: $HOME/.ssh/config : # contents of $HOME/.ssh/config Host dev HostName dev.example.com Port 22000 User fooey If you use different usernames to log on the servers, you may also specify the usernames as well: pXXX . sshRunner = { servers : [ user1@server1 , user2@server2 , ... ]} You can also add preScript and postScript for all jobs: pXXX . sshRunner = { servers :[ ... ], preScript : mkdir some/dir/to/be/made , postScript : rm -rf /path/to/job/tmp/dir } To make a running profile with it for a pipeline for all processes: PyPPL ({ # default profile default : { sshRunner : { servers : [ user1@server1 , user2@server2 , ... ]} }, ssh3 : { runner : ssh , sshRunner : { servers : [ server1 , server2 , server3 ], keys : [ /path/to/key1 , /path/to/key2 , /path/to/key3 ] } } }) Also see \" pipeline configration \" for more details. The constructor of the runner will change the actual script to run the following ( workdir /0/job.script.ssh ): #!/usr/bin/env bash ssh -i /path/to/key1 user1@server1 cd cwd ; workdir /0/job.script Configurations for sge runner Similarly, you can also submit your jobs to SGE servers using qsub . To set the options for a process: pXXX . sgeRunner = { sge.q : 1-day , # the queue sge.M : user@domain.com , # The email for notification sge.l : h_vmem=4G , sge.l : h_stack=512M , # Remember to add an extra space # so that it won t override the previous sge.l sge.m : abe , # When to notify sge.notify : True , preScript : source /home/user/.bash_profile /dev/null; mkdir /tmp/my , # load the environment and create the temporary directory postScript : rm -rf /tmp/my # clean up } Please check man qsub to find other options. Remember to add a sge. prefix to the option name. To make a running profile with it for a pipeline for all processes: PyPPL ({ proc : { sgeRunner : { #... } } }) Also see \" pipeline configuration \" for more details. The constructor of the runner will change the script to run to the following ( workdir /0/job.script.sge ): #!/usr/bin/env bash #$ -N id . tag .0 #$ -q 1-day #$ -o workdir /PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stdout #$ -e workdir /PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stderr #$ -cwd #$ -M wang.panwen@mayo.edu #$ -m abe #$ -l h_vmem=4G #$ -l h_stack=512M #$ -notify trap status=\\$?; echo \\$status workdir /0/job.rc; exit \\$status 1 2 3 6 7 8 9 10 11 12 15 16 17 EXIT source /home/whoever/.bash_profile /dev/null ; mkdir /tmp/my workdir /0/job.script rm -rf /tmp/my Configurations for slurm runner Where to configure it: For single process: pXXX . slurmRunner = { ... } For running profiles: config = { proc : { ... # other configurations runner : slurm , # all processes run with slurm slurmRunner : { ... } }, # or you can also create a profile runWithSlurm : { ... # other configurations runner : slurm , slurmRunner : { ... } } } PyPPL ( config ) . start ( ... ) . run () # uses configurations of proc # for profile: # PyPPL(config).start(...).run( runWithSlurm ) The full configuration: slurmRunner : { preScript : export PATH=$PATH:/path/to/add , // default: postScript : # some cleanup , // default: // commands (some slurm systems have variants of commands) sbatch : yhbatch , // default: sbatch srun : yhrun , // default: srun squeue : yhqueue , // default: squeue // the prefix add to command you want to run // i.e srun -n8 hostname // it defaults to the command you specified to slurmRunner[ srun ] // In this case: yhrun cmdPrefix : srun -n8 , // default: slurmRunner[ srun ] // sbatch options (with prefix slurm. ): slurm.p : normal , slurm.mem : 1GB , // other options // ...... // Note that job name (slurm.J), stdout (slurm.o), stderr file (slurm.e) is calculated by the runner. // Although you can, you are not recommended to set them here. } Dry-run a pipeline You can use dry runner to dry-run a pipeline. The real script will not be running, instead, it just tries to touch the output files and create the output directories. When RunnerDry is being used All processes are running on local machine. Expectations won't be checked. Processes won't be cached. Output files/directories won't be exported. Better set runner of all processes in a pipeline to dry . ( pyppl().starts(...).run('dry') ), since empty file/directory will be created for output. Problems will happen if you have a non-dry-run process depending on dry-run processes. Define your own runner You are also able to define your own runner, which should be a class extends Runner (jobs run immediately after submission) or RunnerQueue (jobs are put into a queue after submission). There are several methods and variables you may need to redefine (You may check the API documentation for all available methods and variables). The class name MUST start with Runner and end with the runner name with first letter capitalized. For example, to define the runner my : from pyppl.runners import Runner class RunnerMy ( Runner ): pass The base class Runner defines the runners where the jobs will immediately run after submission; while RunnerQueue defines the runners where the jobs will be put into a queue and wait for its turn to run (for example, clusters). Before you define your own runner, you may have to write a helper class, which includes following methods: __init__(self, script, cmds = None) The constructor. script : The real script to run. cmds : Some extra commands, such as qsub , qstat and qdel for sge runner @property pid(self) : How to get the job id from job pid file. @pid.setter pid(self, pid) : Save job id to job pid file. submit(self) : How to submit the job kill(self) : How to kill the job alive(self) : Tell if a job is alive See https://github.com/pwwang/PyPPL/blob/master/pyppl/runners/helpers.py . Then it's easy to write your own runner, just parse the configuration and create a real script to run, and use it to initialize a helper . Example: a delay runner: class RunnerDelay ( Runner ): def __init__ ( self , job ): Constructor @params: `job`: The job object super ( RunnerDelay , self ) . __init__ ( job ) # construct an local script delayfile = self . job . script + .delay delaysrc = [ #!/usr/bin/env bash ] delaysrc . append ( sleep 10 ) delaysrc . append ( self . cmd2run ) with open ( delayfile , w ) as f : f . write ( \\n . join ( delaysrc ) + \\n ) self . helper = DelayHelper ( delayfile ) Key points in writing your own runner : Write a proper helper class Compose the right script to run the job ( self.script ) in __init__ . MAKE SURE you save the identity of the job to job.pidfile , rc to job.rcfile , stdout to job.outfile and stderr to job.errfile Register your runner It very easy to register your runner, just do PyPPL.registerRunner (RunnerMy) (static method) before you start to run the pipeline. The 5 built-in runners have already been registered: PyPPL . registerRunner ( RunnerLocal ) PyPPL . registerRunner ( RunnerSsh ) PyPPL . registerRunner ( RunnerSge ) PyPPL . registerRunner ( RunnerSlurm ) PyPPL . registerRunner ( RunnerDry ) To register yours: PyPPL . registerRunner ( RunnerMy ) After registration, you are able to ask a process to use it: pXXX.runner = \"my\"","title":"Runners and running profiles"},{"location":"runners/#runners-and-running-profiles","text":"","title":"Runners and running profiles"},{"location":"runners/#running-profile","text":"A running profile defines the parameters that needed for a pipeline to run. Generally it contains the runner, the parameters for the runner and the common settings for the processes. A typical running profile is as follows: { runner : sge , sgeRunner : { queue : 1-day }, forks : 32 } Caution You may also put other settings of processes into a running profile, but keep in mind: 1. The value will not be overridden if the attribute is set explicitly (i.e: p.forks = 10 ) 2. Only set common attributes for all processes in a pipeline to avoid unexprected behavior. For example, you probably don't want this in general cases to set the same script for all processes: { script : file:/path/to/script }","title":"Running profile"},{"location":"runners/#defining-running-profiles","text":"You may pre-define some profiles so that you can easily swith them by: PyPPL () . start ( pXXX ) . run ( profile1 ) PyPPL () . start ( pXXX ) . run ( profile2 ) You can define profiles in PyPPL 's default configuration files: $HOME/.PyPPL.yaml , $HOME/.PyPPL and/or $HOME/.PyPPL.json . The latter ones have high priorities. $HOME/.PyPPL should also be in JSON format. Take $HOME/.PyPPL.yaml (requiring pyyaml ) for example, the content is like: default : runner : local forks : 1 echo : stderr profile1 : runner : sge sgeRunner : queue : 1-day profile2 : runner : sge sgeRunner : queue : 7-days Note If a key is not in a profile, then it will be inherited from default . You may also define some profiles in a file somewhere else, say /path/to/myprofiles.yaml . Just pass the file to PyPPL constructor: PyPPL ( cfgfile = /path/to/myprofiles.yaml ) . start ( pXXX ) . run ( profile1 ) Note This has higher priority than default configuration files. You can also pass the profiles to PyPPL constructor directly: PyPPL ({ default : { runner : local , forks : 1 , echo : stderr }, profile1 : { runner : sge , sgeRunner : { queue : 1-day } }, profile2 : { runner : sge , sgeRunner : { queue : 7-days } } }) . start ( pXXX ) . run ( profile1 ) Note In this way, the profiles have higher priorities than the ones defined in configuration files. Or even, you can also specify a profile to run function to ask the pipeline run with the profile directly: PyPPL () . start ( pXXX ) . run ({ runner : sge , sgeRunner : { queue : 1-day } }) Note This has the highest priority.","title":"Defining running profiles"},{"location":"runners/#built-in-runners","text":"We have 5 built-in runners ( RunnerLocal , RunnerSsh , RunnerSge , RunnerSlurm , runnerDry ), you can also define you own runners. You can either tell one process to use a runner, or even, you can tell the pipeline to use one runner for all the processes. That means each process can have the same runner or a different one. To tell a process which runner to use, just specify the runner name to pXXX.runner (for example, pXXX.runner = \"sge\" to use the sge runner). Each process may use different configuration for the runner ( pXXX.sgeRunner ) or the same one by configuring the pipeline .","title":"Built-in runners"},{"location":"runners/#configurations-for-ssh-runner","text":"Ssh runner takes the advantage to use the computing resources from other servers that can be connected via ssh . The ssh command allows us to pass the command to the server and execute it: ssh [options] [command] Caution ssh runner only works when the servers share the same file system. you have to configure so that you don't need a password to log onto the servers, or use a private key to connect to the ssh servers. The jobs will be distributed equally to the servers. To tell a process the available ssh servers: pXXX . sshRunner = { servers : [ server1 , server2 , ... ], keys : [ /path/to/keyfile1 , /path/to/keyfile2 , ... ] } You can have complicated ssh configurations which can be set by the system ssh config subsystem: $HOME/.ssh/config : # contents of $HOME/.ssh/config Host dev HostName dev.example.com Port 22000 User fooey If you use different usernames to log on the servers, you may also specify the usernames as well: pXXX . sshRunner = { servers : [ user1@server1 , user2@server2 , ... ]} You can also add preScript and postScript for all jobs: pXXX . sshRunner = { servers :[ ... ], preScript : mkdir some/dir/to/be/made , postScript : rm -rf /path/to/job/tmp/dir } To make a running profile with it for a pipeline for all processes: PyPPL ({ # default profile default : { sshRunner : { servers : [ user1@server1 , user2@server2 , ... ]} }, ssh3 : { runner : ssh , sshRunner : { servers : [ server1 , server2 , server3 ], keys : [ /path/to/key1 , /path/to/key2 , /path/to/key3 ] } } }) Also see \" pipeline configration \" for more details. The constructor of the runner will change the actual script to run the following ( workdir /0/job.script.ssh ): #!/usr/bin/env bash ssh -i /path/to/key1 user1@server1 cd cwd ; workdir /0/job.script","title":"Configurations for ssh runner"},{"location":"runners/#configurations-for-sge-runner","text":"Similarly, you can also submit your jobs to SGE servers using qsub . To set the options for a process: pXXX . sgeRunner = { sge.q : 1-day , # the queue sge.M : user@domain.com , # The email for notification sge.l : h_vmem=4G , sge.l : h_stack=512M , # Remember to add an extra space # so that it won t override the previous sge.l sge.m : abe , # When to notify sge.notify : True , preScript : source /home/user/.bash_profile /dev/null; mkdir /tmp/my , # load the environment and create the temporary directory postScript : rm -rf /tmp/my # clean up } Please check man qsub to find other options. Remember to add a sge. prefix to the option name. To make a running profile with it for a pipeline for all processes: PyPPL ({ proc : { sgeRunner : { #... } } }) Also see \" pipeline configuration \" for more details. The constructor of the runner will change the script to run to the following ( workdir /0/job.script.sge ): #!/usr/bin/env bash #$ -N id . tag .0 #$ -q 1-day #$ -o workdir /PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stdout #$ -e workdir /PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stderr #$ -cwd #$ -M wang.panwen@mayo.edu #$ -m abe #$ -l h_vmem=4G #$ -l h_stack=512M #$ -notify trap status=\\$?; echo \\$status workdir /0/job.rc; exit \\$status 1 2 3 6 7 8 9 10 11 12 15 16 17 EXIT source /home/whoever/.bash_profile /dev/null ; mkdir /tmp/my workdir /0/job.script rm -rf /tmp/my","title":"Configurations for sge runner"},{"location":"runners/#configurations-for-slurm-runner","text":"Where to configure it: For single process: pXXX . slurmRunner = { ... } For running profiles: config = { proc : { ... # other configurations runner : slurm , # all processes run with slurm slurmRunner : { ... } }, # or you can also create a profile runWithSlurm : { ... # other configurations runner : slurm , slurmRunner : { ... } } } PyPPL ( config ) . start ( ... ) . run () # uses configurations of proc # for profile: # PyPPL(config).start(...).run( runWithSlurm ) The full configuration: slurmRunner : { preScript : export PATH=$PATH:/path/to/add , // default: postScript : # some cleanup , // default: // commands (some slurm systems have variants of commands) sbatch : yhbatch , // default: sbatch srun : yhrun , // default: srun squeue : yhqueue , // default: squeue // the prefix add to command you want to run // i.e srun -n8 hostname // it defaults to the command you specified to slurmRunner[ srun ] // In this case: yhrun cmdPrefix : srun -n8 , // default: slurmRunner[ srun ] // sbatch options (with prefix slurm. ): slurm.p : normal , slurm.mem : 1GB , // other options // ...... // Note that job name (slurm.J), stdout (slurm.o), stderr file (slurm.e) is calculated by the runner. // Although you can, you are not recommended to set them here. }","title":"Configurations for slurm runner"},{"location":"runners/#dry-run-a-pipeline","text":"You can use dry runner to dry-run a pipeline. The real script will not be running, instead, it just tries to touch the output files and create the output directories. When RunnerDry is being used All processes are running on local machine. Expectations won't be checked. Processes won't be cached. Output files/directories won't be exported. Better set runner of all processes in a pipeline to dry . ( pyppl().starts(...).run('dry') ), since empty file/directory will be created for output. Problems will happen if you have a non-dry-run process depending on dry-run processes.","title":"Dry-run a pipeline"},{"location":"runners/#define-your-own-runner","text":"You are also able to define your own runner, which should be a class extends Runner (jobs run immediately after submission) or RunnerQueue (jobs are put into a queue after submission). There are several methods and variables you may need to redefine (You may check the API documentation for all available methods and variables). The class name MUST start with Runner and end with the runner name with first letter capitalized. For example, to define the runner my : from pyppl.runners import Runner class RunnerMy ( Runner ): pass The base class Runner defines the runners where the jobs will immediately run after submission; while RunnerQueue defines the runners where the jobs will be put into a queue and wait for its turn to run (for example, clusters). Before you define your own runner, you may have to write a helper class, which includes following methods: __init__(self, script, cmds = None) The constructor. script : The real script to run. cmds : Some extra commands, such as qsub , qstat and qdel for sge runner @property pid(self) : How to get the job id from job pid file. @pid.setter pid(self, pid) : Save job id to job pid file. submit(self) : How to submit the job kill(self) : How to kill the job alive(self) : Tell if a job is alive See https://github.com/pwwang/PyPPL/blob/master/pyppl/runners/helpers.py . Then it's easy to write your own runner, just parse the configuration and create a real script to run, and use it to initialize a helper . Example: a delay runner: class RunnerDelay ( Runner ): def __init__ ( self , job ): Constructor @params: `job`: The job object super ( RunnerDelay , self ) . __init__ ( job ) # construct an local script delayfile = self . job . script + .delay delaysrc = [ #!/usr/bin/env bash ] delaysrc . append ( sleep 10 ) delaysrc . append ( self . cmd2run ) with open ( delayfile , w ) as f : f . write ( \\n . join ( delaysrc ) + \\n ) self . helper = DelayHelper ( delayfile ) Key points in writing your own runner : Write a proper helper class Compose the right script to run the job ( self.script ) in __init__ . MAKE SURE you save the identity of the job to job.pidfile , rc to job.rcfile , stdout to job.outfile and stderr to job.errfile","title":"Define your own runner"},{"location":"runners/#register-your-runner","text":"It very easy to register your runner, just do PyPPL.registerRunner (RunnerMy) (static method) before you start to run the pipeline. The 5 built-in runners have already been registered: PyPPL . registerRunner ( RunnerLocal ) PyPPL . registerRunner ( RunnerSsh ) PyPPL . registerRunner ( RunnerSge ) PyPPL . registerRunner ( RunnerSlurm ) PyPPL . registerRunner ( RunnerDry ) To register yours: PyPPL . registerRunner ( RunnerMy ) After registration, you are able to ask a process to use it: pXXX.runner = \"my\"","title":"Register your runner"},{"location":"set-other-properties-of-a-process/","text":"Other attributes of a process Currently we introduced in previous chapters a set of attributes of a process and we will introduce the rest of them in this chapter: Attribute Meaning Possibile values/types Default value Where it's first mentioned id The id of the process str the variable name Link tag The tag of the process, makes it possible to have two processes with the same id but different tag . str \"notag\" Link desc The description of the process. str \"No description\" echo Whether to print out the stdout and stderr bool / dict False Link input The input of the process dict / list / str Link output The output of the process list / str / OrderedDict Link script The script of the process str Link lang The language for the script str \"bash\" Link exdir The export directory str Link exhow How to export \"move\" , \"copy\" , \"symlink\" , \"gzip\" \"move\" Link exow Whether to overwrite existing files when export bool True Link cache Whether to cache the process True , False , \"export\" True Link runner Which runner to use str \"local\" Link ppldir The directory to store workdir s for all processes in this pipeline str \"./workdir\" Link workdir The work directory of the process str \" id . tag . uid \" Link expart Partial export str / list Link brings Definition of bring-in files str / list Link template The name of the template engine str PyPPL Link envs Environments for the template engine dict Link cclean Whether do cleanup (output checking/exporting) if a job was cached. bool False Link dirsig Get the modified time for directory recursively (taking into account the dirs and files in it) for cache checking bool True Link errhow What's next if jobs fail \"terminate\" , \"retry\" , \"ignore\" \"terminate\" Link errntry If errhow is \"retry\" , how many time to re-try? int 3 Link expect A command to check whether expected results generated str Link nthread Number of theads used for job construction and submission int min(int(cpu_count() / 2), 16) - args The arguments for the process dict {} This chapter rc Valid return codes str / list / int 0 This chapter beforeCmd The command to run before jobs run str This chapter afterCmd The command to run after jobs finish str This chapter depends The processes the process depends on proc / list This chapter callback The callback, called after the process finishes callable This chapter callfront The callfront, called after properties are computed callable This chapter Hint Instead of using setattr ( pXXX. attrname = attrvalue ), Now you may also pass the attrname and attrvalue to the Proc constructor (since v1.0.1): pXXX = Proc ( ... , attrname = attrvalue , ... ) Set arguments of a process pXXX.args It is a dict used to set some common arguments shared within the process (different jobs). For example, all jobs use the same program: bedtools . but to make the process portable and shareable, you may want others can give a different path of bedtools as well. Then you can use pXXX.args : pXXX = Proc () pXXX . input = { infile1:file, infile2:file : [( file1.bed , file2.bed )]} pXXX . output = outfile:file:{{in.infile1 | fn}}.out pXXX . args = { bedtools : /path/to/bedtools } # You can also do: # pXXX.args.bedtools = /path/to/bedtools pXXX . script = {{args.bedtools}} intersect -a {{in.infile1}} -b {{in.infile2}} {{out.outfile}} That's NOT recommended that you put it in the input channel: pXXX = proc () pXXX . input = { infile1:file, infile2:file, bedtools : [( file1.bed , file2.bed , /path/to/bedtools )]} pXXX . output = outfile:file:{{infile.fn}}.out pXXX . script = {{bedtools}} intersect -a {{infile1}} -b {{infile2}} {{outfile}} Of course, you can do that, but a common argument is not usually generated from prior processes, then you have to modify the input channels. If the argument is a file, and you put it in input with type file , PyPPL will try to create a link in indir . If you have 100 jobs, we need to do that 100 times or to determine whether the link exists for 100 times. You may not want that to happen. Caution Never use a key with dot . in pXXX.args , since we use {{args.key}} to access it. Hint PyPPL uses a built-in class Box to allow dot to be used to refer the attributes. So you can set the value of args like this: pXXX . args . bedtools = bedtools Set the valid return/exit codes pXXX.rc When a program exits, it will return a code (or exit status ), usually a small integer to exhibit it's status. Generally if a program finishes successfully, it will return 0 , which is the default value of p.rc . pyppl relies on this return code to determine whether a job finishes successfully. If not, p.errorhow will be triggered. You can set multiple valid return codes for a process: p . rc = [ 0 , 1 ] #or 0,1 # exit code with 0 or 1 will be both regarded as success Caution beforeCmd / afterCmd only run locally, no matter which runner you choose to run the jobs. Set the processes current process depends on pXXX.depends A process can not only depend on a single process: p2 . depends = p1 but also multiple processes p2 . depends = p1 , p0 To set prior processes not only let the process use the output channel as input for current process, but also determines when the process starts to run (right after the prior processes finish). Caution You can copy a process by p2 = p.copy() , but remember depends will not be copied, you have to specify it for the copied processes. When you specify new dependents for a process, its original ones will be removed, which means each time pXXX.depends will overwrite the previous setting. Use callback to modify the process pXXX.callback The processes NOT initialized until it's ready to run. So you may not be able to modify some of the values until it is initialized. For example, you may want to change the output channel before it passes to the its dependent process: pSingle = Proc () pSingle . input = { infile:file : [ file1.txt , file2.txt , file3.txt ]} pSingle . output = outfile:file:{{in.infile | fn}}.sorted pSingle . script = # Sort {{in. infile}} and save to {{out.outfile}}.sorted # pSingle.channel == [( file1.sorted ,), ( file2.sorted ,), ( file3.sorted ,)] # BUT NOT NOW!! the output channel is only generated after the process finishes pCombine = Proc () pCombine . depends = pSingle pCombine . input = indir:file # the directory contains file1.sorted , file2.sorted , file3.sorted pCombine . output = outfile:{{in.indir | fn}}.combined pCombine . script = # combine files to {{out.outfile}}.combined # To use the directory of file1.sorted , file2.sorted , file3.sorted as the input channel for pCombine # You can use callback def pSingleCallback ( p ): p . channel = p . channel . collapse () pSingle . callback = pSingleCallback PyPPL () . start ( pSingle ) . run () You can also use a callback in pCombine.input to modify the channel, see here , which is recommended. Because p.callback will change the original output channel of pSingle , but the input callback will keep the output channel intact. However, p.callback can not only change the output channel, but also change other properties of current process or even set the properties of coming processes. Use callfront to modify the process pXXX.callfront One possible scenario is that, value in pXXX.args depends on the other process. For example: # generate bam files # ... pBam . output = bamfile:file:{{in.infile | fn}}.bam # generate/download reference file, and index it # ... pRef . output = reffile:file:{{in.in}}.fa # call variance # ... pCall . depends = pBam , pRef pCall . callfront = lambda p : p . args . update ({ reffile : pRef . channel . reffile [ 0 ][ 0 ]}) # pCall also depends on pRef, as it needs the reference file to run. # But you may not want to put it in input. PyPPL () . start ( pBam , pRef ) . run ()","title":"Other attributes of a process"},{"location":"set-other-properties-of-a-process/#other-attributes-of-a-process","text":"Currently we introduced in previous chapters a set of attributes of a process and we will introduce the rest of them in this chapter: Attribute Meaning Possibile values/types Default value Where it's first mentioned id The id of the process str the variable name Link tag The tag of the process, makes it possible to have two processes with the same id but different tag . str \"notag\" Link desc The description of the process. str \"No description\" echo Whether to print out the stdout and stderr bool / dict False Link input The input of the process dict / list / str Link output The output of the process list / str / OrderedDict Link script The script of the process str Link lang The language for the script str \"bash\" Link exdir The export directory str Link exhow How to export \"move\" , \"copy\" , \"symlink\" , \"gzip\" \"move\" Link exow Whether to overwrite existing files when export bool True Link cache Whether to cache the process True , False , \"export\" True Link runner Which runner to use str \"local\" Link ppldir The directory to store workdir s for all processes in this pipeline str \"./workdir\" Link workdir The work directory of the process str \" id . tag . uid \" Link expart Partial export str / list Link brings Definition of bring-in files str / list Link template The name of the template engine str PyPPL Link envs Environments for the template engine dict Link cclean Whether do cleanup (output checking/exporting) if a job was cached. bool False Link dirsig Get the modified time for directory recursively (taking into account the dirs and files in it) for cache checking bool True Link errhow What's next if jobs fail \"terminate\" , \"retry\" , \"ignore\" \"terminate\" Link errntry If errhow is \"retry\" , how many time to re-try? int 3 Link expect A command to check whether expected results generated str Link nthread Number of theads used for job construction and submission int min(int(cpu_count() / 2), 16) - args The arguments for the process dict {} This chapter rc Valid return codes str / list / int 0 This chapter beforeCmd The command to run before jobs run str This chapter afterCmd The command to run after jobs finish str This chapter depends The processes the process depends on proc / list This chapter callback The callback, called after the process finishes callable This chapter callfront The callfront, called after properties are computed callable This chapter Hint Instead of using setattr ( pXXX. attrname = attrvalue ), Now you may also pass the attrname and attrvalue to the Proc constructor (since v1.0.1): pXXX = Proc ( ... , attrname = attrvalue , ... )","title":"Other attributes of a process"},{"location":"set-other-properties-of-a-process/#set-arguments-of-a-process-pxxxargs","text":"It is a dict used to set some common arguments shared within the process (different jobs). For example, all jobs use the same program: bedtools . but to make the process portable and shareable, you may want others can give a different path of bedtools as well. Then you can use pXXX.args : pXXX = Proc () pXXX . input = { infile1:file, infile2:file : [( file1.bed , file2.bed )]} pXXX . output = outfile:file:{{in.infile1 | fn}}.out pXXX . args = { bedtools : /path/to/bedtools } # You can also do: # pXXX.args.bedtools = /path/to/bedtools pXXX . script = {{args.bedtools}} intersect -a {{in.infile1}} -b {{in.infile2}} {{out.outfile}} That's NOT recommended that you put it in the input channel: pXXX = proc () pXXX . input = { infile1:file, infile2:file, bedtools : [( file1.bed , file2.bed , /path/to/bedtools )]} pXXX . output = outfile:file:{{infile.fn}}.out pXXX . script = {{bedtools}} intersect -a {{infile1}} -b {{infile2}} {{outfile}} Of course, you can do that, but a common argument is not usually generated from prior processes, then you have to modify the input channels. If the argument is a file, and you put it in input with type file , PyPPL will try to create a link in indir . If you have 100 jobs, we need to do that 100 times or to determine whether the link exists for 100 times. You may not want that to happen. Caution Never use a key with dot . in pXXX.args , since we use {{args.key}} to access it. Hint PyPPL uses a built-in class Box to allow dot to be used to refer the attributes. So you can set the value of args like this: pXXX . args . bedtools = bedtools","title":"Set arguments of a process pXXX.args"},{"location":"set-other-properties-of-a-process/#set-the-valid-returnexit-codes-pxxxrc","text":"When a program exits, it will return a code (or exit status ), usually a small integer to exhibit it's status. Generally if a program finishes successfully, it will return 0 , which is the default value of p.rc . pyppl relies on this return code to determine whether a job finishes successfully. If not, p.errorhow will be triggered. You can set multiple valid return codes for a process: p . rc = [ 0 , 1 ] #or 0,1 # exit code with 0 or 1 will be both regarded as success Caution beforeCmd / afterCmd only run locally, no matter which runner you choose to run the jobs.","title":"Set the valid return/exit codes pXXX.rc"},{"location":"set-other-properties-of-a-process/#set-the-processes-current-process-depends-on-pxxxdepends","text":"A process can not only depend on a single process: p2 . depends = p1 but also multiple processes p2 . depends = p1 , p0 To set prior processes not only let the process use the output channel as input for current process, but also determines when the process starts to run (right after the prior processes finish). Caution You can copy a process by p2 = p.copy() , but remember depends will not be copied, you have to specify it for the copied processes. When you specify new dependents for a process, its original ones will be removed, which means each time pXXX.depends will overwrite the previous setting.","title":"Set the processes current process depends on pXXX.depends"},{"location":"set-other-properties-of-a-process/#use-callback-to-modify-the-process-pxxxcallback","text":"The processes NOT initialized until it's ready to run. So you may not be able to modify some of the values until it is initialized. For example, you may want to change the output channel before it passes to the its dependent process: pSingle = Proc () pSingle . input = { infile:file : [ file1.txt , file2.txt , file3.txt ]} pSingle . output = outfile:file:{{in.infile | fn}}.sorted pSingle . script = # Sort {{in. infile}} and save to {{out.outfile}}.sorted # pSingle.channel == [( file1.sorted ,), ( file2.sorted ,), ( file3.sorted ,)] # BUT NOT NOW!! the output channel is only generated after the process finishes pCombine = Proc () pCombine . depends = pSingle pCombine . input = indir:file # the directory contains file1.sorted , file2.sorted , file3.sorted pCombine . output = outfile:{{in.indir | fn}}.combined pCombine . script = # combine files to {{out.outfile}}.combined # To use the directory of file1.sorted , file2.sorted , file3.sorted as the input channel for pCombine # You can use callback def pSingleCallback ( p ): p . channel = p . channel . collapse () pSingle . callback = pSingleCallback PyPPL () . start ( pSingle ) . run () You can also use a callback in pCombine.input to modify the channel, see here , which is recommended. Because p.callback will change the original output channel of pSingle , but the input callback will keep the output channel intact. However, p.callback can not only change the output channel, but also change other properties of current process or even set the properties of coming processes.","title":"Use callback to modify the process pXXX.callback"},{"location":"set-other-properties-of-a-process/#use-callfront-to-modify-the-process-pxxxcallfront","text":"One possible scenario is that, value in pXXX.args depends on the other process. For example: # generate bam files # ... pBam . output = bamfile:file:{{in.infile | fn}}.bam # generate/download reference file, and index it # ... pRef . output = reffile:file:{{in.in}}.fa # call variance # ... pCall . depends = pBam , pRef pCall . callfront = lambda p : p . args . update ({ reffile : pRef . channel . reffile [ 0 ][ 0 ]}) # pCall also depends on pRef, as it needs the reference file to run. # But you may not want to put it in input. PyPPL () . start ( pBam , pRef ) . run ()","title":"Use callfront to modify the process pXXX.callfront"},{"location":"specify-input-and-output-of-a-process/","text":"Input and output of a process Specify input of a process The input of a process of basically a dict with keys as the placeholders and the values as the input channels: p = Proc () p . input = { ph1 :[ 1 , 2 , 3 ], ph2 :[ 4 , 5 , 6 ]} # You can also use combined keys and channels # p.input = { ph1, ph2 : [(1,4), (2,5), (3,6)]} The complete form of an input key is key : type . The type could be var , file (a.k.a path , dir or folder ) and files (a.k.a paths , dirs or folders ). A type of var can be omitted. So {\"ph1\":[1,2,3], \"ph2\":[4,5,6]} is the same as {\"ph1:var\":[1,2,3], \"ph2:var\":[4,5,6]} You can also use a str or a list if a process depends on a prior process, it will automatically use the output channel of the prior process, or you want to use the arguments from command line as input channel (in most case for starting processes, which do not depend on any other processes). For example: Danger The number of input keys should be no more than that of the output from the prior process. Otherwise, there is not enough data for the keys. Note For output, dict is not supported. As we need the order of the keys and data to be kept when it's being passed on. But you may use OrderedDict . Hint If you have input keys defined by a string before, for example: p1 . input = ph1, ph2 You can then specify the input data/channel directly: p1 . input = [( 1 , 4 ), ( 2 , 5 ), ( 3 , 6 )] # same as: p1 . input = { ph1 :[ 1 , 2 , 3 ], ph2 :[ 4 , 5 , 6 ]} One thing has to be reminded is that, you can do: p1 . input = { in : a } # same as p1.input = { in : [ a ]} But you cannot do: p1 . input = in p1 . input = a # the right way is p.input = [ a ] # because PyPPL will take a as the input key instead of data, as it s a string Note When a job is being prepared, the input files (type: file , path , dir or folder ) will be linked to indir . In the template, for example, you may use {{in.infile}} to get its path. However, it may have different paths: The original path The path from indir The realpath (if the original file specified to the job is a symbolic link, it will be different from the original path) Then you are able to swith the value of {{in.infile}} using the setting p.infile : \"indir\" (default): The path from indir \"origin\" : The original path \"real\" : The realpath You may also use them directly by: {{in.IN_infile}} : The path from indir {{in.OR_infile}} : The original path {{in.RE_infile}} : The realpath Use sys.argv (see details for Channel.fromArgv ): p3 = Proc () p3 . input = in1 # same as p3.input = { in1 : channel.fromArgv ()} # Run the program: python test.py 1 2 3 # Then in job#0: {{in.in1}} - 1 # Then in job#1: {{in.in1}} - 2 # Then in job#2: {{in.in1}} - 3 p4 = Proc () p4 . input = in1, in2 # same as p4.input = { in1, in2 : channel.fromArgv ()} # Run the program: python test.py 1,a 2,b 3,c # Job#0: {{in.in1}} - 1, {{in.in2}} - a # Job#1: {{in.in1}} - 2, {{in.in2}} - b # Job#2: {{in.in1}} - 3, {{in.in2}} - c Specify files as input Use a single file: When you specify file as input, you should use file (a.k.a path , dir or folder ) flag for the type: p . input = { infile:file : channel . fromPattern ( ./*.txt )} Then PyPPL will create symbolic links in workdir / job.index /input/ . Note The {{in.infile}} will return the path of the link in indir pointing to the actual input file. If you want to get the path of the actual path, you may use: {{ in.infile | readlink }} or {{ in._infile }} - Use a list of files: Similar as a single file, but you have to specify it as files : p . input = { infiles:files : [ channel . fromPattern ( ./*.txt ) . flatten ()]} Then remember {{in.infiles}} is a list, so is {{in._infiles}} - Rename input file links When there are input files (different files) with the same basename, later ones will be renamed in indir . For example: pXXX . input = { infile1:file : /path1/to/theSameBasename.txt , infile2:file : /path2/to/theSameBasename.txt } Remember both files will have symblic links created in indir . To avoid infile2 being overwritten, the basename of the link will be theSameBasename[1].txt . If you are using built-in template functions to get the filename ( {{in.file2 | fn}} ), we can still get theSameBasename.txt instead of theSameBasename[1].txt . bn , basename , prefix act similarly. Use callback to modify the input channel You can modify the input channel of a process by a callback. For example: p1 = Proc () p1 . input = { ph1 :[ 1 , 2 , 3 ], ph2 :[ 4 , 5 , 6 ]} p1 . output = out1:{{ph1}},out2:{{ph2}} p1 . script = # your logic here # the output channel is [(1,4), (2,5), (3,6)] p2 . depends = p1 p2 . input = { in1, in2 : lambda ch : ch . slice ( 1 )} # just use the last 2 columns: [(2,5), (3,6)] # p1.channel keeps intact You can check more examples in some channel methods: channel.expand and channel.collapse . Caution If you use callback to modify the channel, you may combine the keys: in the above case \"in1, in2\": ... , or specify them independently: p2.input = {\"in1\": lambda ch: ch.slice(1,1), \"in2\": lambda ch: ch.slice(2)} . But remember, all channels from p2.depends will be passed to each callback function. For example: p2 . depends = [ p0 , p1 ] p2 . input = { in1 : lambda ch0 , ch1 : ... , in2 : labmda ch0 , ch1 : ... } # all channels from p2.depends are passed to each function Specify output of a process Different from input, instead of channels, you have to tell PyPPL how to compute the output channel. The output can be a list , str or OrderedDict ( but not a dict , as the order of keys has to be kept ). If it's str , a comma ( , ) is used to separate different keys: p . input = { invar :[ 1 ], infile:file : [ /a/b/c.txt ]} p . output = outvar:var:{{in.invar}}2, outfile:file:{{in.infile | bn}}2, outdir:dir:{{in.indir | fn}}-dir # The type var is omitted in the first element. # The output channel (pXXX.channel) will be: # [( 12 , c.txt2 , c-dir )] The output keys are automatically attached to the output channel, so you may use them to access the columns. In previous example: p . channel . outvar == [( 12 , )] p . channel . outfile == [( outdir /c.txt2 , )] p . channel . outdir == [( outdir /c-dir , )] Types of input and output Input/Output Type Aliases Behavior Example-assignment ( p.input/output=? ) Example-template-value Input var - Use the value directly {\"in:var\": [1]} {{in.in}} - 1 Input file path dir folder Create link in indir and assign the original path to in._in {\"in:file\": [\"/path/to/file\"]} {{in.in}} - indir /file {{in._in}} - /path/to/file Input files paths dirs folders Same as file but do for multiple files { \"in:files\": ([\"/path/to/file1\", \"/path/to/file2\"],) } {{in.in asquote}} - \" indir /file1\" \" indir /file2\" {{in._in asquote}} - \"/path/to/file1\" \"/path/to/file2\" Output var - Specify direct value \"out:var:{{job.index}}\" {{out.out}} - job.index Output file path Just specify the basename, output file will be generated in job.outdir \"out:file:{{in.infile fn}}.out\" {{out.out}} == outdir / filename of infile .out Output dir folder Do the same thing as file but will create the directory \"out:dir:{{in.infile fn}}-outdir\" {{out.out}} == outdir / filename of infile -outdir (automatically created) Output stdout - Link job.stdout file to outdir out:stdout:{{in.infile fn}}.out {{out.out}} == outdir / filename of infile .out Output stderr - Link job.stderr file to outdir err:stderr:{{in.infile fn}}.err {{out.err}} == outdir / filename of infile .err","title":"Input and output of a process"},{"location":"specify-input-and-output-of-a-process/#input-and-output-of-a-process","text":"","title":"Input and output of a process"},{"location":"specify-input-and-output-of-a-process/#specify-input-of-a-process","text":"The input of a process of basically a dict with keys as the placeholders and the values as the input channels: p = Proc () p . input = { ph1 :[ 1 , 2 , 3 ], ph2 :[ 4 , 5 , 6 ]} # You can also use combined keys and channels # p.input = { ph1, ph2 : [(1,4), (2,5), (3,6)]} The complete form of an input key is key : type . The type could be var , file (a.k.a path , dir or folder ) and files (a.k.a paths , dirs or folders ). A type of var can be omitted. So {\"ph1\":[1,2,3], \"ph2\":[4,5,6]} is the same as {\"ph1:var\":[1,2,3], \"ph2:var\":[4,5,6]} You can also use a str or a list if a process depends on a prior process, it will automatically use the output channel of the prior process, or you want to use the arguments from command line as input channel (in most case for starting processes, which do not depend on any other processes). For example: Danger The number of input keys should be no more than that of the output from the prior process. Otherwise, there is not enough data for the keys. Note For output, dict is not supported. As we need the order of the keys and data to be kept when it's being passed on. But you may use OrderedDict . Hint If you have input keys defined by a string before, for example: p1 . input = ph1, ph2 You can then specify the input data/channel directly: p1 . input = [( 1 , 4 ), ( 2 , 5 ), ( 3 , 6 )] # same as: p1 . input = { ph1 :[ 1 , 2 , 3 ], ph2 :[ 4 , 5 , 6 ]} One thing has to be reminded is that, you can do: p1 . input = { in : a } # same as p1.input = { in : [ a ]} But you cannot do: p1 . input = in p1 . input = a # the right way is p.input = [ a ] # because PyPPL will take a as the input key instead of data, as it s a string Note When a job is being prepared, the input files (type: file , path , dir or folder ) will be linked to indir . In the template, for example, you may use {{in.infile}} to get its path. However, it may have different paths: The original path The path from indir The realpath (if the original file specified to the job is a symbolic link, it will be different from the original path) Then you are able to swith the value of {{in.infile}} using the setting p.infile : \"indir\" (default): The path from indir \"origin\" : The original path \"real\" : The realpath You may also use them directly by: {{in.IN_infile}} : The path from indir {{in.OR_infile}} : The original path {{in.RE_infile}} : The realpath Use sys.argv (see details for Channel.fromArgv ): p3 = Proc () p3 . input = in1 # same as p3.input = { in1 : channel.fromArgv ()} # Run the program: python test.py 1 2 3 # Then in job#0: {{in.in1}} - 1 # Then in job#1: {{in.in1}} - 2 # Then in job#2: {{in.in1}} - 3 p4 = Proc () p4 . input = in1, in2 # same as p4.input = { in1, in2 : channel.fromArgv ()} # Run the program: python test.py 1,a 2,b 3,c # Job#0: {{in.in1}} - 1, {{in.in2}} - a # Job#1: {{in.in1}} - 2, {{in.in2}} - b # Job#2: {{in.in1}} - 3, {{in.in2}} - c","title":"Specify input of a process"},{"location":"specify-input-and-output-of-a-process/#specify-files-as-input","text":"Use a single file: When you specify file as input, you should use file (a.k.a path , dir or folder ) flag for the type: p . input = { infile:file : channel . fromPattern ( ./*.txt )} Then PyPPL will create symbolic links in workdir / job.index /input/ . Note The {{in.infile}} will return the path of the link in indir pointing to the actual input file. If you want to get the path of the actual path, you may use: {{ in.infile | readlink }} or {{ in._infile }} - Use a list of files: Similar as a single file, but you have to specify it as files : p . input = { infiles:files : [ channel . fromPattern ( ./*.txt ) . flatten ()]} Then remember {{in.infiles}} is a list, so is {{in._infiles}} - Rename input file links When there are input files (different files) with the same basename, later ones will be renamed in indir . For example: pXXX . input = { infile1:file : /path1/to/theSameBasename.txt , infile2:file : /path2/to/theSameBasename.txt } Remember both files will have symblic links created in indir . To avoid infile2 being overwritten, the basename of the link will be theSameBasename[1].txt . If you are using built-in template functions to get the filename ( {{in.file2 | fn}} ), we can still get theSameBasename.txt instead of theSameBasename[1].txt . bn , basename , prefix act similarly.","title":"Specify files as input"},{"location":"specify-input-and-output-of-a-process/#use-callback-to-modify-the-input-channel","text":"You can modify the input channel of a process by a callback. For example: p1 = Proc () p1 . input = { ph1 :[ 1 , 2 , 3 ], ph2 :[ 4 , 5 , 6 ]} p1 . output = out1:{{ph1}},out2:{{ph2}} p1 . script = # your logic here # the output channel is [(1,4), (2,5), (3,6)] p2 . depends = p1 p2 . input = { in1, in2 : lambda ch : ch . slice ( 1 )} # just use the last 2 columns: [(2,5), (3,6)] # p1.channel keeps intact You can check more examples in some channel methods: channel.expand and channel.collapse . Caution If you use callback to modify the channel, you may combine the keys: in the above case \"in1, in2\": ... , or specify them independently: p2.input = {\"in1\": lambda ch: ch.slice(1,1), \"in2\": lambda ch: ch.slice(2)} . But remember, all channels from p2.depends will be passed to each callback function. For example: p2 . depends = [ p0 , p1 ] p2 . input = { in1 : lambda ch0 , ch1 : ... , in2 : labmda ch0 , ch1 : ... } # all channels from p2.depends are passed to each function","title":"Use callback to modify the input channel"},{"location":"specify-input-and-output-of-a-process/#specify-output-of-a-process","text":"Different from input, instead of channels, you have to tell PyPPL how to compute the output channel. The output can be a list , str or OrderedDict ( but not a dict , as the order of keys has to be kept ). If it's str , a comma ( , ) is used to separate different keys: p . input = { invar :[ 1 ], infile:file : [ /a/b/c.txt ]} p . output = outvar:var:{{in.invar}}2, outfile:file:{{in.infile | bn}}2, outdir:dir:{{in.indir | fn}}-dir # The type var is omitted in the first element. # The output channel (pXXX.channel) will be: # [( 12 , c.txt2 , c-dir )] The output keys are automatically attached to the output channel, so you may use them to access the columns. In previous example: p . channel . outvar == [( 12 , )] p . channel . outfile == [( outdir /c.txt2 , )] p . channel . outdir == [( outdir /c-dir , )]","title":"Specify output of a process"},{"location":"specify-input-and-output-of-a-process/#types-of-input-and-output","text":"Input/Output Type Aliases Behavior Example-assignment ( p.input/output=? ) Example-template-value Input var - Use the value directly {\"in:var\": [1]} {{in.in}} - 1 Input file path dir folder Create link in indir and assign the original path to in._in {\"in:file\": [\"/path/to/file\"]} {{in.in}} - indir /file {{in._in}} - /path/to/file Input files paths dirs folders Same as file but do for multiple files { \"in:files\": ([\"/path/to/file1\", \"/path/to/file2\"],) } {{in.in asquote}} - \" indir /file1\" \" indir /file2\" {{in._in asquote}} - \"/path/to/file1\" \"/path/to/file2\" Output var - Specify direct value \"out:var:{{job.index}}\" {{out.out}} - job.index Output file path Just specify the basename, output file will be generated in job.outdir \"out:file:{{in.infile fn}}.out\" {{out.out}} == outdir / filename of infile .out Output dir folder Do the same thing as file but will create the directory \"out:dir:{{in.infile fn}}-outdir\" {{out.out}} == outdir / filename of infile -outdir (automatically created) Output stdout - Link job.stdout file to outdir out:stdout:{{in.infile fn}}.out {{out.out}} == outdir / filename of infile .out Output stderr - Link job.stderr file to outdir err:stderr:{{in.infile fn}}.err {{out.err}} == outdir / filename of infile .err","title":"Types of input and output"},{"location":"write-your-script/","text":"Write and debug your script Choose your language You can either specify the path of interpreter to pXXX.lang . If the interpreter is in $PATH , you can directly give the basename of the interpreter. For example, if you have your own perl installed at /home/user/bin/perl , then you need to tell PyPPL where it is: pXXX.lang = \"/home/user/bin/perl\" . If /home/user/bin is in your $PATH , you can simply do: p.lang = \"perl\" You can also use shebang to specify the interperter: #!/home/usr/bin/perl # You perl code goes here Use script from a file You can also put the script into a file, and use it with a file: prefix: pXXX.script = \"file:/a/b/c.pl\" Note You may also use a relative-path template, which is relative to where pXXX.script is defined. For example: pXXX.script = \"file:./scripts/script.py\" is defined in /a/b/pipeline.py , then the script file refers to /a/b/scripts/script.py Hint Indents are important in python, when you write your scripts, you have to follow exactly the indents in the script string, for example: def test (): p = Proc () p . lang = python p . script = import os import re def somefunc (): pass But with '# PYPPL INDENT REMOVE' , you can do it more elegantly: def test (): p = proc () p . lang = python p . script = # make sure it s not at the beginning of the file # PYPPL INDENT REMOVE import os import re def somefunc(): pass The leading white spaces of line # PYPPL INDENT REMOVE will be removed for each line (including itself) below it. In this case, the extra tab of pass will be kept. You may use # PYPPL INDENT KEEP to stop removing the white spaces for the following lines. Caution # PYPPL INDENT REMOVE Should not be at the beginning of the file, otherwise the leading spaces will be stripped so we can detect how many spaces should be removed for the following lines. Debug your script If you need to debug your script, you just need to find the real running script, which is at: workdir / job.index /job.script . The template is rendered already in the file. You can debug it using the tool according to the language you used for the script. You may also add logs to pyppl's main logs on the screen or in log files. To do that, you just need to print you message starting with pyppl.log to STDERR: # python import sys sys . stderr . write ( pyppl.log: Something for debug. ) # bash echo pyppl.log: Something for debug. 1 2 # Rscript cat ( pyppl.log: Something for debug. , file = stderr ()) Either one of the above will have a log message like: [2017-01-01 01:01:01][ LOG] Something for debug. You may also use a different log level (flag): # python import sys sys . stderr . write ( pyppl.log.flag: Something for debug. ) Then the log message will be: [2017-01-01 01:01:01][ FLAG] Something for debug. Note You have to tell PyPPL which jobs to output these logs. Just simply by: # You have to specify an empty string to type to disable other outputs, unless you want them. pXXX . echo = { jobs : [ 0 , 1 , 2 , 3 ], type : } Note The level name you specified after pyppl.log does not apply to normal log filters or themes , because the actual level is _FLAG in this case. So unless you set loglevels to None , it will be anyway printed out. For themes, the color at the empty string key will be used. You can define filters or themes for this kind of logs, just remember the actual level name has an _ prefix. See here to learn how to define filters and themes. Output stdout/stderr to PyPPL logs Instead of log some information, you may also choose to output the stdout/stderr from the jobs to the main PyPPL log. This is controlled by setting p.echo , which is set to False by default. The full configuration of the value could be: { jobs : [ 0 , 1 ], # the jobs that are allowed to output # the regular expression for each type of output type : { stdout : r ^STDOUT: , stderr : r ^STDERR } } But there are also some abbrevations for the setting: Abbrevation ( p.echo = ? ) Full setting Memo False False Disable output True {'jobs':[0], 'type': {'stderr':None, 'stdout':None}} Output all stdout/stderr of job #0 'stderr' {'jobs':[0], 'type': {'stderr':None}} Output all stderr of job #0 {'jobs':0, 'type': 'stdout'} {'jobs':[0], 'type': {'stdout':None}} Output all stdout of job #0 {'type': {'all': r'^output'}} { 'jobs': [0], 'type': {'stdout': r'^output', 'stderr': r'^output'} } Output all lines starting with \"output\" from stdout/stderr of job #0","title":"The heart: script"},{"location":"write-your-script/#write-and-debug-your-script","text":"","title":"Write and debug your script"},{"location":"write-your-script/#choose-your-language","text":"You can either specify the path of interpreter to pXXX.lang . If the interpreter is in $PATH , you can directly give the basename of the interpreter. For example, if you have your own perl installed at /home/user/bin/perl , then you need to tell PyPPL where it is: pXXX.lang = \"/home/user/bin/perl\" . If /home/user/bin is in your $PATH , you can simply do: p.lang = \"perl\" You can also use shebang to specify the interperter: #!/home/usr/bin/perl # You perl code goes here","title":"Choose your language"},{"location":"write-your-script/#use-script-from-a-file","text":"You can also put the script into a file, and use it with a file: prefix: pXXX.script = \"file:/a/b/c.pl\" Note You may also use a relative-path template, which is relative to where pXXX.script is defined. For example: pXXX.script = \"file:./scripts/script.py\" is defined in /a/b/pipeline.py , then the script file refers to /a/b/scripts/script.py Hint Indents are important in python, when you write your scripts, you have to follow exactly the indents in the script string, for example: def test (): p = Proc () p . lang = python p . script = import os import re def somefunc (): pass But with '# PYPPL INDENT REMOVE' , you can do it more elegantly: def test (): p = proc () p . lang = python p . script = # make sure it s not at the beginning of the file # PYPPL INDENT REMOVE import os import re def somefunc(): pass The leading white spaces of line # PYPPL INDENT REMOVE will be removed for each line (including itself) below it. In this case, the extra tab of pass will be kept. You may use # PYPPL INDENT KEEP to stop removing the white spaces for the following lines. Caution # PYPPL INDENT REMOVE Should not be at the beginning of the file, otherwise the leading spaces will be stripped so we can detect how many spaces should be removed for the following lines.","title":"Use script from a file"},{"location":"write-your-script/#debug-your-script","text":"If you need to debug your script, you just need to find the real running script, which is at: workdir / job.index /job.script . The template is rendered already in the file. You can debug it using the tool according to the language you used for the script. You may also add logs to pyppl's main logs on the screen or in log files. To do that, you just need to print you message starting with pyppl.log to STDERR: # python import sys sys . stderr . write ( pyppl.log: Something for debug. ) # bash echo pyppl.log: Something for debug. 1 2 # Rscript cat ( pyppl.log: Something for debug. , file = stderr ()) Either one of the above will have a log message like: [2017-01-01 01:01:01][ LOG] Something for debug. You may also use a different log level (flag): # python import sys sys . stderr . write ( pyppl.log.flag: Something for debug. ) Then the log message will be: [2017-01-01 01:01:01][ FLAG] Something for debug. Note You have to tell PyPPL which jobs to output these logs. Just simply by: # You have to specify an empty string to type to disable other outputs, unless you want them. pXXX . echo = { jobs : [ 0 , 1 , 2 , 3 ], type : } Note The level name you specified after pyppl.log does not apply to normal log filters or themes , because the actual level is _FLAG in this case. So unless you set loglevels to None , it will be anyway printed out. For themes, the color at the empty string key will be used. You can define filters or themes for this kind of logs, just remember the actual level name has an _ prefix. See here to learn how to define filters and themes.","title":"Debug your script"},{"location":"write-your-script/#output-stdoutstderr-to-pyppl-logs","text":"Instead of log some information, you may also choose to output the stdout/stderr from the jobs to the main PyPPL log. This is controlled by setting p.echo , which is set to False by default. The full configuration of the value could be: { jobs : [ 0 , 1 ], # the jobs that are allowed to output # the regular expression for each type of output type : { stdout : r ^STDOUT: , stderr : r ^STDERR } } But there are also some abbrevations for the setting: Abbrevation ( p.echo = ? ) Full setting Memo False False Disable output True {'jobs':[0], 'type': {'stderr':None, 'stdout':None}} Output all stdout/stderr of job #0 'stderr' {'jobs':[0], 'type': {'stderr':None}} Output all stderr of job #0 {'jobs':0, 'type': 'stdout'} {'jobs':[0], 'type': {'stdout':None}} Output all stdout of job #0 {'type': {'all': r'^output'}} { 'jobs': [0], 'type': {'stdout': r'^output', 'stderr': r'^output'} } Output all lines starting with \"output\" from stdout/stderr of job #0","title":"Output stdout/stderr to PyPPL logs"}]}