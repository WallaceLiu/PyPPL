{
    "docs": [
        {
            "location": "/", 
            "text": "PyPPL\n - A \nPy\nthon \nP\ni\nP\ne\nL\nine framework\n\n\n \n \n \n \n\n\nDocumentation\n | \nAPI\n | \nChange log\n | \nFAQ\n\n\n\n\n\nFeatures\n\n\n\n\nEasy-to-use command line parser.\n\n\nFancy logs.\n\n\nProcess caching.\n\n\nScript templating (using either builtin engine or Jinja2).\n\n\nRunner customization\n.\n\n\nError handling for processes.\n\n\nEasy-switching running profile.\n\n\nFlowchat in \nDOT\n for your pipelines (\nDetails\n).\n\n\nAggregations (a set of processes predefined).\n\n\nHighly reusable processes (see \na set of highly reusable bioinformatics processes\n).\n\n\n\n\nRequirements\n\n\n\n\nOS: Linux, OSX, WSL (Windows Subsystem for Linux) or Cygwin\n\n\nPython 2.7 or Python 3.6\n\n\nPython packages: \nsix\n, \nfilelock\n, \nloky\n, \nfutures\n (suggested: \ngraphviz\n, \npyyaml\n and \npython-testly\n).\n\n\n\n\nInstallation\n\n\n# install latest version\n\ngit clone https://github.com/pwwang/PyPPL.git\n\ncd\n PyPPL\npython setup.py install\n\n# or simply:\n\npip install git+git://github.com/pwwang/PyPPL.git\n\n\n# install released version\n\npip install PyPPL\n\n\n# run tests \n\npip install python-testly\n\n# or pip install git+git://github.com/pwwang/testly.git\n\nmake \ntest\n\n\n\n# run tests only for python2\n\nmake test2\n\n\n# run tests only for python3\n\nmake test3\n\n\n# run tutorials\n\nmake tutorials\n\n\n\n\nGet started\n\n\nSee \ntutorials/getStarted/\n\nSort 5 files simultaneously: \n\n1.\n \nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n,\n \nChannel\n\n\n\n2.\n \npSort\n         \n=\n \nProc\n(\ndesc\n \n=\n \nSort files.\n)\n\n\n3.\n \npSort\n.\ninput\n   \n=\n \n{\ninfile:file\n:\n \nChannel\n.\nfromPattern\n(\n./data/*.txt\n)}\n\n\n4.\n \npSort\n.\noutput\n  \n=\n \noutfile:file:{{in.infile | fn}}.sorted\n\n\n5.\n \npSort\n.\nforks\n   \n=\n \n5\n\n\n6.\n \npSort\n.\nexdir\n   \n=\n \n./export\n\n\n7.\n \npSort\n.\nscript\n  \n=\n \n\n\n  sort -k1r {{in.infile}} \n {{out.outfile}} \n\n\n \n\n\n8.\n \nPyPPL\n()\n.\nstart\n(\npSort\n)\n.\nrun\n()\n\n\n\n\nLine 1\n: Import the modules.\n\n\nLine 2\n: Define the process with a description.\n\n\nLine 3\n: Define the input data for the process.\n\n\nLine 4\n: Define the output. Templates are also applied here.\n\n\nLine 5\n: Define how many jobs are running simultaneously.\n\n\nLine 6\n: Set the directory to export the output files.\n\n\nLine 7\n: Set your script to run.\n\n\nLine 8\n: Set the starting process and run the pipeline.  \n\n\n\n\n ls -l ./export\ntotal 0\n-rw-rw-rw- 1 pwwang pwwang 44 Sep 14 20:50 test1.sorted\n-rw-rw-rw- 1 pwwang pwwang 56 Sep 14 20:50 test2.sorted\n-rw-rw-rw- 1 pwwang pwwang 59 Sep 14 20:50 test3.sorted\n-rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20:50 test4.sorted\n-rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20:50 test5.sorted\n\n\n\nInfer input channel from dependent process\n\n\nSee \ntutorials/inputFromDependent/\n\nIf a process depends on another one, the input channel can be inferred from the output channel of the latter process.\n\nSort 5 files and then add line number to each line.\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n,\n \nChannel\n\n\n\npSort\n        \n=\n \nProc\n(\ndesc\n \n=\n \nSort files.\n)\n\n\npSort\n.\ninput\n  \n=\n \n{\ninfile:file\n:\n \nChannel\n.\nfromPattern\n(\n./data/*.txt\n)}\n\n\npSort\n.\noutput\n \n=\n \noutfile:file:{{in.infile | fn}}.sorted\n\n\npSort\n.\nforks\n  \n=\n \n5\n\n\npSort\n.\nscript\n \n=\n \n\n\n  sort -k1r {{in.infile}} \n {{out.outfile}} \n\n\n \n\n\npAddPrefix\n         \n=\n \nProc\n(\ndesc\n \n=\n \nAdd line number to each line.\n)\n\n\npAddPrefix\n.\ndepends\n \n=\n \npSort\n\n\n# automatically inferred from pSort.output\n\n\npAddPrefix\n.\ninput\n   \n=\n \ninfile:file\n  \n\npAddPrefix\n.\noutput\n  \n=\n \noutfile:file:{{in.infile | fn}}.ln\n\n\npAddPrefix\n.\nexdir\n   \n=\n \n./export\n\n\npAddPrefix\n.\nforks\n   \n=\n \n5\n\n\npAddPrefix\n.\nscript\n  \n=\n \n\n\npaste -d. \n(seq 1 $(wc -l {{in.infile}} | cut -f1 -d\n \n)) {{in.infile}} \n {{out.outfile}}\n\n\n \n\n\nPyPPL\n()\n.\nstart\n(\npSort\n)\n.\nrun\n()\n\n\n\n\n head -3 ./export/test1.ln\n1.8984\n2.663\n3.625\n\n\n\nModify input channel\n\n\nSee \ntutorials/transformInputChannels/\n\nSort 5 files, add line numbers, and merge them into one file.\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n,\n \nChannel\n\n\n\npSort\n        \n=\n \nProc\n(\ndesc\n \n=\n \nSort files.\n)\n\n\npSort\n.\ninput\n  \n=\n \n{\ninfile:file\n:\n \nChannel\n.\nfromPattern\n(\n./data/*.txt\n)}\n\n\npSort\n.\noutput\n \n=\n \noutfile:file:{{in.infile | fn}}.sorted\n\n\npSort\n.\nforks\n  \n=\n \n5\n\n\npSort\n.\nscript\n \n=\n \n\n\n  sort -k1r {{in.infile}} \n {{out.outfile}} \n\n\n \n\n\npAddPrefix\n         \n=\n \nProc\n(\ndesc\n \n=\n \nAdd line number to each line.\n)\n\n\npAddPrefix\n.\ndepends\n \n=\n \npSort\n\n\npAddPrefix\n.\ninput\n   \n=\n \ninfile:file\n  \n# automatically inferred from pSort.output\n\n\npAddPrefix\n.\noutput\n  \n=\n \noutfile:file:{{in.infile | fn}}.ln\n\n\npAddPrefix\n.\nforks\n   \n=\n \n5\n\n\npAddPrefix\n.\nscript\n  \n=\n \n\n\npaste -d. \n(seq 1 $(wc -l {{in.infile}} | cut -f1 -d\n \n)) {{in.infile}} \n {{out.outfile}}\n\n\n \n\n\npMergeFiles\n         \n=\n \nProc\n(\ndesc\n \n=\n \nMerge files, each as a column.\n)\n\n\npMergeFiles\n.\ndepends\n \n=\n \npAddPrefix\n\n\n# Transform it into a list of files\n\n\n# [\ntest1.ln\n, \ntest2.ln\n, ..., \ntest5.ln\n]\n\n\npMergeFiles\n.\ninput\n   \n=\n \n{\ninfiles:files\n:\n \nlambda\n \nch\n:\n \n[\nch\n.\nflatten\n()]}\n\n\npMergeFiles\n.\noutput\n  \n=\n \noutfile:file:mergedfile.txt\n\n\npMergeFiles\n.\nexdir\n   \n=\n \n./export\n\n\npMergeFiles\n.\nscript\n  \n=\n \n\n\npaste {{in.infiles | asquote}} \n {{out.outfile}}\n\n\n\n\n\nPyPPL\n()\n.\nstart\n(\npSort\n)\n.\nrun\n()\n\n\n\n\n head -3 ./export/mergedfile.txt\n1.8984  1.6448  1.2915  1.7269  1.7692\n2.663   2.3369  2.26223 2.3866  2.7536\n3.625   3.28984 3.25945 3.29971 3.30204\n\n\n\nUse a different language\n\n\nSee \ntutorials/differentLang/\n\nPlot heatmap using R.\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n\n\n\npHeatmap\n        \n=\n \nProc\n(\ndesc\n \n=\n \nDraw heatmap.\n)\n\n\npHeatmap\n.\ninput\n  \n=\n \n{\nseed\n:\n \n8525\n}\n\n\npHeatmap\n.\noutput\n \n=\n \noutfile:file:heatmap.png\n\n\npHeatmap\n.\nexdir\n  \n=\n \n./export\n\n\n# Use full path \n/path/to/Rscript\n if it\ns not in $PATH\n\n\n# You can also use a shebang in script\n\n\n# in this case: #!/usr/bin/env Rscript\n\n\npHeatmap\n.\nlang\n   \n=\n \nRscript\n \n\npHeatmap\n.\nscript\n \n=\n \n\n\nset.seed({{in.seed}})\n\n\nmat = matrix(rnorm(100), ncol=10)\n\n\npng(filename = \n{{out.outfile}}\n)\n\n\nheatmap(mat)\n\n\ndev.off()\n\n\n\n\n\nPyPPL\n()\n.\nstart\n(\npHeatmap\n)\n.\nrun\n()\n\n\n\n\n./export/heatmap.png\n\n\n\n\nUse args\n\n\nSee \ntutorials/useArgs/\n\nIf the jobs are sharing the same set of configurations (in this case, the number of rows and columns of the matrix), they can be set in \npXXX.args\n. The other benefit is to make the channels intact if the configurations are not suppose to be channeling.\n\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n\n\n\npHeatmap\n           \n=\n \nProc\n(\ndesc\n \n=\n \nDraw heatmap.\n)\n\n\npHeatmap\n.\ninput\n     \n=\n \n{\nseed\n:\n \n[\n1\n,\n2\n,\n3\n]}\n\n\npHeatmap\n.\noutput\n    \n=\n \noutfile:file:heatmap{{in.seed}}.png\n\n\npHeatmap\n.\nexdir\n     \n=\n \n./export\n\n\npHeatmap\n.\nforks\n     \n=\n \n3\n\n\npHeatmap\n.\nargs\n.\nncol\n \n=\n \n10\n\n\npHeatmap\n.\nargs\n.\nnrow\n \n=\n \n10\n\n\npHeatmap\n.\nlang\n      \n=\n \nRscript\n \n# or /path/to/Rscript if it\ns not in $PATH\n\n\npHeatmap\n.\nscript\n \n=\n \n\n\nset.seed({{in.seed}})\n\n\nmat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}})\n\n\npng(filename = \n{{out.outfile}}\n, width=150, height=150)\n\n\nheatmap(mat)\n\n\ndev.off()\n\n\n\n\n\nPyPPL\n()\n.\nstart\n(\npHeatmap\n)\n.\nrun\n()\n\n\n\n\n\n\n\n\n\n\n./export/heatmap1.png\n\n\n./export/heatmap2.png\n\n\n./export/heatmap3.png\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the command line argument parser\n\n\nSee \ntutorials/useParams/\n\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n,\n \nChannel\n,\n \nparams\n\n\n\nparams\n.\ndatadir\n    \\\n  \n.\nsetRequired\n()\n  \\\n  \n.\nsetDesc\n(\nThe data directory containing the data files.\n)\n\n\n\n# or\n\n\n# params.datadir.required = True\n\n\n# params.datadir.desc     = \nThe data directory containing the data files.\n\n\n\nparams\n \n=\n \nparams\n.\nparse\n()\n\n\n\npSort\n         \n=\n \nProc\n(\ndesc\n \n=\n \nSort files.\n)\n\n\npSort\n.\ninput\n   \n=\n \n{\ninfile:file\n:\n \nChannel\n.\nfromPattern\n(\nparams\n.\ndatadir\n \n+\n \n/*.txt\n)}\n\n\npSort\n.\noutput\n  \n=\n \noutfile:file:{{in.infile | fn}}.sorted\n\n\npSort\n.\nforks\n   \n=\n \n5\n\n\npSort\n.\nexdir\n   \n=\n \n./export\n\n\npSort\n.\nscript\n  \n=\n \n\n\n  sort -k1r {{in.infile}} \n {{out.outfile}} \n\n\n \n\n\nPyPPL\n()\n.\nstart\n(\npSort\n)\n.\nrun\n()\n\n\n\nRun the pipeline:\n\n\n python useParams.py\n\n\nUSAGE:\n  useParams.py -datadir \nstr\n\n\nREQUIRED OPTIONS:\n  -datadir \nstr\n                        The data directory containing the data files.\n\nOPTIONAL OPTIONS:\n  -h, --help, -H, -?                    Print this help information.\n\n\nProvide value to \n-datadir\n:\n\n\n python useParams.py -datadir ./data\n\n\nUse a different runner\n\n\nSee \n/tutorials/differentRunner/\n\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n,\n \nChannel\n\n\n\npSort\n         \n=\n \nProc\n(\ndesc\n \n=\n \nSort files.\n)\n\n\npSort\n.\ninput\n   \n=\n \n{\ninfile:file\n:\n \nChannel\n.\nfromPattern\n(\n./data/*.txt\n)}\n\n\npSort\n.\noutput\n  \n=\n \noutfile:file:{{in.infile | fn}}.sorted\n\n\n# specify the runner\n\n\npSort\n.\nrunner\n  \n=\n \nsge\n\n\n# specify the runner options\n\n\npSort\n.\nsgeRunner\n \n=\n \n{\n\n\t\nsge.q\n \n:\n \n1-day\n\n\n}\n\n\npSort\n.\nforks\n   \n=\n \n5\n\n\npSort\n.\nexdir\n   \n=\n \n./export\n\n\npSort\n.\nscript\n  \n=\n \n\n\n  sort -k1r {{in.infile}} \n {{out.outfile}} \n\n\n \n\n\nPyPPL\n()\n.\nstart\n(\npSort\n)\n.\nrun\n()\n\n\n# or run all process with sge runner:\n\n\n# PyPPL().start(pSort).run(\nsge\n)\n\n\n# or:\n\n\n# PyPPL({\n\n\n#   \ndefault\n: {\n\n\n#       \nrunner\n: \nsge\n, \n\n\n#       \nsgeRunner\n: {\nsge.q\n: \n1-day\n}\n\n\n#   }\n\n\n# }).start(pSort).run()\n\n\n\n\nUse Jinja2 as template engine\n\n\nSee \n/tutorials/useJinja2/\n\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n,\n \nChannel\n\n\n\npSort\n          \n=\n \nProc\n(\ndesc\n \n=\n \nSort files.\n)\n\n\npSort\n.\ninput\n    \n=\n \n{\ninfile:file\n:\n \nChannel\n.\nfromPattern\n(\n./data/*.txt\n)}\n\n\n# Notice the different between builtin template engine and Jinja2\n\n\npSort\n.\noutput\n   \n=\n \noutfile:file:{{ fn(in.infile) }}.sorted\n\n\n# pSort.output = \noutfile:file:{{in.infile | fn}}.sorted\n\n\npSort\n.\nforks\n    \n=\n \n5\n\n\n# You have to have Jinja2 installed (pip install Jinja2)\n\n\npSort\n.\ntemplate\n \n=\n \nJinja2\n\n\npSort\n.\nexdir\n    \n=\n \n./export\n\n\npSort\n.\nscript\n   \n=\n \n\n\n  sort -k1r {{in.infile}} \n {{out.outfile}} \n\n\n \n\n\nPyPPL\n()\n.\nstart\n(\npSort\n)\n.\nrun\n()\n\n\n\n\nDebug your script\n\n\nSee \n/tutorials/debugScript/\n\nYou can directly go to \nworkdir\n/\njob.index\n/job.script\n to debug your script, or you can also print some values out throught \nPyPPL\n log system.\n\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n\n\n\npHeatmap\n           \n=\n \nProc\n(\ndesc\n \n=\n \nDraw heatmap.\n)\n\n\npHeatmap\n.\ninput\n     \n=\n \n{\nseed\n:\n \n[\n1\n,\n2\n,\n3\n,\n4\n,\n5\n]}\n\n\npHeatmap\n.\noutput\n    \n=\n \noutfile:file:heatmap{{in.seed}}.png\n\n\npHeatmap\n.\nexdir\n     \n=\n \n./export\n\n\n# Don\nt cache jobs for debugging\n\n\npHeatmap\n.\ncache\n     \n=\n \nFalse\n\n\n# Output debug information for all jobs, but don\nt echo stdout and stderr\n\n\npHeatmap\n.\necho\n      \n=\n \n{\njobs\n:\n \nrange\n(\n5\n),\n \ntype\n:\n \n}\n\n\npHeatmap\n.\nargs\n.\nncol\n \n=\n \n10\n\n\npHeatmap\n.\nargs\n.\nnrow\n \n=\n \n10\n\n\npHeatmap\n.\nlang\n      \n=\n \nRscript\n \n# or /path/to/Rscript if it\ns not in $PATH\n\n\npHeatmap\n.\nscript\n \n=\n \n\n\nset.seed({{in.seed}})\n\n\nmat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}})\n\n\npng(filename = \n{{out.outfile}}\n, width=150, height=150)\n\n\n\n# have to be on stderr\n\n\ncat(\npyppl.log.debug:Plotting heatmap #{{job.index | lambda x: int(x) + 1}} ...\n, file = stderr())\n\n\n\nheatmap(mat)\n\n\ndev.off()\n\n\n\n\n\nPyPPL\n({\n\n\t\n_log\n:\n \n{\n\n\t\t\nlevels\n \n:\n \nbasic\n,\n\n\t\t\nlvldiff\n:\n \n[]\n\n\t\n}\n\n\n})\n.\nstart\n(\npHeatmap\n)\n.\nrun\n()\n\n\n\nYou will get something like this in your log:\n\n\n\nSwitch runner profiles\n\n\nSee \ntutorials/siwthcRunnerProfile/\n\nWe can define a set of runner profiles in a \njson\n file (\n./profiles.json\n):\n\n\n{\n\n  \ndefault\n:\n \n{\n\n    \nrunner\n:\n \nlocal\n,\n\n    \nforks\n \n:\n \n1\n,\n\n    \nsgeRunner\n:\n \n{\n\n      \nsge.q\n:\n \n1-day\n\n    \n}\n \n  \n},\n\n  \nlocal5\n:\n \n{\n\n    \nrunner\n:\n \nlocal\n,\n\n    \nforks\n:\n  \n5\n\n  \n},\n\n  \nsge7days\n:\n \n{\n\n    \nrunner\n:\n \nsge\n,\n\n    \nsgeRunner\n:\n \n{\n\n      \nsge.q\n:\n \n7-days\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\nor you can also use \n.yaml\n(\npyyaml\n is required) file:\n\ndefault\n:\n\n  \nrunner\n:\n \nlocal\n\n  \nforks\n \n:\n \n1\n\n  \nsgeRunner\n:\n\n    \nsge.q\n:\n \n1-day\n\n\nlocal5\n:\n\n  \nrunner\n:\n \nlocal\n\n  \nforks\n \n:\n \n5\n\n\nsge7days\n:\n\n  \nrunner\n:\n \nlocal\n\n  \nsgeRunner\n:\n\n    \nsge.q\n:\n \n7-days\n\n\n\n\nTo switch profile:\n\n# default profile (default)\n\n\nPyPPL\n(\ncfgfile\n \n=\n \n./profiles.json\n)\n.\nstart\n(\npHeatmap\n)\n.\nrun\n()\n\n\n# switch to local5 or sge7days:\n\n\n# PyPPL(cfgfile = \n./profiles.json\n).start(pHeatmap).run(\nlocal5\n)\n\n\n# PyPPL(cfgfile = \n./profiles.json\n).start(pHeatmap).run(\nsge7days\n)\n\n\n\n# You may also use runner name as profile, which means to run using the runner with default options:\n\n\n# PyPPL(cfgfile = \n./profiles.json\n).start(pHeatmap).run(\nsge\n) # use 1-day queue\n\n\n\n\nDraw the pipeline chart\n\n\nPyPPL\n can generate the graph in \nDOT language\n. \n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n\n\n\np1\n \n=\n \nProc\n()\n\n\np2\n \n=\n \nProc\n()\n\n\np3\n \n=\n \nProc\n()\n\n\np4\n \n=\n \nProc\n()\n\n\np5\n \n=\n \nProc\n()\n\n\np6\n \n=\n \nProc\n()\n\n\np7\n \n=\n \nProc\n()\n\n\np8\n \n=\n \nProc\n()\n\n\np9\n \n=\n \nProc\n()\n\n\n\n\n\t\t   p1         p8\n\n\n\t\t/      \\      /\n\n\n\t p2           p3\n\n\n\t\t\\      /\n\n\n\t\t   p4         p9\n\n\n\t\t/      \\      /\n\n\n\t p5          p6 (export)\n\n\n\t\t\\      /\n\n\n\t\t  p7 (export)\n\n\n\n\np2\n.\ndepends\n \n=\n \np1\n\n\np3\n.\ndepends\n \n=\n \np1\n,\n \np8\n\n\np4\n.\ndepends\n \n=\n \np2\n,\n \np3\n\n\np4\n.\nexdir\n   \n=\n \n./export\n\n\np5\n.\ndepends\n \n=\n \np4\n\n\np6\n.\ndepends\n \n=\n \np4\n,\n \np9\n\n\np6\n.\nexdir\n   \n=\n \n./export\n\n\np7\n.\ndepends\n \n=\n \np5\n,\n \np6\n\n\np7\n.\nexdir\n   \n=\n \n./export\n\n\n\n# make sure at least one job is created.\n\n\np1\n.\ninput\n \n=\n \n{\nin\n:\n \n[\n0\n]}\n\n\np8\n.\ninput\n \n=\n \n{\nin\n:\n \n[\n0\n]}\n\n\np9\n.\ninput\n \n=\n \n{\nin\n:\n \n[\n0\n]}\n\n\n\nPyPPL\n()\n.\nstart\n(\np1\n,\n \np8\n,\n \np9\n)\n.\nflowchart\n()\n.\nrun\n()\n\n\n\n\ndrawFlowchart.pyppl.dot\n:\n\ndigraph PyPPL {\n    \np8\n [color=\n#259229\n fillcolor=\n#ffffff\n fontcolor=\n#000000\n shape=\nbox\n style=\nfilled\n]\n    \np1\n [color=\n#259229\n fillcolor=\n#ffffff\n fontcolor=\n#000000\n shape=\nbox\n style=\nfilled\n]\n    \np9\n [color=\n#259229\n fillcolor=\n#ffffff\n fontcolor=\n#000000\n shape=\nbox\n style=\nfilled\n]\n    \np7\n [color=\n#d63125\n fillcolor=\n#ffffff\n fontcolor=\n#c71be4\n shape=\nbox\n style=\nfilled\n]\n    \np5\n [color=\n#000000\n fillcolor=\n#ffffff\n fontcolor=\n#000000\n shape=\nbox\n style=\nrounded,filled\n]\n    \np4\n [color=\n#000000\n fillcolor=\n#ffffff\n fontcolor=\n#c71be4\n shape=\nbox\n style=\nrounded,filled\n]\n    \np2\n [color=\n#000000\n fillcolor=\n#ffffff\n fontcolor=\n#000000\n shape=\nbox\n style=\nrounded,filled\n]\n    \np3\n [color=\n#000000\n fillcolor=\n#ffffff\n fontcolor=\n#000000\n shape=\nbox\n style=\nrounded,filled\n]\n    \np6\n [color=\n#000000\n fillcolor=\n#ffffff\n fontcolor=\n#c71be4\n shape=\nbox\n style=\nrounded,filled\n]\n    \np2\n -\n \np4\n\n    \np3\n -\n \np4\n\n    \np1\n -\n \np2\n\n    \np1\n -\n \np3\n\n    \np6\n -\n \np7\n\n    \np4\n -\n \np5\n\n    \np4\n -\n \np6\n\n    \np5\n -\n \np7\n\n    \np8\n -\n \np3\n\n    \np9\n -\n \np6\n\n}\n\n\n\nTo generate svg file, you have to have \ngraphviz\n installed.\n\n\ndrawFlowchart.pyppl.svg\n:\n\n\n\n\nEnjoy pipelining!!!", 
            "title": "Introduction"
        }, 
        {
            "location": "/#pyppl-a-python-pipeline-framework", 
            "text": "Documentation  |  API  |  Change log  |  FAQ", 
            "title": "PyPPL - A Python PiPeLine framework"
        }, 
        {
            "location": "/#features", 
            "text": "Easy-to-use command line parser.  Fancy logs.  Process caching.  Script templating (using either builtin engine or Jinja2).  Runner customization .  Error handling for processes.  Easy-switching running profile.  Flowchat in  DOT  for your pipelines ( Details ).  Aggregations (a set of processes predefined).  Highly reusable processes (see  a set of highly reusable bioinformatics processes ).", 
            "title": "Features"
        }, 
        {
            "location": "/#requirements", 
            "text": "OS: Linux, OSX, WSL (Windows Subsystem for Linux) or Cygwin  Python 2.7 or Python 3.6  Python packages:  six ,  filelock ,  loky ,  futures  (suggested:  graphviz ,  pyyaml  and  python-testly ).", 
            "title": "Requirements"
        }, 
        {
            "location": "/#installation", 
            "text": "# install latest version \ngit clone https://github.com/pwwang/PyPPL.git cd  PyPPL\npython setup.py install # or simply: \npip install git+git://github.com/pwwang/PyPPL.git # install released version \npip install PyPPL # run tests  \npip install python-testly # or pip install git+git://github.com/pwwang/testly.git \nmake  test  # run tests only for python2 \nmake test2 # run tests only for python3 \nmake test3 # run tutorials \nmake tutorials", 
            "title": "Installation"
        }, 
        {
            "location": "/#get-started", 
            "text": "See  tutorials/getStarted/ \nSort 5 files simultaneously:  1.   from   pyppl   import   PyPPL ,   Proc ,   Channel  2.   pSort           =   Proc ( desc   =   Sort files. )  3.   pSort . input     =   { infile:file :   Channel . fromPattern ( ./data/*.txt )}  4.   pSort . output    =   outfile:file:{{in.infile | fn}}.sorted  5.   pSort . forks     =   5  6.   pSort . exdir     =   ./export  7.   pSort . script    =      sort -k1r {{in.infile}}   {{out.outfile}}     8.   PyPPL () . start ( pSort ) . run ()   Line 1 : Import the modules.  Line 2 : Define the process with a description.  Line 3 : Define the input data for the process.  Line 4 : Define the output. Templates are also applied here.  Line 5 : Define how many jobs are running simultaneously.  Line 6 : Set the directory to export the output files.  Line 7 : Set your script to run.  Line 8 : Set the starting process and run the pipeline.      ls -l ./export\ntotal 0\n-rw-rw-rw- 1 pwwang pwwang 44 Sep 14 20:50 test1.sorted\n-rw-rw-rw- 1 pwwang pwwang 56 Sep 14 20:50 test2.sorted\n-rw-rw-rw- 1 pwwang pwwang 59 Sep 14 20:50 test3.sorted\n-rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20:50 test4.sorted\n-rw-rw-rw- 1 pwwang pwwang 58 Sep 14 20:50 test5.sorted", 
            "title": "Get started"
        }, 
        {
            "location": "/#infer-input-channel-from-dependent-process", 
            "text": "See  tutorials/inputFromDependent/ \nIf a process depends on another one, the input channel can be inferred from the output channel of the latter process. \nSort 5 files and then add line number to each line. from   pyppl   import   PyPPL ,   Proc ,   Channel  pSort          =   Proc ( desc   =   Sort files. )  pSort . input    =   { infile:file :   Channel . fromPattern ( ./data/*.txt )}  pSort . output   =   outfile:file:{{in.infile | fn}}.sorted  pSort . forks    =   5  pSort . script   =      sort -k1r {{in.infile}}   {{out.outfile}}     pAddPrefix           =   Proc ( desc   =   Add line number to each line. )  pAddPrefix . depends   =   pSort  # automatically inferred from pSort.output  pAddPrefix . input     =   infile:file    pAddPrefix . output    =   outfile:file:{{in.infile | fn}}.ln  pAddPrefix . exdir     =   ./export  pAddPrefix . forks     =   5  pAddPrefix . script    =    paste -d.  (seq 1 $(wc -l {{in.infile}} | cut -f1 -d   )) {{in.infile}}   {{out.outfile}}    PyPPL () . start ( pSort ) . run ()    head -3 ./export/test1.ln\n1.8984\n2.663\n3.625", 
            "title": "Infer input channel from dependent process"
        }, 
        {
            "location": "/#modify-input-channel", 
            "text": "See  tutorials/transformInputChannels/ \nSort 5 files, add line numbers, and merge them into one file. from   pyppl   import   PyPPL ,   Proc ,   Channel  pSort          =   Proc ( desc   =   Sort files. )  pSort . input    =   { infile:file :   Channel . fromPattern ( ./data/*.txt )}  pSort . output   =   outfile:file:{{in.infile | fn}}.sorted  pSort . forks    =   5  pSort . script   =      sort -k1r {{in.infile}}   {{out.outfile}}     pAddPrefix           =   Proc ( desc   =   Add line number to each line. )  pAddPrefix . depends   =   pSort  pAddPrefix . input     =   infile:file    # automatically inferred from pSort.output  pAddPrefix . output    =   outfile:file:{{in.infile | fn}}.ln  pAddPrefix . forks     =   5  pAddPrefix . script    =    paste -d.  (seq 1 $(wc -l {{in.infile}} | cut -f1 -d   )) {{in.infile}}   {{out.outfile}}    pMergeFiles           =   Proc ( desc   =   Merge files, each as a column. )  pMergeFiles . depends   =   pAddPrefix  # Transform it into a list of files  # [ test1.ln ,  test2.ln , ...,  test5.ln ]  pMergeFiles . input     =   { infiles:files :   lambda   ch :   [ ch . flatten ()]}  pMergeFiles . output    =   outfile:file:mergedfile.txt  pMergeFiles . exdir     =   ./export  pMergeFiles . script    =    paste {{in.infiles | asquote}}   {{out.outfile}}   PyPPL () . start ( pSort ) . run ()    head -3 ./export/mergedfile.txt\n1.8984  1.6448  1.2915  1.7269  1.7692\n2.663   2.3369  2.26223 2.3866  2.7536\n3.625   3.28984 3.25945 3.29971 3.30204", 
            "title": "Modify input channel"
        }, 
        {
            "location": "/#use-a-different-language", 
            "text": "See  tutorials/differentLang/ \nPlot heatmap using R. from   pyppl   import   PyPPL ,   Proc  pHeatmap          =   Proc ( desc   =   Draw heatmap. )  pHeatmap . input    =   { seed :   8525 }  pHeatmap . output   =   outfile:file:heatmap.png  pHeatmap . exdir    =   ./export  # Use full path  /path/to/Rscript  if it s not in $PATH  # You can also use a shebang in script  # in this case: #!/usr/bin/env Rscript  pHeatmap . lang     =   Rscript   pHeatmap . script   =    set.seed({{in.seed}})  mat = matrix(rnorm(100), ncol=10)  png(filename =  {{out.outfile}} )  heatmap(mat)  dev.off()   PyPPL () . start ( pHeatmap ) . run ()   ./export/heatmap.png", 
            "title": "Use a different language"
        }, 
        {
            "location": "/#use-args", 
            "text": "See  tutorials/useArgs/ \nIf the jobs are sharing the same set of configurations (in this case, the number of rows and columns of the matrix), they can be set in  pXXX.args . The other benefit is to make the channels intact if the configurations are not suppose to be channeling.  from   pyppl   import   PyPPL ,   Proc  pHeatmap             =   Proc ( desc   =   Draw heatmap. )  pHeatmap . input       =   { seed :   [ 1 , 2 , 3 ]}  pHeatmap . output      =   outfile:file:heatmap{{in.seed}}.png  pHeatmap . exdir       =   ./export  pHeatmap . forks       =   3  pHeatmap . args . ncol   =   10  pHeatmap . args . nrow   =   10  pHeatmap . lang        =   Rscript   # or /path/to/Rscript if it s not in $PATH  pHeatmap . script   =    set.seed({{in.seed}})  mat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}})  png(filename =  {{out.outfile}} , width=150, height=150)  heatmap(mat)  dev.off()   PyPPL () . start ( pHeatmap ) . run ()      ./export/heatmap1.png  ./export/heatmap2.png  ./export/heatmap3.png", 
            "title": "Use args"
        }, 
        {
            "location": "/#use-the-command-line-argument-parser", 
            "text": "See  tutorials/useParams/  from   pyppl   import   PyPPL ,   Proc ,   Channel ,   params  params . datadir     \\\n   . setRequired ()   \\\n   . setDesc ( The data directory containing the data files. )  # or  # params.datadir.required = True  # params.datadir.desc     =  The data directory containing the data files.  params   =   params . parse ()  pSort           =   Proc ( desc   =   Sort files. )  pSort . input     =   { infile:file :   Channel . fromPattern ( params . datadir   +   /*.txt )}  pSort . output    =   outfile:file:{{in.infile | fn}}.sorted  pSort . forks     =   5  pSort . exdir     =   ./export  pSort . script    =      sort -k1r {{in.infile}}   {{out.outfile}}     PyPPL () . start ( pSort ) . run ()  \nRun the pipeline:   python useParams.py  USAGE:\n  useParams.py -datadir  str \n\nREQUIRED OPTIONS:\n  -datadir  str                         The data directory containing the data files.\n\nOPTIONAL OPTIONS:\n  -h, --help, -H, -?                    Print this help information. \nProvide value to  -datadir :   python useParams.py -datadir ./data", 
            "title": "Use the command line argument parser"
        }, 
        {
            "location": "/#use-a-different-runner", 
            "text": "See  /tutorials/differentRunner/  from   pyppl   import   PyPPL ,   Proc ,   Channel  pSort           =   Proc ( desc   =   Sort files. )  pSort . input     =   { infile:file :   Channel . fromPattern ( ./data/*.txt )}  pSort . output    =   outfile:file:{{in.infile | fn}}.sorted  # specify the runner  pSort . runner    =   sge  # specify the runner options  pSort . sgeRunner   =   { \n\t sge.q   :   1-day  }  pSort . forks     =   5  pSort . exdir     =   ./export  pSort . script    =      sort -k1r {{in.infile}}   {{out.outfile}}     PyPPL () . start ( pSort ) . run ()  # or run all process with sge runner:  # PyPPL().start(pSort).run( sge )  # or:  # PyPPL({  #    default : {  #        runner :  sge ,   #        sgeRunner : { sge.q :  1-day }  #   }  # }).start(pSort).run()", 
            "title": "Use a different runner"
        }, 
        {
            "location": "/#use-jinja2-as-template-engine", 
            "text": "See  /tutorials/useJinja2/  from   pyppl   import   PyPPL ,   Proc ,   Channel  pSort            =   Proc ( desc   =   Sort files. )  pSort . input      =   { infile:file :   Channel . fromPattern ( ./data/*.txt )}  # Notice the different between builtin template engine and Jinja2  pSort . output     =   outfile:file:{{ fn(in.infile) }}.sorted  # pSort.output =  outfile:file:{{in.infile | fn}}.sorted  pSort . forks      =   5  # You have to have Jinja2 installed (pip install Jinja2)  pSort . template   =   Jinja2  pSort . exdir      =   ./export  pSort . script     =      sort -k1r {{in.infile}}   {{out.outfile}}     PyPPL () . start ( pSort ) . run ()", 
            "title": "Use Jinja2 as template engine"
        }, 
        {
            "location": "/#debug-your-script", 
            "text": "See  /tutorials/debugScript/ \nYou can directly go to  workdir / job.index /job.script  to debug your script, or you can also print some values out throught  PyPPL  log system.  from   pyppl   import   PyPPL ,   Proc  pHeatmap             =   Proc ( desc   =   Draw heatmap. )  pHeatmap . input       =   { seed :   [ 1 , 2 , 3 , 4 , 5 ]}  pHeatmap . output      =   outfile:file:heatmap{{in.seed}}.png  pHeatmap . exdir       =   ./export  # Don t cache jobs for debugging  pHeatmap . cache       =   False  # Output debug information for all jobs, but don t echo stdout and stderr  pHeatmap . echo        =   { jobs :   range ( 5 ),   type :   }  pHeatmap . args . ncol   =   10  pHeatmap . args . nrow   =   10  pHeatmap . lang        =   Rscript   # or /path/to/Rscript if it s not in $PATH  pHeatmap . script   =    set.seed({{in.seed}})  mat = matrix(rnorm({{args.ncol, args.nrow | lambda x, y: x*y}}), ncol={{args.ncol}})  png(filename =  {{out.outfile}} , width=150, height=150)  # have to be on stderr  cat( pyppl.log.debug:Plotting heatmap #{{job.index | lambda x: int(x) + 1}} ... , file = stderr())  heatmap(mat)  dev.off()   PyPPL ({ \n\t _log :   { \n\t\t levels   :   basic , \n\t\t lvldiff :   [] \n\t }  }) . start ( pHeatmap ) . run ()  \nYou will get something like this in your log:", 
            "title": "Debug your script"
        }, 
        {
            "location": "/#switch-runner-profiles", 
            "text": "See  tutorials/siwthcRunnerProfile/ \nWe can define a set of runner profiles in a  json  file ( ./profiles.json ):  { \n   default :   { \n     runner :   local , \n     forks   :   1 , \n     sgeRunner :   { \n       sge.q :   1-day \n     }  \n   }, \n   local5 :   { \n     runner :   local , \n     forks :    5 \n   }, \n   sge7days :   { \n     runner :   sge , \n     sgeRunner :   { \n       sge.q :   7-days \n     } \n   }  }   or you can also use  .yaml ( pyyaml  is required) file: default : \n   runner :   local \n   forks   :   1 \n   sgeRunner : \n     sge.q :   1-day  local5 : \n   runner :   local \n   forks   :   5  sge7days : \n   runner :   local \n   sgeRunner : \n     sge.q :   7-days   To switch profile: # default profile (default)  PyPPL ( cfgfile   =   ./profiles.json ) . start ( pHeatmap ) . run ()  # switch to local5 or sge7days:  # PyPPL(cfgfile =  ./profiles.json ).start(pHeatmap).run( local5 )  # PyPPL(cfgfile =  ./profiles.json ).start(pHeatmap).run( sge7days )  # You may also use runner name as profile, which means to run using the runner with default options:  # PyPPL(cfgfile =  ./profiles.json ).start(pHeatmap).run( sge ) # use 1-day queue", 
            "title": "Switch runner profiles"
        }, 
        {
            "location": "/#draw-the-pipeline-chart", 
            "text": "PyPPL  can generate the graph in  DOT language .  from   pyppl   import   PyPPL ,   Proc  p1   =   Proc ()  p2   =   Proc ()  p3   =   Proc ()  p4   =   Proc ()  p5   =   Proc ()  p6   =   Proc ()  p7   =   Proc ()  p8   =   Proc ()  p9   =   Proc ()   \t\t   p1         p8  \t\t/      \\      /  \t p2           p3  \t\t\\      /  \t\t   p4         p9  \t\t/      \\      /  \t p5          p6 (export)  \t\t\\      /  \t\t  p7 (export)   p2 . depends   =   p1  p3 . depends   =   p1 ,   p8  p4 . depends   =   p2 ,   p3  p4 . exdir     =   ./export  p5 . depends   =   p4  p6 . depends   =   p4 ,   p9  p6 . exdir     =   ./export  p7 . depends   =   p5 ,   p6  p7 . exdir     =   ./export  # make sure at least one job is created.  p1 . input   =   { in :   [ 0 ]}  p8 . input   =   { in :   [ 0 ]}  p9 . input   =   { in :   [ 0 ]}  PyPPL () . start ( p1 ,   p8 ,   p9 ) . flowchart () . run ()   drawFlowchart.pyppl.dot : digraph PyPPL {\n     p8  [color= #259229  fillcolor= #ffffff  fontcolor= #000000  shape= box  style= filled ]\n     p1  [color= #259229  fillcolor= #ffffff  fontcolor= #000000  shape= box  style= filled ]\n     p9  [color= #259229  fillcolor= #ffffff  fontcolor= #000000  shape= box  style= filled ]\n     p7  [color= #d63125  fillcolor= #ffffff  fontcolor= #c71be4  shape= box  style= filled ]\n     p5  [color= #000000  fillcolor= #ffffff  fontcolor= #000000  shape= box  style= rounded,filled ]\n     p4  [color= #000000  fillcolor= #ffffff  fontcolor= #c71be4  shape= box  style= rounded,filled ]\n     p2  [color= #000000  fillcolor= #ffffff  fontcolor= #000000  shape= box  style= rounded,filled ]\n     p3  [color= #000000  fillcolor= #ffffff  fontcolor= #000000  shape= box  style= rounded,filled ]\n     p6  [color= #000000  fillcolor= #ffffff  fontcolor= #c71be4  shape= box  style= rounded,filled ]\n     p2  -   p4 \n     p3  -   p4 \n     p1  -   p2 \n     p1  -   p3 \n     p6  -   p7 \n     p4  -   p5 \n     p4  -   p6 \n     p5  -   p7 \n     p8  -   p3 \n     p9  -   p6 \n}  To generate svg file, you have to have  graphviz  installed.  drawFlowchart.pyppl.svg :   Enjoy pipelining!!!", 
            "title": "Draw the pipeline chart"
        }, 
        {
            "location": "/basic-concepts-and-directory-structure/", 
            "text": "Basic concepts and folder structure\n\n\n\n\n\nLayers of a pipeline\n\n\n\nThe pipeline consists of channels and processes. A process may have many jobs. Each job uses the corresponding elements from the input channel of the process, and generates values for output channel.\n\nActually, what you need to do is just specify the first input channel, and then tell \nPyPPL\n the dependencies of the processes. The later processes will use the output channel of the processes they depend on. Of course, you can interfere by using functions in the input specification.\n\n\nFolder structure\n\n\n./\n|-- pipeline.py\n`-- workdir/\n\t`-- PyPPL.\nid\n.\ntag\n.\nsuffix\n/\n\t\t|-- lock\n\t\t|-- proc.settings\n\t\t`-- \njob.index\n/\n\t\t\t|-- input/\n\t\t\t|-- output/\n\t\t\t|-- job.cache\n\t\t\t|-- job.script\n\t\t\t|-- job.pid\n\t\t\t|-- job.rc\n\t\t\t|-- job.stdout\n\t\t\t|-- job.stderr\n\t\t\t|-- [job.script.submit]\n\t\t\t|-- [job.script.local]\n\t\t\t|-- [job.script.ssh]\n\t\t\t`-- [job.script.sge]\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nContent\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nworkdir/\n\n\nWhere the pipeline directories of all processes of current pipeline are located.\n\n\nCan be set by \np.ppldir\n\n\n\n\n\n\nPyPPL.\nid\n.\ntag\n.\nsuffix\n/\n\n\nThe work directory of current process.\n\n\nThe \nsuffix\n is a unique identify of the process according to its configuration.\nYou may set it by \np.workdir\n\n\n\n\n\n\nproc.settings/\n\n\nThe settings of the process\n\n\nA copy of proc settings\n\n\n\n\n\n\njob.index\n/\n\n\nThe job directory\n\n\nStarts with \n1\n\n\n\n\n\n\njob.index\n/input/\n\n\nWhere you can find the links to all the input files\n\n\n\n\n\n\n\n\njob.index\n/output/\n\n\nWhere you can find all the output files\n\n\n\n\n\n\n\n\njob.index\n/job.cache\n\n\nThe file containing the signature of the job\n\n\n\n\n\n\n\n\njob.index\n/job.script\n\n\nTo script file to be running\n\n\n\n\n\n\n\n\njob.index\n/job.pid\n\n\nThe id of the job of its running system.\n\n\nMostly used to tell whether the process is still running.\n\n\n\n\n\n\njob.index\n/job.rc\n\n\nTo file containing the return code\n\n\n\n\n\n\n\n\njob.index\n/job.stdout\n\n\nThe STDOUT of the script\n\n\n\n\n\n\n\n\njob.index\n/job.stderr\n\n\nThe STDERR of the script\n\n\n\n\n\n\n\n\njob.index\n/job.script.submit\n\n\nThe file to submit jobs (local, ssh)\n\n\n\n\n\n\n\n\njob.index\n/job.script.local\n\n\nThe wrapper for local jobs to save return code, stdout and stderr\n\n\n\n\n\n\n\n\njob.index\n/job.script.ssh\n\n\nThe script file for ssh runner\n\n\n\n\n\n\n\n\njob.index\n/job.script.sge\n\n\nThe script file for sge runner\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nYou are encouraged to set \np.ppldir\n \nBUT NOT\n \np.workdir\n, as it contains a unique \nsuffix\n that is automatically computed.  \n\n\n\n\nSymbols used in this documentation\n\n\n\n\nworkdir\n refers to \n./workdir/PyPPL.\nid\n.\ntag\n.\nsuffix\n/\n,  \n\n\nindir\n refers to \n./workdir/PyPPL.\nid\n.\ntag\n.\nsuffix\n/\njob.index\n/input/\n  \n\n\noutdir\n refers to \n./workdir/PyPPL.\nid\n.\ntag\n.\nsuffix\n/\njob.index\n/output/\n  \n\n\npXXX\n or \np\n refers to a process instantiated from \npyppl.Proc\n class.", 
            "title": "Basics and folder structure"
        }, 
        {
            "location": "/basic-concepts-and-directory-structure/#basic-concepts-and-folder-structure", 
            "text": "", 
            "title": "Basic concepts and folder structure"
        }, 
        {
            "location": "/basic-concepts-and-directory-structure/#layers-of-a-pipeline", 
            "text": "The pipeline consists of channels and processes. A process may have many jobs. Each job uses the corresponding elements from the input channel of the process, and generates values for output channel. \nActually, what you need to do is just specify the first input channel, and then tell  PyPPL  the dependencies of the processes. The later processes will use the output channel of the processes they depend on. Of course, you can interfere by using functions in the input specification.", 
            "title": "Layers of a pipeline"
        }, 
        {
            "location": "/basic-concepts-and-directory-structure/#folder-structure", 
            "text": "./\n|-- pipeline.py\n`-- workdir/\n\t`-- PyPPL. id . tag . suffix /\n\t\t|-- lock\n\t\t|-- proc.settings\n\t\t`--  job.index /\n\t\t\t|-- input/\n\t\t\t|-- output/\n\t\t\t|-- job.cache\n\t\t\t|-- job.script\n\t\t\t|-- job.pid\n\t\t\t|-- job.rc\n\t\t\t|-- job.stdout\n\t\t\t|-- job.stderr\n\t\t\t|-- [job.script.submit]\n\t\t\t|-- [job.script.local]\n\t\t\t|-- [job.script.ssh]\n\t\t\t`-- [job.script.sge]     Path  Content  Memo      workdir/  Where the pipeline directories of all processes of current pipeline are located.  Can be set by  p.ppldir    PyPPL. id . tag . suffix /  The work directory of current process.  The  suffix  is a unique identify of the process according to its configuration. You may set it by  p.workdir    proc.settings/  The settings of the process  A copy of proc settings    job.index /  The job directory  Starts with  1    job.index /input/  Where you can find the links to all the input files     job.index /output/  Where you can find all the output files     job.index /job.cache  The file containing the signature of the job     job.index /job.script  To script file to be running     job.index /job.pid  The id of the job of its running system.  Mostly used to tell whether the process is still running.    job.index /job.rc  To file containing the return code     job.index /job.stdout  The STDOUT of the script     job.index /job.stderr  The STDERR of the script     job.index /job.script.submit  The file to submit jobs (local, ssh)     job.index /job.script.local  The wrapper for local jobs to save return code, stdout and stderr     job.index /job.script.ssh  The script file for ssh runner     job.index /job.script.sge  The script file for sge runner       Note  You are encouraged to set  p.ppldir   BUT NOT   p.workdir , as it contains a unique  suffix  that is automatically computed.", 
            "title": "Folder structure"
        }, 
        {
            "location": "/basic-concepts-and-directory-structure/#symbols-used-in-this-documentation", 
            "text": "workdir  refers to  ./workdir/PyPPL. id . tag . suffix / ,    indir  refers to  ./workdir/PyPPL. id . tag . suffix / job.index /input/     outdir  refers to  ./workdir/PyPPL. id . tag . suffix / job.index /output/     pXXX  or  p  refers to a process instantiated from  pyppl.Proc  class.", 
            "title": "Symbols used in this documentation"
        }, 
        {
            "location": "/placeholders/", 
            "text": "Templating\n\n\nPyPPL\n has its own template engine, which is derived from a \n500-line-or-less template engine\n. It also supports \nJinja2\n if you have it installed and specify \n\"Jinja2\"\n to \npXXX.template\n. The built-in template engine is enabled by default.\n\n\nCommon data avaible for rendering\n\n\nWhen rendering a template, following data are fed to the render function. So that you can use those values in the template. Some attribute values of a process are shared for all templates that are applied:\n\n\n\n\nproc.aggr\n: The aggregation name of the process\n\n\nproc.args\n: A \ndict\n of process arguments. To access an item of it: \n{{args.\nitem\n}}\n\n\nproc.cache\n: The cache option\n\n\nproc.cclean\n: Whether clean (error check and export) the job when it's cached?\n\n\nproc.desc\n: The description of the process\n\n\nproc.echo\n: The echo option\n\n\nproc.errhow\n: What to do if error happens\n\n\nproc.errntry\n: If \nerrorhow == 'retry'\n, how many times to re-try if a job fails\n\n\nproc.exdir\n: The export directory\n\n\nproc.exhow\n: How to export output files\n\n\nproc.exow\n: Whether to overwrite existing files when exporting output files\n\n\nproc.forks\n: How many jobs to run concurrently\n\n\nproc.id\n: The id of the process.\n\n\nproc.infile\n: Where does \n{{in.infile}}\n refer to? (including other input files)\n\n\nproc.lang\n: The interpreter for the script\n\n\nproc.ppldir\n: Where the workdirs are located\n\n\nproc.procvars\n: The \ndict\n of all avaiable attributes of a process, can be accessed directly by \n{{proc.\nvar\n}}\n\n\nproc.rc\n: The rc option\n\n\nproc.resume\n: The resume option\n\n\nproc.runner\n: The runner\n\n\nproc.sets\n: A list of attribute names that has been set explictly\n\n\nproc.size\n: Number of jobs\n\n\nproc.suffix\n: The unique suffix of the process\n\n\nproc.tag\n: The tag of the process\n\n\nproc.workdir\n: The workdir of the process\n\n\n\n\nOther data for rendering\n\n\nFor each job, we also have some value available for rendering:\n\n\n\n\njob.index\n: The index of the job\n\n\njob.indir\n: The input directory of the job\n\n\njob.outdir\n: The output directory of the job\n\n\njob.dir\n: The directory of the job\n\n\njob.outfile\n: The stdout file of the job\n\n\njob.errfile\n: The stderr file of the job\n\n\njob.pidfile\n: The file stores the PID of the job or the identity from a queue runner.\n\n\n\n\nInput and output data are under namespace \nin\n and \nout\n, respectively.\nFor example, you have following definition:\n\npXXX\n.\ninput\n  \n=\n \n{\na\n:\n \n[\nhello\n],\n \nb\n:\n \n[\n/path/to/file\n]}\n\n\npXXX\n.\noutput\n \n=\n \na:{{in.a}} world!\n\n\n\nNow you can access them by: \n{{in.a}}\n, \n{{in.b}}\n and \n{{out.a}}\n\n\nThe scope of data\n\n\n\n\n\n\n\n\nAttribute\n\n\nData available\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\npXXX.beforeCmd\n\n\n{{proc.*}}\n\n\nCommand to run before job starts\n\n\n\n\n\n\npXXX.afterCmd\n\n\n{{proc.*}}\n\n\nCommand to run after job finishes\n\n\n\n\n\n\npXXX.brings\n\n\n{{proc.*}}\n, \n{{job.*}}\n, \n{{in.*}}\n\n\nThe bring-in files\n\n\n\n\n\n\npXXX.output\n\n\n{{proc.*}}\n, \n{{job.*}}\n, \n{{in.*}}\n, \n{{bring.*}}\n\n\nThe output of the process\n\n\n\n\n\n\npXXX.expect\n\n\nAll above-mentioned data\n\n\nCommand to check output\n\n\n\n\n\n\npXXX.expart\n\n\nAll above-mentioned data\n\n\nPartial export\n\n\n\n\n\n\npXXX.script\n\n\nAll above-mentioned data\n\n\nThe script to run\n\n\n\n\n\n\n\n\nBuilt-in functions\n\n\nSometimes we need to transform the data in a template. We have some built-in functions available for the transformation.\n\nFor built-in template engine, you may use pipe, for example: \n{{in.file | basename}}\n; for \nJinja2\n, you have to use functions as \"functions\", for example: \n{{basename(in.file)}}\n. Here we give the examples with built-in template engine syntax.\n\n\n\n\nR\n: Transform a python value to R value. For example:\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nData\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n{{v\nR}}\n\n\n{'v': True}\n\n\nTRUE\n\n\n\n\n\n\n\n\n{'v': 'TRUE'}\n\n\nTRUE\n\n\n\n\n\n\n\n\n{'v': 'NA'}\n\n\nNA\n\n\n\n\n\n\n\n\n{'v': 'NULL'}\n\n\nNULL\n\n\n\n\n\n\n\n\n{'v': 1}\n\n\n1\n\n\n\n\n\n\n\n\n{'v': 'r:c(1,2,3)'}\n\n\nc(1,2,3)\n\n\n\n\n\n\n\n\n{'v': 'plainstring'}\n\n\n\"plainstring\"\n\n\n\n\n\n\n\n\n\n\nRvec\n: Transform a python list to a R vector. For example:\n\n\n{{v | Rvec}}\n with \n{'v': [1,2,3]}\n results in \nc(1,2,3)\n\n\nRlist\n: Transform a python dict to a R list. For example:\n\n\n{{v | Rlist}}\n with \n{'v': {'a':1, 'b':2}}\n results in \nlist(a=1, b=2)\n\n\nrealpath\n: Alias of \nos.path.realpath\n\n\nreadlink\n: Alias of \nos.readlink\n\n\ndirname\n: Alias of \nos.path.dirname\n\n\nbasename\n: Get the basename of a file. If a file is renamed by \nPyPPL\n in case of input files with the same basename, it tries to get the original basename. For example:\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nData\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n{{v\nbasename}}\n\n\n{'v': '/path/to/file.txt'}\n\n\nfile.txt\n\n\n\n\n\n\n\n\n{'v': '/path/to/file[1].txt'}\n\n\nfile.txt\n\n\n\n\n\n\n{{v, orig\nbasename}}\n\n\n{'v': '/path/to/file[1].txt', 'orig': True}\n\n\nfile[1].txt\n\n\n\n\n\n\n\n\n\n\nbn\n: Alias of \nbasename\n\n\nfilename\n: Similar as \nbasename\n but without extension.\n\n\nfn\n: Alias of \nfilename\n\n\nfilename2\n: Get the filename without dot.\n\n\nfn2\n: Alias of \nfilename2\n. (i.e: \n/a/b/c.d.e.txt\n -\n \nc\n)\n\n\next\n: Get extension of a file. Alias of \nos.path.splitext(x)[1]\n\n\nDot is included. To remove the dot: \n{{v | ext | [1:]}}\n\n\nprefix\n: Get the prefix of a path, without extension. It acts like \n{{v | dirname}}/{{v | filename}}\n\n\nprefix2\n: Get the prefix of a path without dot in filename. (i.e: \n/a/b/c.d.e.txt\n -\n \n/a/b/c\n)\n\n\nquote\n: Double-quote a string.\n\n\nasquote\n: Double quote items in a list and join them by space. For example:\n\n\n{{v | asquote}}\n with \n{'v': [1,2,3]}\n results in \n\"1\" \"2\" \"3\"\n\n\nacquote\n: Double quote items in a list and join them by comma.\n\n\nsquote\n: Single-quote a string.\n\n\njson\n: Dumps a python object to a json string. Alias of \njson.dumps\n\n\nread\n: Read the content of a file.\n\n\nreadlines\n: Read the lines of a file. Empty lines are skipped by default. To return the empty lines for \n{'v': '/path/to/file', 'skipEmptyLines': False}\n: \n\n\n{{v, skipEmptyLines | readlines}}\n\n\nrepr\n: Alias of python \nrepr\n built-in function.\n\n\n\n\nUsage of built-in template engine\n\n\n\n\nBasic usage:\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nData\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n{{v}}\n\n\n{'v': 1}\n\n\n1\n\n\n\n\n\n\n{{v.a}}\n, \n{{v['a']}}\n\n\n{'v': {'a': 1}}\n\n\n1\n\n\n\n\n\n\n{{v.0}}\n, \n{{v[0]}}\n\n\n{'v': [1]}\n\n\n1\n\n\n\n\n\n\n{{v.upper()}}\n\n\n{'v': \"a\"}\n\n\nA\n\n\n\n\n\n\n{{v.0.upper()}}\n\n\n{'v': [\"a\"]}\n\n\nA\n\n\n\n\n\n\n\n\n\n\nApplying functions:\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nData\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n{{v\nR}}\n\n\n{'v': True}\n\n\nTRUE\n\n\n\n\n\n\n{{v1, v2\npaste}}\n\n\n{'v1': 'Hello', 'v2': 'world!', 'paste': lambda x, y: x + ' ' + y}\n\n\nHello world!\n\n\n\n\n\n\n{{v1, v2\nlambda x, y: x + ' ' + y}}\n\n\n{'v1': 'Hello', 'v2': 'world!'}\n\n\nHello world!\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIf you want to pass a literal value (for example: \n1\n, \nTrue\n), you CANNOT do this: \n{{v, False | readlines}}\n. Instead, you can either:\n\n\n\n\nspecify the value in data: \n{'v': '/path/to/file', 'skipEmptyLines': False}\n, \n\nthen \n{{v, skipEmptyLines | readlines}}\n; or\n\n\nuse \nlambda\n function: \n{{v, readlines | lambda x, func: func(x, False)}}\n\n\n\n\n\n\n\n\nIf-else/elif\n statements:\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nData\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n{% if v %}\n1\n{% else %}\n2\n{% endif %}\n\n\n{'v': True}\n\n\n1\n\n\n\n\n\n\n{% if v\nnotExists %}\nPath not exists.\n{% elif v\nisDir %}\nPath is a directory.\n{% else %}\nPath exists but is not a directory.\n{% endif %}\n\n\n{\n'v': '/path/to/file',\n'notExists': lambda x: not __import__('os').path.exists(x),\n'isDir': __import__('os').path.isdir\n}\n\n\nPath exists but is not a directory.\n\n\n\n\n\n\n\n\n\n\nLoops:\n\n\n\n\n\n\n\n\n\n\nUsage\n\n\nData\n\n\nResult\n\n\n\n\n\n\n\n\n\n\n{% for var in varlist %}{{var\nR}}{% endfor %}\n\n\n{'varlist': ['abc', 'True', 1, False]}\n\n\n\"abc\"TRUE1FALSE\n\n\n\n\n\n\n{% for k , v in data.items() %}{{k}}:{{v}}{% endfor %}\n\n\n{'data': {'a':1, 'b':2}}\n\n\na:1b:2\n\n\n\n\n\n\n\n\nSet environment of template engine\n\n\nYou can define you own functions/data for template rendering:\n\npXXX\n.\nenvs\n.\ndata\n  \n=\n \n{\nv1\n:\n \na\n,\n \nv2\n:\n \nb\n,\n \nb\n:\n \nTrue\n}\n\n\npXXX\n.\nenvs\n.\nos\n    \n=\n \n__import__\n(\nos\n)\n\n\npXXX\n.\nenvs\n.\npaste\n \n=\n \nlambda\n \nx\n,\ny\n:\n \nx\n \n+\n \n \n+\n \ny\n\n\n# use them\n\n\npXXX\n.\nscript\n \n=\n \n\n\n{\n% i\nf data.b %}\n\n\nprint \n{{data.v1, data.v2 | paste}}\n\n\n{\n% e\nlse %}\n\n\nprint \n{{data.v1, data.v2 | os.path.join}}\n\n\n{\n% e\nndif %}\n\n\n\n\n\nThen if \npXXX.envs.data['b'] is True\n, it prints \na b\n; otherwise it prints \na/b\n.\n\n\nUse Jinja2\n\n\nAll the data and environment definition mentioned above are all applicable when you use \nJinja2\n as your template engine.\n\nFor usage of \nJinja2\n, you may refer to its \nofficial documentation\n.", 
            "title": "Templating"
        }, 
        {
            "location": "/placeholders/#templating", 
            "text": "PyPPL  has its own template engine, which is derived from a  500-line-or-less template engine . It also supports  Jinja2  if you have it installed and specify  \"Jinja2\"  to  pXXX.template . The built-in template engine is enabled by default.", 
            "title": "Templating"
        }, 
        {
            "location": "/placeholders/#common-data-avaible-for-rendering", 
            "text": "When rendering a template, following data are fed to the render function. So that you can use those values in the template. Some attribute values of a process are shared for all templates that are applied:   proc.aggr : The aggregation name of the process  proc.args : A  dict  of process arguments. To access an item of it:  {{args. item }}  proc.cache : The cache option  proc.cclean : Whether clean (error check and export) the job when it's cached?  proc.desc : The description of the process  proc.echo : The echo option  proc.errhow : What to do if error happens  proc.errntry : If  errorhow == 'retry' , how many times to re-try if a job fails  proc.exdir : The export directory  proc.exhow : How to export output files  proc.exow : Whether to overwrite existing files when exporting output files  proc.forks : How many jobs to run concurrently  proc.id : The id of the process.  proc.infile : Where does  {{in.infile}}  refer to? (including other input files)  proc.lang : The interpreter for the script  proc.ppldir : Where the workdirs are located  proc.procvars : The  dict  of all avaiable attributes of a process, can be accessed directly by  {{proc. var }}  proc.rc : The rc option  proc.resume : The resume option  proc.runner : The runner  proc.sets : A list of attribute names that has been set explictly  proc.size : Number of jobs  proc.suffix : The unique suffix of the process  proc.tag : The tag of the process  proc.workdir : The workdir of the process", 
            "title": "Common data avaible for rendering"
        }, 
        {
            "location": "/placeholders/#other-data-for-rendering", 
            "text": "For each job, we also have some value available for rendering:   job.index : The index of the job  job.indir : The input directory of the job  job.outdir : The output directory of the job  job.dir : The directory of the job  job.outfile : The stdout file of the job  job.errfile : The stderr file of the job  job.pidfile : The file stores the PID of the job or the identity from a queue runner.   Input and output data are under namespace  in  and  out , respectively.\nFor example, you have following definition: pXXX . input    =   { a :   [ hello ],   b :   [ /path/to/file ]}  pXXX . output   =   a:{{in.a}} world!  \nNow you can access them by:  {{in.a}} ,  {{in.b}}  and  {{out.a}}", 
            "title": "Other data for rendering"
        }, 
        {
            "location": "/placeholders/#the-scope-of-data", 
            "text": "Attribute  Data available  Meaning      pXXX.beforeCmd  {{proc.*}}  Command to run before job starts    pXXX.afterCmd  {{proc.*}}  Command to run after job finishes    pXXX.brings  {{proc.*}} ,  {{job.*}} ,  {{in.*}}  The bring-in files    pXXX.output  {{proc.*}} ,  {{job.*}} ,  {{in.*}} ,  {{bring.*}}  The output of the process    pXXX.expect  All above-mentioned data  Command to check output    pXXX.expart  All above-mentioned data  Partial export    pXXX.script  All above-mentioned data  The script to run", 
            "title": "The scope of data"
        }, 
        {
            "location": "/placeholders/#built-in-functions", 
            "text": "Sometimes we need to transform the data in a template. We have some built-in functions available for the transformation. \nFor built-in template engine, you may use pipe, for example:  {{in.file | basename}} ; for  Jinja2 , you have to use functions as \"functions\", for example:  {{basename(in.file)}} . Here we give the examples with built-in template engine syntax.   R : Transform a python value to R value. For example:      Usage  Data  Result      {{v R}}  {'v': True}  TRUE     {'v': 'TRUE'}  TRUE     {'v': 'NA'}  NA     {'v': 'NULL'}  NULL     {'v': 1}  1     {'v': 'r:c(1,2,3)'}  c(1,2,3)     {'v': 'plainstring'}  \"plainstring\"      Rvec : Transform a python list to a R vector. For example:  {{v | Rvec}}  with  {'v': [1,2,3]}  results in  c(1,2,3)  Rlist : Transform a python dict to a R list. For example:  {{v | Rlist}}  with  {'v': {'a':1, 'b':2}}  results in  list(a=1, b=2)  realpath : Alias of  os.path.realpath  readlink : Alias of  os.readlink  dirname : Alias of  os.path.dirname  basename : Get the basename of a file. If a file is renamed by  PyPPL  in case of input files with the same basename, it tries to get the original basename. For example:      Usage  Data  Result      {{v basename}}  {'v': '/path/to/file.txt'}  file.txt     {'v': '/path/to/file[1].txt'}  file.txt    {{v, orig basename}}  {'v': '/path/to/file[1].txt', 'orig': True}  file[1].txt      bn : Alias of  basename  filename : Similar as  basename  but without extension.  fn : Alias of  filename  filename2 : Get the filename without dot.  fn2 : Alias of  filename2 . (i.e:  /a/b/c.d.e.txt  -   c )  ext : Get extension of a file. Alias of  os.path.splitext(x)[1]  Dot is included. To remove the dot:  {{v | ext | [1:]}}  prefix : Get the prefix of a path, without extension. It acts like  {{v | dirname}}/{{v | filename}}  prefix2 : Get the prefix of a path without dot in filename. (i.e:  /a/b/c.d.e.txt  -   /a/b/c )  quote : Double-quote a string.  asquote : Double quote items in a list and join them by space. For example:  {{v | asquote}}  with  {'v': [1,2,3]}  results in  \"1\" \"2\" \"3\"  acquote : Double quote items in a list and join them by comma.  squote : Single-quote a string.  json : Dumps a python object to a json string. Alias of  json.dumps  read : Read the content of a file.  readlines : Read the lines of a file. Empty lines are skipped by default. To return the empty lines for  {'v': '/path/to/file', 'skipEmptyLines': False} :   {{v, skipEmptyLines | readlines}}  repr : Alias of python  repr  built-in function.", 
            "title": "Built-in functions"
        }, 
        {
            "location": "/placeholders/#usage-of-built-in-template-engine", 
            "text": "Basic usage:      Usage  Data  Result      {{v}}  {'v': 1}  1    {{v.a}} ,  {{v['a']}}  {'v': {'a': 1}}  1    {{v.0}} ,  {{v[0]}}  {'v': [1]}  1    {{v.upper()}}  {'v': \"a\"}  A    {{v.0.upper()}}  {'v': [\"a\"]}  A      Applying functions:      Usage  Data  Result      {{v R}}  {'v': True}  TRUE    {{v1, v2 paste}}  {'v1': 'Hello', 'v2': 'world!', 'paste': lambda x, y: x + ' ' + y}  Hello world!    {{v1, v2 lambda x, y: x + ' ' + y}}  {'v1': 'Hello', 'v2': 'world!'}  Hello world!      Note  If you want to pass a literal value (for example:  1 ,  True ), you CANNOT do this:  {{v, False | readlines}} . Instead, you can either:   specify the value in data:  {'v': '/path/to/file', 'skipEmptyLines': False} ,  \nthen  {{v, skipEmptyLines | readlines}} ; or  use  lambda  function:  {{v, readlines | lambda x, func: func(x, False)}}     If-else/elif  statements:      Usage  Data  Result      {% if v %} 1 {% else %} 2 {% endif %}  {'v': True}  1    {% if v notExists %} Path not exists. {% elif v isDir %} Path is a directory. {% else %} Path exists but is not a directory. {% endif %}  { 'v': '/path/to/file', 'notExists': lambda x: not __import__('os').path.exists(x), 'isDir': __import__('os').path.isdir }  Path exists but is not a directory.      Loops:      Usage  Data  Result      {% for var in varlist %}{{var R}}{% endfor %}  {'varlist': ['abc', 'True', 1, False]}  \"abc\"TRUE1FALSE    {% for k , v in data.items() %}{{k}}:{{v}}{% endfor %}  {'data': {'a':1, 'b':2}}  a:1b:2", 
            "title": "Usage of built-in template engine"
        }, 
        {
            "location": "/placeholders/#set-environment-of-template-engine", 
            "text": "You can define you own functions/data for template rendering: pXXX . envs . data    =   { v1 :   a ,   v2 :   b ,   b :   True }  pXXX . envs . os      =   __import__ ( os )  pXXX . envs . paste   =   lambda   x , y :   x   +     +   y  # use them  pXXX . script   =    { % i f data.b %}  print  {{data.v1, data.v2 | paste}}  { % e lse %}  print  {{data.v1, data.v2 | os.path.join}}  { % e ndif %}   \nThen if  pXXX.envs.data['b'] is True , it prints  a b ; otherwise it prints  a/b .", 
            "title": "Set environment of template engine"
        }, 
        {
            "location": "/placeholders/#use-jinja2", 
            "text": "All the data and environment definition mentioned above are all applicable when you use  Jinja2  as your template engine. \nFor usage of  Jinja2 , you may refer to its  official documentation .", 
            "title": "Use Jinja2"
        }, 
        {
            "location": "/channels/", 
            "text": "Channels\n\n\nChannels are used to pass data from one process (an instance of \nProc\n) to another. It is derived from a \nlist\n, where each element is a \ntuple\n. \nSo all python functions/methods that apply on \nlist\n will also apply on \nChannel\n.\n The length a the \ntuple\n corresponds to the number of variables of the input or output of a \nproc\n.\n\n# v1  v2  v3\n\n\nc\n \n=\n \n[\n\n \n(\na1\n,\n \nb1\n,\n \nc1\n),\n  \n# data for job #0\n\n \n(\na2\n,\n \nb2\n,\n \nc2\n),\n  \n# data for job #1\n\n\n# ...\n\n\n]\n\n\n\nIf we specify this channel to the input of a \nproc\n:\n\np\n \n=\n \nproc\n()\n\n\np\n.\ninput\n \n=\n \n{\nv1,v2,v3\n:\n \nc\n}\n\n\n\nThen the values for different variables in different jobs wil be:\n\n\n\n\n\n\n\n\nJob Index\n\n\nv1\n\n\nv2\n\n\nv3\n\n\n\n\n\n\n\n\n\n\n0\n\n\na1\n\n\nb1\n\n\nc1\n\n\n\n\n\n\n1\n\n\na2\n\n\nb2\n\n\nc2\n\n\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\n\n\n\nInitialize a channel\n\n\nThere are several ways to initialize a channel:\n\n\n\n\nNote\n\n\nPlease use \nChannel.create(...)\n instead of \nChannel(...)\n unless each element is 'tuplized' properly. \n\nChannel\n.\ncreate\n([\n1\n,\n2\n,\n3\n])\n \n!=\n \nChannel\n([\n1\n,\n2\n,\n3\n])\n\n\nChannel\n.\ncreate\n([\n1\n,\n2\n,\n3\n])\n \n==\n \nChannel\n([(\n1\n,),\n \n(\n2\n,),\n \n(\n3\n,)])\n\n\n\n\n\n\n\n\n\n\nFrom other channels: \n\n  \nch1\n \n=\n \nChannel\n.\ncreate\n([(\n1\n,\n \n2\n),\n \n(\n3\n,\n \n4\n)])\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n(\na\n)\n\n\nch3\n \n=\n \nChannel\n.\ncreate\n([\n5\n,\n \n6\n])\n\n\nch\n  \n=\n \nChannel\n.\nfromChannels\n(\nch1\n,\n \nch2\n,\n \nch3\n)\n\n\n# channels are column-bound\n\n\n# ch == [(1, 2, \na\n, 5), (3, 4, \na\n, 6)]\n\n\n\n\n\n\n\n\nFrom a file path pattern:\n\n  Use \nglob.glob\n to grab files by the pattern, you may use different arguments for filter, sort or reverse the list:\n\n\n\n\n\n\nfilter the files with type (\nt\n): \ndir\n, \nfile\n, \nlink\n or \nany\n (default), \n\n\n\n\nsort them by (\nsortby\n): \nsize\n, \nmtime\n or \nname\n (default)\n\n\n\n\nreverse the list (\nreverse\n): \nFalse\n (default, don't reverse)\n\n\nc\n \n=\n \nChannel\n.\nfromPattern\n \n(\n/a/b/*.txt\n,\n \nt\n \n=\n \nany\n,\n \nsortby\n \n=\n \nsize\n,\n \nreverse\n \n=\n \nFalse\n)\n\n\n\n\n\n\n\n\n\nFrom file pairs:\n  \nc\n \n=\n \nChannel\n.\nfromPairs\n \n(\n/a/b/*.txt\n)\n\n\n# the files will be sorted by names and then split into pairs\n\n\n# c == [(\n/a/b/a1.txt\n, \n/a/b/a2.txt\n), (\n/a/b/b1.txt\n, \n/a/b/b2.txt\n)]\n\n\n\n\n\n\n\n\nFrom file content:\n\n\nChannel.fromFile(fn, header=False, skip=0, delimit=\"\\t\")\n\n  For example, we have a file \n\"chan.txt\"\n with content:\n  \nA\ntab\nB\ntab\nC\na1\ntab\nb1\ntab\nc1\na2\ntab\nb2\ntab\nc2\n\n\n  Read the file as a channel:\n  \nc\n \n=\n \nChannel\n.\nfromFile\n \n(\nchan.txt\n)\n\n\n# c == [(\nA\n, \nB\n, \nC\n), (\na1\n, \nb1\n, \nc1\n), (\na2\n, \nb2\n, \nc2\n)]\n\n\nc\n \n=\n \nChannel\n.\nfromFile\n \n(\nchan.txt\n,\n \nheader\n=\nTrue\n)\n\n\n# c == [(\na1\n, \nb1\n, \nc1\n), (\na2\n, \nb2\n, \nc2\n)]\n\n\n# c.A == [(\na1\n, ), (\na2\n, )]\n\n\n# c.B == [(\nb1\n, ), (\nb2\n, )]\n\n\n# c.C == [(\nc1\n, ), (\nc2\n, )]\n\n\nc\n \n=\n \nChannel\n.\nfromFile\n \n(\nchan.txt\n,\n \nskip\n \n=\n \n1\n)\n\n\n# c == [(\na1\n, \nb1\n, \nc1\n), (\na2\n, \nb2\n, \nc2\n)]\n\n\n\n\n\n\n\n\nFrom \nsys.argv\n (command line arguments):\n  \nc\n \n==\n \nchannel\n.\nfromArgv\n()\n\n\n# python whatever.py /a/b/*.txt\n\n\n# c == [(\n/a/b/1.txt\n,), (\n/a/b/2.txt\n,), (\n/a/b/3.txt\n,), (\n/a/b/4.txt\n,)]\n\n\n# Make a multple-variable channel:\n\n\n# python whatever.py /a/b/1.txt,/a/b/2.txt /a/b/3.txt,/a/b/4.txt\n\n\n# c == [(\n/a/b/1.txt\n, \n/a/b/2.txt\n), (\n/a/b/3.txt\n, \n/a/b/4.txt\n)]\n\n\n\n\n\n\n\n\nFrom command line argument parser:\n  See \ncommand line argument parser\n for details.\n  \nfrom\n \nPyPPL\n \nimport\n \nChannel\n,\n \nparams\n\n\nparams\n.\na\n \n=\n \na\n\n\nparams\n.\nb\n \n=\n \n2\n\n\nparams\n.\nb\n.\ntype\n \n=\n \nint\n\n\nparams\n.\nc\n \n=\n \n[\n1\n,\n \n2\n]\n\n\nparams\n.\nc\n.\ntype\n \n=\n \nlist\n\n\nparams\n.\nd\n \n=\n \n[\na\n,\n \nb\n]\n\n\nparams\n.\nd\n.\ntype\n \n=\n \nlist\n\n\nparams\n.\ne\n \n=\n \n[]\n\n\nparams\n.\ne\n.\ntype\n \n=\n \nlist\n\n\n\nch\n \n=\n \nChannel\n.\nfromParams\n(\nc\n,\n \ne\n)\n\n\n# Raises ValueError, non-equal length\n\n\nch\n \n=\n \nChannel\n.\nfromParams\n(\nc\n,\n \nd\n)\n\n\n# ch == [(1, \na\n), (2, \nb\n)]\n\n\nch\n \n=\n \nChannel\n.\nfromParams\n(\na\n,\n \nb\n)\n\n\n# ch == [(\na\n, 2)]\n\n\n\n\n\n\n\n\nMethods for channels\n\n\nGet the length and width of a channel\n\n\nchan\n \n=\n \nChannel\n.\ncreate\n \n([(\n1\n,\n2\n,\n3\n),\n \n(\n4\n,\n5\n,\n6\n)])\n\n\n#chan.length() == 2 == len(chan)\n\n\n#chan.width()  == 3\n\n\n\n\n\nGet value from a channel\n\n\nchan\n \n=\n \nChannel\n.\ncreate\n \n([(\n1\n,\n2\n,\n3\n),\n \n(\n4\n,\n5\n,\n6\n)])\n\n\n# chan.get() == 1\n\n\n# chan.get(0) == 1\n\n\n# chan.get(1) == 2\n\n\n# chan.get(2) == 3\n\n\n# chan.get(3) == 4\n\n\n# chan.get(4) == 5\n\n\n# chan.get(5) == 6\n\n\n\n\n\nRepeat rows and columns\n\n\nchan\n \n=\n \nChannel\n.\ncreate\n \n([(\n1\n,\n2\n,\n3\n),\n \n(\n4\n,\n5\n,\n6\n)])\n\n\nchan2\n \n=\n \nchan\n.\nrepCol\n()\n\n\nchan3\n \n=\n \nchan\n.\nrepCol\n(\nn\n=\n3\n)\n\n\n# chan2 == [(1,2,3,1,2,3), (4,5,6,4,5,6)]\n\n\n# chan3 == [(1,2,3,1,2,3,1,2,3), (4,5,6,4,5,6,4,5,6)]\n\n\nchan4\n \n=\n \nchan\n.\nrepRow\n()\n\n\nchan5\n \n=\n \nchan\n.\nrepRow\n(\nn\n=\n3\n)\n\n\n# chan4 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6)]\n\n\n# chan5 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6), (1,2,3), (4,5,6)]\n\n\n\n\n\nExpand a channel by directory\n\n\nChannel.expand (col= 0, pattern = '*', t='any', sortby='name', reverse=False)\n\n\nSometimes we prepare files in one process (for example, split a big file into small ones in a directory), then handle these files by different jobs in another process, so that they can be processed simultaneously. \n\n\n \n\n\n\n\nCaution\n\n\n\n\nexpand\n only works for original channels with length is 1, which will expand to \nN\n (number of files included). If original channel has more than 1 element, only first element will be used, and other elements will be ignored.\n\n\nOnly the value of the column to be expanded will be changed, values of other columns remain the same. \n\n\n\n\n\n\nCollapse a channel by files in a common ancestor directory\n\n\nChannel.collapse(col=0)\n\n\nIt's basically the reverse process of \nexpand\n. It applies when you deal with different files and in next process you need them all involved (i.e. combine the results):\n\n\n \n\n\n\n\nCaution\n\n\n\n\nos.path.dirname(os.path.commonprefix(...))\n is used to detect the common ancestor directory, so the files could be \n['/a/1/1.file', '/a/2/1.file']\n. In this case \n/a/\n will be returned.\n\n\nvalues at other columns should be the same, \nPyPPL\n will NOT check it, the first value at the column will be used.\n\n\n\n\n\n\nFetch rows from a channel\n\n\n\n\nChannel.rowAt(index)\n  \n\n\n\n\nchan1\n \n=\n \nChannel\n.\ncreate\n \n([(\n1\n,\n2\n,\n3\n,\n4\n),\n \n(\n4\n,\n5\n,\n6\n,\n7\n)])\n\n\nchan2\n \n=\n \nchan1\n.\nrowAt\n(\n1\n)\n\n\n# chan2 == [(4,5,6,7)]\n\n\n\n# Now you can also fetch multiple columus as a channel:\n\n\nchan3\n \n=\n \nchan1\n.\nrowAt\n([:\n2\n])\n\n\nchan3\n \n==\n \nchan1\n\n\n\n\n\nFetch columns from a channel\n\n\n\n\nChannel.slice(start, length=None)\n\n\n\n\nchan1\n \n=\n \nChannel\n.\ncreate\n \n([(\n1\n,\n2\n,\n3\n,\n4\n),\n \n(\n4\n,\n5\n,\n6\n,\n7\n)])\n\n\nchan2\n \n=\n \nchan1\n.\nslice\n(\n1\n,\n2\n)\n\n\n# chan2 == [(2,3), (5,6)]\n\n\nchan3\n \n=\n \nchan1\n.\nslice\n(\n2\n)\n\n\n# chan3 == [(3,4), (6,7)]\n\n\nchan4\n \n=\n \nchan1\n.\nslice\n(\n-\n1\n)\n\n\n# chan4 == [(4,), (7,)]\n\n\n\n\n\n\n\nChannel.colAt(index)\n\n\n\n\nchan\n.\ncolAt\n(\nindex\n)\n \n==\n \nchan\n.\nslice\n(\nindex\n,\n \n1\n)\n\n\n\n# Now you may also fetch multiple columns:\n\n\nchan\n.\ncolAt\n([\n1\n,\n2\n])\n \n==\n \nchan\n.\nslice\n(\n1\n,\n \n2\n)\n\n\n\n\n\nFlatten a channel\n\n\nChannel.flatten(col = None)\n\nFlatten a channel, make it into a list.\n\nchan\n  \n=\n \nChannel\n.\ncreate\n \n([(\n1\n,\n2\n,\n3\n),\n \n(\n4\n,\n5\n,\n6\n)])\n\n\nf1\n \n=\n \nchan\n.\nflatten\n()\n\n\n# f1 == [1,2,3,4,5,6]\n\n\nf2\n \n=\n \nchan\n.\nflatten\n(\n1\n)\n\n\n# f1 == [2,5]\n\n\n\n\nSplit a channel to single-width channels\n\n\nChannel.split(flatten = False)\n\n\nchan\n  \n=\n \nChannel\n.\ncreate\n \n([(\n1\n,\n2\n,\n3\n),\n \n(\n4\n,\n5\n,\n6\n)])\n\n\nchans\n \n=\n \nchan\n.\nsplit\n()\n\n\n# isinstance (chans, list) == True\n\n\n# isinstance (chans, Channel) == False\n\n\n# chans == [\n\n\n#   [(1,), (4,)],  # isinstance (chans[0], Channel) == True\n\n\n#   [(2,), (5,)],\n\n\n#   [(3,), (6,)],\n\n\n# ]\n\n\nchans2\n \n=\n \nchan\n.\nsplit\n(\nTrue\n)\n\n\n# chans2 == [\n\n\n#   [1, 4],        # isinstance (chans2[0], Channel) == False\n\n\n#   [2, 5],\n\n\n#   [3, 6],\n\n\n# ]\n\n\n\n\nAttach column names\n\n\nChannel.attach(*names)\n\nWe can attach the column names and then use them to access the columns.\n\nch\n \n=\n \nChannel\n.\ncreate\n([(\n1\n,\n2\n,\n3\n),\n \n(\n4\n,\n5\n,\n6\n)])\n\n\nch\n.\nattach\n \n(\ncol1\n,\n \ncol2\n,\n \ncol3\n)\n\n\n# ch.col1 == [(1,), (4,)]\n\n\n# ch.col2 == [(2,), (5,)]\n\n\n# ch.col3 == [(3,), (6,)]\n\n\n# isinstance(ch.col1, Channel) == True\n\n\n\n# flatten the columns\n\n\nch\n.\nattach\n \n(\ncol1\n,\n \ncol2\n,\n \ncol3\n,\n \nTrue\n)\n\n\n# ch.col1 == [1,4]\n\n\n# ch.col2 == [2.5]\n\n\n# ch.col3 == [3,6]\n\n\n# isinstance(ch.col1, Channel) == False\n\n\n\n\nMap, filter, reduce\n\n\n\n\nChannel.map(func)\n\n\nChannel.mapCol(func, col=0)\n\n  \nch1\n \n=\n \nChannel\n.\ncreate\n()\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n([\n1\n,\n2\n,\n3\n,\n4\n,\n5\n])\n\n\nch3\n \n=\n \nChannel\n.\ncreate\n([(\na\n,\n \n1\n),\n \n(\nb\n,\n \n2\n)])\n\n\n# ch1.map(lambda x: (x[0]*x[0],)) == []\n\n\n# ch2.map(lambda x: (x[0]*x[0],)) == [(1,),(4,),(9,),(16,),(25,)]\n\n\n# ch3.map(lambda x: (x[0], x[1]*x[1])) == [(\na\n, 1), (\nb\n, 4)]\n\n\n# ch1.mapCol(lambda x: x*x) == []\n\n\n# ch2.mapCol(lambda x: x*x) == [(1,),(4,),(9,),(16,),(25,)]\n\n\n# ch3.mapCol(lambda x: x*x, 1) == [(\na\n, 1), (\nb\n, 4)]\n\n\n# map \n mapCol return an instance of Channel\n\n\n\n\nChannel.filter(func)\n\n\nChannel.filterCol(func, col=0)\n\n  \nch1\n \n=\n \nChannel\n.\ncreate\n([\n\n  \n(\n1\n,\n    \n0\n,\n     \n0\n,\n   \n1\n  \n),\n\n  \n(\na\n,\n  \n,\n    \nb\n,\n \n0\n),\n\n  \n(\nTrue\n,\n \nFalse\n,\n \n0\n,\n   \n1\n  \n),\n\n  \n([],\n   \n[\n1\n],\n   \n[\n2\n],\n \n[\n0\n]),\n\n\n])\n\n\n# Filter by the first column, only first three rows remained\n\n\nch1\n.\nfilterCol\n()\n \n==\n \nch1\n[:\n3\n]\n \n\n# Filter by the second column, only the last row remained\n\n\nch1\n.\nfilterCol\n(\ncol\n \n=\n \n1\n)\n \n==\n \nch1\n[\n3\n:\n4\n]\n\n\n# Filter by the third column, the 2nd and 4th row remained\n\n\nch1\n.\nfilterCol\n(\ncol\n \n=\n \n2\n)\n \n==\n \n[\nch1\n[\n1\n],\n \nch1\n[\n3\n]]\n\n\n# Filter by the fourth column, all rows remained\n\n\nch1\n.\nfilterCol\n(\ncol\n \n=\n \n3\n)\n \n==\n \nch1\n\n\n# Filter with a function:\t\t\n\n\nch1\n.\nfilter\n(\nlambda\n \nx\n:\n \nisinstance\n(\nx\n[\n2\n],\n \nint\n))\n \n==\n \n[\nch1\n[\n0\n],\n \nch1\n[\n2\n]]\n\n\n# filter \n filterCol return an instance of Channel\n\n\n\n\nChannel.reduce(func)\n\n\nChannel.reduceCol(func, col=0)\n\n  \nch1\n \n=\n \nChannel\n.\ncreate\n()\n\n\n# Raises TypeError, no elements\n\n\nch1\n.\nreduce\n(\nlambda\n \nx\n,\ny\n:\n \nx\n+\ny\n)\n\n\nch1\n \n=\n \nChannel\n.\ncreate\n([\n1\n,\n2\n,\n3\n,\n4\n,\n5\n])\n\n\n# Notice the different\n\n\nch1\n.\nreduce\n(\nlambda\n \nx\n,\ny\n:\n \nx\n+\ny\n)\n \n==\n \n(\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n)\n \n# x and y are tuples\n\n\nch1\n.\nreduceCol\n(\nlambda\n \nx\n,\ny\n:\n \nx\n+\ny\n)\n \n==\n \n15\n           \n# x and y are numbers\n\n\n\n\n\n\nAdd rows/columns to a channel\n\n\n\n\nChannel.rbind(*rows)\n  \n\n\n\n\nEach row can be either a channel, a tuple, a list or a non-iterable element(including string) \n\n  \nch1\n \n=\n \nChannel\n.\ncreate\n()\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n((\n1\n,\n2\n,\n3\n))\n\n\n\nrow1\n \n=\n \nChannel\n.\ncreate\n(\n1\n)\n\n\nrow2\n \n=\n \nChannel\n.\ncreate\n((\n2\n,\n2\n,\n2\n))\n\n\nrow3\n \n=\n \n[\n3\n]\n\n\nrow4\n \n=\n \n(\n3\n,)\n\n\nrow5\n \n=\n \n(\n4\n,\n4\n,\n4\n)\n\n\nrow6\n \n=\n \n[\n4\n,\n4\n,\n4\n]\n\n\nrow7\n \n=\n \n5\n\n\n\nch1\n.\nrbind\n(\nrow1\n)\n \n==\n \n[(\n1\n,\n \n)]\n\n\nch2\n.\nrbind\n(\nrow1\n)\n \n==\n \n[(\n1\n,\n2\n,\n3\n),(\n1\n,\n1\n,\n1\n)],\n\n\nch1\n.\nrbind\n(\nrow2\n)\n \n==\n \n[(\n2\n,\n2\n,\n2\n)]\n\n\nch2\n.\nrbind\n(\nrow2\n)\n \n==\n \n[(\n1\n,\n2\n,\n3\n),\n \n(\n2\n,\n2\n,\n2\n)]\n\n\nch1\n.\nrbind\n(\nrow3\n)\n \n==\n \n[(\n3\n,)]\n\n\nch2\n.\nrbind\n(\nrow3\n)\n \n==\n \n[(\n1\n,\n2\n,\n3\n),(\n3\n,\n3\n,\n3\n)]\n\n\nch1\n.\nrbind\n(\nrow4\n)\n \n==\n \n[(\n3\n,)]\n\n\nch2\n.\nrbind\n(\nrow4\n)\n \n==\n \n[(\n1\n,\n2\n,\n3\n),(\n3\n,\n3\n,\n3\n)]\n\n\nch1\n.\nrbind\n(\nrow5\n)\n \n==\n \n[(\n4\n,\n4\n,\n4\n)]\n\n\nch2\n.\nrbind\n(\nrow5\n)\n \n==\n \n[(\n1\n,\n2\n,\n3\n),(\n4\n,\n4\n,\n4\n)]\n\n\nch1\n.\nrbind\n(\nrow6\n)\n \n==\n \n[(\n4\n,\n4\n,\n4\n)]\n\n\nch2\n.\nrbind\n(\nrow6\n)\n \n==\n \n[(\n1\n,\n2\n,\n3\n),(\n4\n,\n4\n,\n4\n)]\n\n\nch1\n.\nrbind\n(\nrow7\n)\n \n==\n \n[(\n5\n,)]\n\n\nch2\n.\nrbind\n(\nrow7\n)\n \n==\n \n[(\n1\n,\n2\n,\n3\n),(\n5\n,\n5\n,\n5\n)]\n\n\n\n\n\n\n\n\nChannel.cbind(*cols)\n\n  \nch1\n \n=\n \nChannel\n.\ncreate\n([(\n1\n,\n \n2\n),\n \n(\n3\n,\n \n4\n)])\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n([\n5\n,\n \n6\n])\n\n\n\nch1\n.\ncbind\n(\nch2\n)\n \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n6\n)]\n\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n(\n5\n)\n\n\nch1\n.\ncbind\n(\nch2\n)\n    \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n5\n)]\n\n\nch1\n.\ncbind\n([\n5\n,\n \n6\n])\n \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n6\n)]\n\n\nch1\n.\ncbind\n((\n5\n,\n \n6\n))\n \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n6\n)]\n\n\nch1\n.\ncbind\n(\na\n)\n    \n==\n \n[(\n1\n,\n \n2\n,\n \na\n),\n \n(\n3\n,\n \n4\n,\n \na\n)]\n\n\n\nch1\n \n=\n \nChannel\n.\ncreate\n()\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n([\n21\n,\n \n22\n])\n\n\nch3\n \n=\n \n3\n\n\nch4\n \n=\n \n[\n41\n,\n \n42\n]\n\n\nch5\n \n=\n \n(\n51\n,\n \n52\n)\n\n\nch6\n \n=\n \na\n\n\n\nch1\n.\ncbind\n(\nch2\n,\n \nch3\n,\n \nch4\n,\n \nch5\n,\n \nch6\n)\n \n==\n \n[(\n21\n,\n \n3\n,\n \n41\n,\n \n51\n,\n \na\n),\n \n(\n22\n,\n \n3\n,\n \n42\n,\n \n52\n,\n \na\n)]\n\n\nch1\n.\ncbind\n(\nch3\n)\n.\ncbind\n(\nch6\n)\n \n==\n \n[(\n3\n,\n \na\n)]\n\n\n\n\n\n\n\n\nChannel.insert(index, col)\n\n  \nch1\n \n=\n \nChannel\n.\ncreate\n([(\n1\n,\n \n2\n),\n \n(\n3\n,\n \n4\n)])\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n([\n5\n,\n \n6\n])\n\n\nch1\n.\ninsert\n(\n0\n,\n \nch2\n)\n    \n==\n \n[(\n5\n,\n \n1\n,\n \n2\n),\n \n(\n6\n,\n \n3\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n1\n,\n \nch2\n)\n    \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n6\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n-\n1\n,\n \nch2\n)\n   \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n6\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\nNone\n,\n \nch2\n)\n \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n6\n)]\n\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n(\n5\n)\n\n\nch1\n.\ninsert\n(\n0\n,\n \nch2\n)\n    \n==\n \n[(\n5\n,\n \n1\n,\n \n2\n),\n \n(\n5\n,\n \n3\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n1\n,\n \nch2\n)\n    \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n5\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n-\n1\n,\n \nch2\n)\n   \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n5\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\nNone\n,\n \nch2\n)\n \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n5\n)]\n\n\n\nch1\n.\ninsert\n(\n0\n,\n \n[\n5\n,\n \n6\n])\n    \n==\n \n[(\n5\n,\n \n1\n,\n \n2\n),\n \n(\n6\n,\n \n3\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n1\n,\n \n[\n5\n,\n \n6\n])\n    \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n6\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n-\n1\n,\n \n[\n5\n,\n \n6\n])\n   \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n6\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\nNone\n,\n \n[\n5\n,\n \n6\n])\n \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n6\n)]\n\n\nch1\n.\ninsert\n(\n0\n,\n \n(\n5\n,\n \n6\n))\n    \n==\n \n[(\n5\n,\n \n1\n,\n \n2\n),\n \n(\n6\n,\n \n3\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n1\n,\n \n(\n5\n,\n \n6\n))\n    \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n6\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n-\n1\n,\n \n(\n5\n,\n \n6\n))\n   \n==\n \n[(\n1\n,\n \n5\n,\n \n2\n),\n \n(\n3\n,\n \n6\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\nNone\n,\n \n(\n5\n,\n \n6\n))\n \n==\n \n[(\n1\n,\n \n2\n,\n \n5\n),\n \n(\n3\n,\n \n4\n,\n \n6\n)]\n\n\nch1\n.\ninsert\n(\n0\n,\n \na\n)\n       \n==\n \n[(\na\n,\n \n1\n,\n \n2\n),\n \n(\na\n,\n \n3\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n1\n,\n \na\n)\n       \n==\n \n[(\n1\n,\n \na\n,\n \n2\n),\n \n(\n3\n,\n \na\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\n-\n1\n,\n \na\n)\n      \n==\n \n[(\n1\n,\n \na\n,\n \n2\n),\n \n(\n3\n,\n \na\n,\n \n4\n)]\n\n\nch1\n.\ninsert\n(\nNone\n,\n \na\n)\n    \n==\n \n[(\n1\n,\n \n2\n,\n \na\n),\n \n(\n3\n,\n \n4\n,\n \na\n)]\n\n\n\nself\n.\nassertEqual\n(\nch1\n,\n \n[(\n1\n,\n \n2\n),\n \n(\n3\n,\n \n4\n)])\n\n\n\nch1\n \n=\n \nChannel\n.\ncreate\n()\n\n\nch2\n \n=\n \nChannel\n.\ncreate\n([\n21\n,\n \n22\n])\n\n\nch3\n \n=\n \n3\n\n\nch4\n \n=\n \n[\n41\n,\n \n42\n]\n\n\nch5\n \n=\n \n(\n51\n,\n \n52\n)\n\n\nch6\n \n=\n \na\n\n\n# Raises ValueError, when 1 is inserted, it is a 1-width channel, \n\n\n# then you can\nt insert a 2-width to it.\n\n\nch1\n.\ninsert\n(\n1\n,\n \nch2\n)\n\n\nch1\n.\ninsert\n(\n0\n,\n \nch2\n,\n \nch3\n,\n \nch4\n,\n \nch5\n,\n \nch6\n)\n \n==\n \n[(\n21\n,\n \n3\n,\n \n41\n,\n \n51\n,\n \na\n),\n \n(\n22\n,\n \n3\n,\n \n42\n,\n \n52\n,\n \na\n)]\n\n\n\n\n\n\n\n\nFold a channel\n\n\nChannel.fold(n = 1)\n\nFold a \nchannel\n, Make a row to n-length chunk rows\nFor example, you have the following channel:\n\n\n\n\n\n\n\n\na1\n\n\na2\n\n\na3\n\n\na4\n\n\n\n\n\n\n\n\n\n\nb1\n\n\nb2\n\n\nb3\n\n\nb4\n\n\n\n\n\n\n\n\nAfter apply \nchan.fold(2)\n you will get:\n\n\n\n\n\n\n\n\na1\n\n\na2\n\n\n\n\n\n\n\n\n\n\na3\n\n\na4\n\n\n\n\n\n\nb1\n\n\nb2\n\n\n\n\n\n\nb3\n\n\nb4\n\n\n\n\n\n\n\n\nUnfold a channel\n\n\nChannel.unfold(n=2)\n\nCombine n-rows into one row; do the reverse thing as \nChannel.fold\n. But note that the different meaning of \nn\n. In \nfold\n, \nn\n means the length of the chunk that a row is cut to; will in \nunfold\n, it means how many rows to combine.\n\n\nCopy a channel\n\n\nChannel.copy()", 
            "title": "Channels"
        }, 
        {
            "location": "/channels/#channels", 
            "text": "Channels are used to pass data from one process (an instance of  Proc ) to another. It is derived from a  list , where each element is a  tuple .  So all python functions/methods that apply on  list  will also apply on  Channel .  The length a the  tuple  corresponds to the number of variables of the input or output of a  proc . # v1  v2  v3  c   =   [ \n  ( a1 ,   b1 ,   c1 ),    # data for job #0 \n  ( a2 ,   b2 ,   c2 ),    # data for job #1  # ...  ]  \nIf we specify this channel to the input of a  proc : p   =   proc ()  p . input   =   { v1,v2,v3 :   c }  \nThen the values for different variables in different jobs wil be:     Job Index  v1  v2  v3      0  a1  b1  c1    1  a2  b2  c2    ...  ...  ...  ...", 
            "title": "Channels"
        }, 
        {
            "location": "/channels/#initialize-a-channel", 
            "text": "There are several ways to initialize a channel:   Note  Please use  Channel.create(...)  instead of  Channel(...)  unless each element is 'tuplized' properly.  Channel . create ([ 1 , 2 , 3 ])   !=   Channel ([ 1 , 2 , 3 ])  Channel . create ([ 1 , 2 , 3 ])   ==   Channel ([( 1 ,),   ( 2 ,),   ( 3 ,)])      From other channels:  \n   ch1   =   Channel . create ([( 1 ,   2 ),   ( 3 ,   4 )])  ch2   =   Channel . create ( a )  ch3   =   Channel . create ([ 5 ,   6 ])  ch    =   Channel . fromChannels ( ch1 ,   ch2 ,   ch3 )  # channels are column-bound  # ch == [(1, 2,  a , 5), (3, 4,  a , 6)]     From a file path pattern: \n  Use  glob.glob  to grab files by the pattern, you may use different arguments for filter, sort or reverse the list:    filter the files with type ( t ):  dir ,  file ,  link  or  any  (default),    sort them by ( sortby ):  size ,  mtime  or  name  (default)   reverse the list ( reverse ):  False  (default, don't reverse)  c   =   Channel . fromPattern   ( /a/b/*.txt ,   t   =   any ,   sortby   =   size ,   reverse   =   False )     From file pairs:\n   c   =   Channel . fromPairs   ( /a/b/*.txt )  # the files will be sorted by names and then split into pairs  # c == [( /a/b/a1.txt ,  /a/b/a2.txt ), ( /a/b/b1.txt ,  /a/b/b2.txt )]     From file content:  Channel.fromFile(fn, header=False, skip=0, delimit=\"\\t\") \n  For example, we have a file  \"chan.txt\"  with content:\n   A tab B tab C\na1 tab b1 tab c1\na2 tab b2 tab c2 \n  Read the file as a channel:\n   c   =   Channel . fromFile   ( chan.txt )  # c == [( A ,  B ,  C ), ( a1 ,  b1 ,  c1 ), ( a2 ,  b2 ,  c2 )]  c   =   Channel . fromFile   ( chan.txt ,   header = True )  # c == [( a1 ,  b1 ,  c1 ), ( a2 ,  b2 ,  c2 )]  # c.A == [( a1 , ), ( a2 , )]  # c.B == [( b1 , ), ( b2 , )]  # c.C == [( c1 , ), ( c2 , )]  c   =   Channel . fromFile   ( chan.txt ,   skip   =   1 )  # c == [( a1 ,  b1 ,  c1 ), ( a2 ,  b2 ,  c2 )]     From  sys.argv  (command line arguments):\n   c   ==   channel . fromArgv ()  # python whatever.py /a/b/*.txt  # c == [( /a/b/1.txt ,), ( /a/b/2.txt ,), ( /a/b/3.txt ,), ( /a/b/4.txt ,)]  # Make a multple-variable channel:  # python whatever.py /a/b/1.txt,/a/b/2.txt /a/b/3.txt,/a/b/4.txt  # c == [( /a/b/1.txt ,  /a/b/2.txt ), ( /a/b/3.txt ,  /a/b/4.txt )]     From command line argument parser:\n  See  command line argument parser  for details.\n   from   PyPPL   import   Channel ,   params  params . a   =   a  params . b   =   2  params . b . type   =   int  params . c   =   [ 1 ,   2 ]  params . c . type   =   list  params . d   =   [ a ,   b ]  params . d . type   =   list  params . e   =   []  params . e . type   =   list  ch   =   Channel . fromParams ( c ,   e )  # Raises ValueError, non-equal length  ch   =   Channel . fromParams ( c ,   d )  # ch == [(1,  a ), (2,  b )]  ch   =   Channel . fromParams ( a ,   b )  # ch == [( a , 2)]", 
            "title": "Initialize a channel"
        }, 
        {
            "location": "/channels/#methods-for-channels", 
            "text": "", 
            "title": "Methods for channels"
        }, 
        {
            "location": "/channels/#get-the-length-and-width-of-a-channel", 
            "text": "chan   =   Channel . create   ([( 1 , 2 , 3 ),   ( 4 , 5 , 6 )])  #chan.length() == 2 == len(chan)  #chan.width()  == 3", 
            "title": "Get the length and width of a channel"
        }, 
        {
            "location": "/channels/#get-value-from-a-channel", 
            "text": "chan   =   Channel . create   ([( 1 , 2 , 3 ),   ( 4 , 5 , 6 )])  # chan.get() == 1  # chan.get(0) == 1  # chan.get(1) == 2  # chan.get(2) == 3  # chan.get(3) == 4  # chan.get(4) == 5  # chan.get(5) == 6", 
            "title": "Get value from a channel"
        }, 
        {
            "location": "/channels/#repeat-rows-and-columns", 
            "text": "chan   =   Channel . create   ([( 1 , 2 , 3 ),   ( 4 , 5 , 6 )])  chan2   =   chan . repCol ()  chan3   =   chan . repCol ( n = 3 )  # chan2 == [(1,2,3,1,2,3), (4,5,6,4,5,6)]  # chan3 == [(1,2,3,1,2,3,1,2,3), (4,5,6,4,5,6,4,5,6)]  chan4   =   chan . repRow ()  chan5   =   chan . repRow ( n = 3 )  # chan4 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6)]  # chan5 == [(1,2,3), (4,5,6), (1,2,3), (4,5,6), (1,2,3), (4,5,6)]", 
            "title": "Repeat rows and columns"
        }, 
        {
            "location": "/channels/#expand-a-channel-by-directory", 
            "text": "Channel.expand (col= 0, pattern = '*', t='any', sortby='name', reverse=False)  Sometimes we prepare files in one process (for example, split a big file into small ones in a directory), then handle these files by different jobs in another process, so that they can be processed simultaneously.       Caution   expand  only works for original channels with length is 1, which will expand to  N  (number of files included). If original channel has more than 1 element, only first element will be used, and other elements will be ignored.  Only the value of the column to be expanded will be changed, values of other columns remain the same.", 
            "title": "Expand a channel by directory"
        }, 
        {
            "location": "/channels/#collapse-a-channel-by-files-in-a-common-ancestor-directory", 
            "text": "Channel.collapse(col=0)  It's basically the reverse process of  expand . It applies when you deal with different files and in next process you need them all involved (i.e. combine the results):      Caution   os.path.dirname(os.path.commonprefix(...))  is used to detect the common ancestor directory, so the files could be  ['/a/1/1.file', '/a/2/1.file'] . In this case  /a/  will be returned.  values at other columns should be the same,  PyPPL  will NOT check it, the first value at the column will be used.", 
            "title": "Collapse a channel by files in a common ancestor directory"
        }, 
        {
            "location": "/channels/#fetch-rows-from-a-channel", 
            "text": "Channel.rowAt(index)      chan1   =   Channel . create   ([( 1 , 2 , 3 , 4 ),   ( 4 , 5 , 6 , 7 )])  chan2   =   chan1 . rowAt ( 1 )  # chan2 == [(4,5,6,7)]  # Now you can also fetch multiple columus as a channel:  chan3   =   chan1 . rowAt ([: 2 ])  chan3   ==   chan1", 
            "title": "Fetch rows from a channel"
        }, 
        {
            "location": "/channels/#fetch-columns-from-a-channel", 
            "text": "Channel.slice(start, length=None)   chan1   =   Channel . create   ([( 1 , 2 , 3 , 4 ),   ( 4 , 5 , 6 , 7 )])  chan2   =   chan1 . slice ( 1 , 2 )  # chan2 == [(2,3), (5,6)]  chan3   =   chan1 . slice ( 2 )  # chan3 == [(3,4), (6,7)]  chan4   =   chan1 . slice ( - 1 )  # chan4 == [(4,), (7,)]    Channel.colAt(index)   chan . colAt ( index )   ==   chan . slice ( index ,   1 )  # Now you may also fetch multiple columns:  chan . colAt ([ 1 , 2 ])   ==   chan . slice ( 1 ,   2 )", 
            "title": "Fetch columns from a channel"
        }, 
        {
            "location": "/channels/#flatten-a-channel", 
            "text": "Channel.flatten(col = None) \nFlatten a channel, make it into a list. chan    =   Channel . create   ([( 1 , 2 , 3 ),   ( 4 , 5 , 6 )])  f1   =   chan . flatten ()  # f1 == [1,2,3,4,5,6]  f2   =   chan . flatten ( 1 )  # f1 == [2,5]", 
            "title": "Flatten a channel"
        }, 
        {
            "location": "/channels/#split-a-channel-to-single-width-channels", 
            "text": "Channel.split(flatten = False)  chan    =   Channel . create   ([( 1 , 2 , 3 ),   ( 4 , 5 , 6 )])  chans   =   chan . split ()  # isinstance (chans, list) == True  # isinstance (chans, Channel) == False  # chans == [  #   [(1,), (4,)],  # isinstance (chans[0], Channel) == True  #   [(2,), (5,)],  #   [(3,), (6,)],  # ]  chans2   =   chan . split ( True )  # chans2 == [  #   [1, 4],        # isinstance (chans2[0], Channel) == False  #   [2, 5],  #   [3, 6],  # ]", 
            "title": "Split a channel to single-width channels"
        }, 
        {
            "location": "/channels/#attach-column-names", 
            "text": "Channel.attach(*names) \nWe can attach the column names and then use them to access the columns. ch   =   Channel . create ([( 1 , 2 , 3 ),   ( 4 , 5 , 6 )])  ch . attach   ( col1 ,   col2 ,   col3 )  # ch.col1 == [(1,), (4,)]  # ch.col2 == [(2,), (5,)]  # ch.col3 == [(3,), (6,)]  # isinstance(ch.col1, Channel) == True  # flatten the columns  ch . attach   ( col1 ,   col2 ,   col3 ,   True )  # ch.col1 == [1,4]  # ch.col2 == [2.5]  # ch.col3 == [3,6]  # isinstance(ch.col1, Channel) == False", 
            "title": "Attach column names"
        }, 
        {
            "location": "/channels/#map-filter-reduce", 
            "text": "Channel.map(func)  Channel.mapCol(func, col=0) \n   ch1   =   Channel . create ()  ch2   =   Channel . create ([ 1 , 2 , 3 , 4 , 5 ])  ch3   =   Channel . create ([( a ,   1 ),   ( b ,   2 )])  # ch1.map(lambda x: (x[0]*x[0],)) == []  # ch2.map(lambda x: (x[0]*x[0],)) == [(1,),(4,),(9,),(16,),(25,)]  # ch3.map(lambda x: (x[0], x[1]*x[1])) == [( a , 1), ( b , 4)]  # ch1.mapCol(lambda x: x*x) == []  # ch2.mapCol(lambda x: x*x) == [(1,),(4,),(9,),(16,),(25,)]  # ch3.mapCol(lambda x: x*x, 1) == [( a , 1), ( b , 4)]  # map   mapCol return an instance of Channel   Channel.filter(func)  Channel.filterCol(func, col=0) \n   ch1   =   Channel . create ([ \n   ( 1 ,      0 ,       0 ,     1    ), \n   ( a ,    ,      b ,   0 ), \n   ( True ,   False ,   0 ,     1    ), \n   ([],     [ 1 ],     [ 2 ],   [ 0 ]),  ])  # Filter by the first column, only first three rows remained  ch1 . filterCol ()   ==   ch1 [: 3 ]   # Filter by the second column, only the last row remained  ch1 . filterCol ( col   =   1 )   ==   ch1 [ 3 : 4 ]  # Filter by the third column, the 2nd and 4th row remained  ch1 . filterCol ( col   =   2 )   ==   [ ch1 [ 1 ],   ch1 [ 3 ]]  # Filter by the fourth column, all rows remained  ch1 . filterCol ( col   =   3 )   ==   ch1  # Filter with a function:\t\t  ch1 . filter ( lambda   x :   isinstance ( x [ 2 ],   int ))   ==   [ ch1 [ 0 ],   ch1 [ 2 ]]  # filter   filterCol return an instance of Channel   Channel.reduce(func)  Channel.reduceCol(func, col=0) \n   ch1   =   Channel . create ()  # Raises TypeError, no elements  ch1 . reduce ( lambda   x , y :   x + y )  ch1   =   Channel . create ([ 1 , 2 , 3 , 4 , 5 ])  # Notice the different  ch1 . reduce ( lambda   x , y :   x + y )   ==   ( 1 ,   2 ,   3 ,   4 ,   5 )   # x and y are tuples  ch1 . reduceCol ( lambda   x , y :   x + y )   ==   15             # x and y are numbers", 
            "title": "Map, filter, reduce"
        }, 
        {
            "location": "/channels/#add-rowscolumns-to-a-channel", 
            "text": "Channel.rbind(*rows)      Each row can be either a channel, a tuple, a list or a non-iterable element(including string)  \n   ch1   =   Channel . create ()  ch2   =   Channel . create (( 1 , 2 , 3 ))  row1   =   Channel . create ( 1 )  row2   =   Channel . create (( 2 , 2 , 2 ))  row3   =   [ 3 ]  row4   =   ( 3 ,)  row5   =   ( 4 , 4 , 4 )  row6   =   [ 4 , 4 , 4 ]  row7   =   5  ch1 . rbind ( row1 )   ==   [( 1 ,   )]  ch2 . rbind ( row1 )   ==   [( 1 , 2 , 3 ),( 1 , 1 , 1 )],  ch1 . rbind ( row2 )   ==   [( 2 , 2 , 2 )]  ch2 . rbind ( row2 )   ==   [( 1 , 2 , 3 ),   ( 2 , 2 , 2 )]  ch1 . rbind ( row3 )   ==   [( 3 ,)]  ch2 . rbind ( row3 )   ==   [( 1 , 2 , 3 ),( 3 , 3 , 3 )]  ch1 . rbind ( row4 )   ==   [( 3 ,)]  ch2 . rbind ( row4 )   ==   [( 1 , 2 , 3 ),( 3 , 3 , 3 )]  ch1 . rbind ( row5 )   ==   [( 4 , 4 , 4 )]  ch2 . rbind ( row5 )   ==   [( 1 , 2 , 3 ),( 4 , 4 , 4 )]  ch1 . rbind ( row6 )   ==   [( 4 , 4 , 4 )]  ch2 . rbind ( row6 )   ==   [( 1 , 2 , 3 ),( 4 , 4 , 4 )]  ch1 . rbind ( row7 )   ==   [( 5 ,)]  ch2 . rbind ( row7 )   ==   [( 1 , 2 , 3 ),( 5 , 5 , 5 )]     Channel.cbind(*cols) \n   ch1   =   Channel . create ([( 1 ,   2 ),   ( 3 ,   4 )])  ch2   =   Channel . create ([ 5 ,   6 ])  ch1 . cbind ( ch2 )   ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   6 )]  ch2   =   Channel . create ( 5 )  ch1 . cbind ( ch2 )      ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   5 )]  ch1 . cbind ([ 5 ,   6 ])   ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   6 )]  ch1 . cbind (( 5 ,   6 ))   ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   6 )]  ch1 . cbind ( a )      ==   [( 1 ,   2 ,   a ),   ( 3 ,   4 ,   a )]  ch1   =   Channel . create ()  ch2   =   Channel . create ([ 21 ,   22 ])  ch3   =   3  ch4   =   [ 41 ,   42 ]  ch5   =   ( 51 ,   52 )  ch6   =   a  ch1 . cbind ( ch2 ,   ch3 ,   ch4 ,   ch5 ,   ch6 )   ==   [( 21 ,   3 ,   41 ,   51 ,   a ),   ( 22 ,   3 ,   42 ,   52 ,   a )]  ch1 . cbind ( ch3 ) . cbind ( ch6 )   ==   [( 3 ,   a )]     Channel.insert(index, col) \n   ch1   =   Channel . create ([( 1 ,   2 ),   ( 3 ,   4 )])  ch2   =   Channel . create ([ 5 ,   6 ])  ch1 . insert ( 0 ,   ch2 )      ==   [( 5 ,   1 ,   2 ),   ( 6 ,   3 ,   4 )]  ch1 . insert ( 1 ,   ch2 )      ==   [( 1 ,   5 ,   2 ),   ( 3 ,   6 ,   4 )]  ch1 . insert ( - 1 ,   ch2 )     ==   [( 1 ,   5 ,   2 ),   ( 3 ,   6 ,   4 )]  ch1 . insert ( None ,   ch2 )   ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   6 )]  ch2   =   Channel . create ( 5 )  ch1 . insert ( 0 ,   ch2 )      ==   [( 5 ,   1 ,   2 ),   ( 5 ,   3 ,   4 )]  ch1 . insert ( 1 ,   ch2 )      ==   [( 1 ,   5 ,   2 ),   ( 3 ,   5 ,   4 )]  ch1 . insert ( - 1 ,   ch2 )     ==   [( 1 ,   5 ,   2 ),   ( 3 ,   5 ,   4 )]  ch1 . insert ( None ,   ch2 )   ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   5 )]  ch1 . insert ( 0 ,   [ 5 ,   6 ])      ==   [( 5 ,   1 ,   2 ),   ( 6 ,   3 ,   4 )]  ch1 . insert ( 1 ,   [ 5 ,   6 ])      ==   [( 1 ,   5 ,   2 ),   ( 3 ,   6 ,   4 )]  ch1 . insert ( - 1 ,   [ 5 ,   6 ])     ==   [( 1 ,   5 ,   2 ),   ( 3 ,   6 ,   4 )]  ch1 . insert ( None ,   [ 5 ,   6 ])   ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   6 )]  ch1 . insert ( 0 ,   ( 5 ,   6 ))      ==   [( 5 ,   1 ,   2 ),   ( 6 ,   3 ,   4 )]  ch1 . insert ( 1 ,   ( 5 ,   6 ))      ==   [( 1 ,   5 ,   2 ),   ( 3 ,   6 ,   4 )]  ch1 . insert ( - 1 ,   ( 5 ,   6 ))     ==   [( 1 ,   5 ,   2 ),   ( 3 ,   6 ,   4 )]  ch1 . insert ( None ,   ( 5 ,   6 ))   ==   [( 1 ,   2 ,   5 ),   ( 3 ,   4 ,   6 )]  ch1 . insert ( 0 ,   a )         ==   [( a ,   1 ,   2 ),   ( a ,   3 ,   4 )]  ch1 . insert ( 1 ,   a )         ==   [( 1 ,   a ,   2 ),   ( 3 ,   a ,   4 )]  ch1 . insert ( - 1 ,   a )        ==   [( 1 ,   a ,   2 ),   ( 3 ,   a ,   4 )]  ch1 . insert ( None ,   a )      ==   [( 1 ,   2 ,   a ),   ( 3 ,   4 ,   a )]  self . assertEqual ( ch1 ,   [( 1 ,   2 ),   ( 3 ,   4 )])  ch1   =   Channel . create ()  ch2   =   Channel . create ([ 21 ,   22 ])  ch3   =   3  ch4   =   [ 41 ,   42 ]  ch5   =   ( 51 ,   52 )  ch6   =   a  # Raises ValueError, when 1 is inserted, it is a 1-width channel,   # then you can t insert a 2-width to it.  ch1 . insert ( 1 ,   ch2 )  ch1 . insert ( 0 ,   ch2 ,   ch3 ,   ch4 ,   ch5 ,   ch6 )   ==   [( 21 ,   3 ,   41 ,   51 ,   a ),   ( 22 ,   3 ,   42 ,   52 ,   a )]", 
            "title": "Add rows/columns to a channel"
        }, 
        {
            "location": "/channels/#fold-a-channel", 
            "text": "Channel.fold(n = 1) \nFold a  channel , Make a row to n-length chunk rows\nFor example, you have the following channel:     a1  a2  a3  a4      b1  b2  b3  b4     After apply  chan.fold(2)  you will get:     a1  a2      a3  a4    b1  b2    b3  b4", 
            "title": "Fold a channel"
        }, 
        {
            "location": "/channels/#unfold-a-channel", 
            "text": "Channel.unfold(n=2) \nCombine n-rows into one row; do the reverse thing as  Channel.fold . But note that the different meaning of  n . In  fold ,  n  means the length of the chunk that a row is cut to; will in  unfold , it means how many rows to combine.", 
            "title": "Unfold a channel"
        }, 
        {
            "location": "/channels/#copy-a-channel", 
            "text": "Channel.copy()", 
            "title": "Copy a channel"
        }, 
        {
            "location": "/specify-input-and-output-of-a-process/", 
            "text": "Input and output of a process\n\n\nSpecify input of a process\n\n\nThe input of a process of basically a \ndict\n with keys as the placeholders and the values as the input channels:\n\n\np\n \n=\n \nProc\n()\n\n\np\n.\ninput\n \n=\n \n{\nph1\n:[\n1\n,\n2\n,\n3\n],\n \nph2\n:[\n4\n,\n5\n,\n6\n]}\n\n\n# You can also use combined keys and channels\n\n\n# p.input = {\nph1, ph2\n: [(1,4), (2,5), (3,6)]}\n\n\n\n\n\nThe complete form of an input key is \nkey\n:\ntype\n. The \ntype\n could be \nvar\n, \nfile\n (a.k.a \npath\n, \ndir\n or \nfolder\n) and \nfiles\n (a.k.a \npaths\n, \ndirs\n or \nfolders\n). \nA type of \nvar\n can be omitted.\n So \n{\"ph1\":[1,2,3], \"ph2\":[4,5,6]}\n is the same as \n{\"ph1:var\":[1,2,3], \"ph2:var\":[4,5,6]}\n\n\nYou can also use a \nstr\n or a \nlist\n if a process depends on a prior process, it will automatically use the output channel of the prior process, or you want to use the arguments from command line as input channel (in most case for starting processes, which do not depend on any other processes). For example:\n\n\n\n\nDanger\n\n\nThe number of input keys should be no more than that of the output from the prior process. Otherwise, there is not enough data for the keys.\n\n\n\n\n\n\nNote\n\n\nFor output, \ndict\n is not supported. As we need the order of the keys and data to be kept when it's being passed on. But you may use \nOrderedDict\n.\n\n\n\n\n\n\nHint\n\n\nIf you have input keys defined by a string before, for example:\n\np1\n.\ninput\n \n=\n \nph1, ph2\n\n\n\nYou can then specify the input data/channel directly:\n\np1\n.\ninput\n \n=\n \n[(\n1\n,\n4\n),\n \n(\n2\n,\n5\n),\n \n(\n3\n,\n6\n)]\n\n\n# same as:\n\n\np1\n.\ninput\n  \n=\n \n{\nph1\n:[\n1\n,\n2\n,\n3\n],\n \nph2\n:[\n4\n,\n5\n,\n6\n]}\n\n\n\nOne thing has to be reminded is that, you can do:\n\np1\n.\ninput\n \n=\n \n{\nin\n:\n \na\n}\n  \n# same as p1.input = {\nin\n: [\na\n]}\n\n\n\nBut you cannot do:\n\np1\n.\ninput\n \n=\n \nin\n\n\np1\n.\ninput\n \n=\n \na\n \n\n# the right way is p.input = [\na\n]\n\n\n# because PyPPL will take \na\n as the input key instead of data, as it\ns a string\n\n\n\n\n\n\n\n\nNote\n\n\nWhen a job is being prepared, the input files (type: \nfile\n, \npath\n, \ndir\n or \nfolder\n) will be linked to \nindir\n. In the template, for example, you may use \n{{in.infile}}\n to get its path. However, it may have different paths:  \n\n\n\n\nThe original path\n\n\nThe path from \nindir\n \n\n\nThe realpath (if the original file specified to the job is a symbolic link, it will be different from the original path)\n\n\n\n\nThen you are able to swith the value of \n{{in.infile}}\n using the setting \np.infile\n:  \n\n\n\n\n\"indir\"\n (default): The path from \nindir\n\n\n\"origin\"\n: The original path\n\n\n\"real\"\n: The realpath\n\n\n\n\nYou may also use them directly by:\n\n\n\n\n{{in.IN_infile}}\n: The path from \nindir\n\n\n{{in.OR_infile}}\n: The original path\n\n\n{{in.RE_infile}}\n: The realpath\n\n\n\n\n\n\nUse \nsys.argv\n (see details for \nChannel.fromArgv\n):\n\np3\n \n=\n \nProc\n()\n\n\np3\n.\ninput\n \n=\n \nin1\n\n\n# same as p3.input = {\nin1\n: channel.fromArgv ()}\n\n\n# Run the program: \n python test.py 1 2 3\n\n\n# Then in job#0: {{in.in1}} -\n 1\n\n\n# Then in job#1: {{in.in1}} -\n 2\n\n\n# Then in job#2: {{in.in1}} -\n 3\n\n\n\np4\n \n=\n \nProc\n()\n\n\np4\n.\ninput\n \n=\n \nin1, in2\n\n\n# same as p4.input = {\nin1, in2\n: channel.fromArgv ()}\n\n\n# Run the program: python test.py 1,a 2,b 3,c\n\n\n# Job#0: {{in.in1}} -\n 1, {{in.in2}} -\n a\n\n\n# Job#1: {{in.in1}} -\n 2, {{in.in2}} -\n b\n\n\n# Job#2: {{in.in1}} -\n 3, {{in.in2}} -\n c\n\n\n\n\nSpecify files as input\n\n\n\n\nUse a single file:\n  When you specify file as input, you should use \nfile\n (a.k.a \npath\n, \ndir\n or \nfolder\n) flag for the type: \n  \np\n.\ninput\n \n=\n \n{\ninfile:file\n:\n \nchannel\n.\nfromPattern\n(\n./*.txt\n)}\n\n\n\n  Then \nPyPPL\n will create symbolic links in \nworkdir\n/\njob.index\n/input/\n. \n\n\n\n\n\n\nNote\n The \n{{in.infile}}\n\n   will return the path of the link in \nindir\n pointing to the actual input file. If you want to get the path of the actual path, you may use: \n  \n{{ in.infile | readlink }} or {{ in._infile }}\n\n\n- Use a list of files:\n  Similar as a single file, but you have to specify it as \nfiles\n:\n  \np\n.\ninput\n \n=\n \n{\ninfiles:files\n:\n \n[\nchannel\n.\nfromPattern\n(\n./*.txt\n)\n.\nflatten\n()]}\n\n\n\n  Then remember \n{{in.infiles}}\n is a list, so is \n{{in._infiles}}\n\n- Rename input file links\n  When there are input files (different files) with the same basename, later ones will be renamed in \nindir\n. For example:\n  \npXXX\n.\ninput\n \n=\n \n{\n\n  \ninfile1:file\n:\n \n/path1/to/theSameBasename.txt\n,\n \n  \ninfile2:file\n:\n \n/path2/to/theSameBasename.txt\n\n\n}\n\n\n\n  Remember both files will have symblic links created in \nindir\n. To avoid \ninfile2\n being overwritten, the basename of the link will be \ntheSameBasename[1].txt\n. If you are using built-in template functions to get the filename (\n{{in.file2 | fn}}\n), we can still get \ntheSameBasename.txt\n instead of \ntheSameBasename[1].txt\n. \nbn\n, \nbasename\n, \nprefix\n act similarly.\n\n\n\n\nUse callback to modify the input channel\n\n\nYou can modify the input channel of a process by a callback. For example:\n\np1\n \n=\n \nProc\n()\n\n\np1\n.\ninput\n  \n=\n \n{\nph1\n:[\n1\n,\n2\n,\n3\n],\n \nph2\n:[\n4\n,\n5\n,\n6\n]}\n\n\np1\n.\noutput\n \n=\n \nout1:{{ph1}},out2:{{ph2}}\n\n\np1\n.\nscript\n \n=\n \n# your logic here\n\n\n# the output channel is [(1,4), (2,5), (3,6)]\n\n\np2\n.\ndepends\n \n=\n \np1\n\n\np2\n.\ninput\n   \n=\n \n{\nin1, in2\n:\n \nlambda\n \nch\n:\n \nch\n.\nslice\n(\n1\n)}\n  \n\n# just use the last 2 columns: [(2,5), (3,6)]\n\n\n# p1.channel keeps intact\n\n\n\nYou can check more examples in some channel methods: \nchannel.expand\n and \nchannel.collapse\n.\n\n\n\n\nCaution\n\n\nIf you use callback to modify the channel, you may combine the keys: in the above case \n\"in1, in2\": ...\n, or specify them independently: \np2.input = {\"in1\": lambda ch: ch.slice(1,1), \"in2\": lambda ch: ch.slice(2)}\n. But remember, \nall channels\n from \np2.depends\n will be passed to each callback function. For example:\n\np2\n.\ndepends\n \n=\n \n[\np0\n,\n \np1\n]\n\n\np2\n.\ninput\n   \n=\n \n{\nin1\n:\n \nlambda\n \nch0\n,\n \nch1\n:\n \n...\n,\n \nin2\n:\n \nlabmda\n \nch0\n,\n \nch1\n:\n \n...\n}\n\n\n# all channels from p2.depends are passed to each function\n\n\n\n\n\n\nSpecify output of a process\n\n\nDifferent from input, instead of channels, you have to tell \nPyPPL\n how to compute the output channel. The output can be a \nlist\n, \nstr\n or \nOrderedDict\n (\nbut not a \ndict\n, as the order of keys has to be kept\n). If it's \nstr\n, a comma (\n,\n) is used to separate different keys:\n\np\n.\ninput\n  \n=\n \n{\ninvar\n:[\n1\n],\n \ninfile:file\n:\n \n[\n/a/b/c.txt\n]}\n\n\np\n.\noutput\n \n=\n \noutvar:var:{{in.invar}}2, outfile:file:{{in.infile | bn}}2, outdir:dir:{{in.indir | fn}}-dir\n\n\n# The type \nvar\n is omitted in the first element.\n\n\n# The output channel (pXXX.channel) will be:\n\n\n# [(\n12\n, \nc.txt2\n, \nc-dir\n)]\n\n\n\nThe output keys are automatically attached to the output channel, so you may use them to access the columns. In previous example:\n\np\n.\nchannel\n.\noutvar\n  \n==\n \n[(\n12\n,\n \n)]\n\n\np\n.\nchannel\n.\noutfile\n \n==\n \n[(\noutdir\n/c.txt2\n,\n \n)]\n\n\np\n.\nchannel\n.\noutdir\n  \n==\n \n[(\noutdir\n/c-dir\n,\n \n)]\n\n\n\n\nTypes of input and output\n\n\n\n\n\n\n\n\nInput/Output\n\n\nType\n\n\nAliases\n\n\nBehavior\n\n\nExample-assignment (\np.input/output=?\n)\n\n\nExample-template-value\n\n\n\n\n\n\n\n\n\n\nInput\n\n\nvar\n\n\n-\n\n\nUse the value directly\n\n\n{\"in:var\": [1]}\n\n\n{{in.in}} -\n 1\n\n\n\n\n\n\nInput\n\n\nfile\n\n\npath\ndir\nfolder\n\n\nCreate link in \nindir\n and assign the original path to \nin._in\n\n\n{\"in:file\": [\"/path/to/file\"]}\n\n\n{{in.in}} -\n \nindir\n/file\n{{in._in}} -\n /path/to/file\n\n\n\n\n\n\nInput\n\n\nfiles\n\n\npaths\ndirs\nfolders\n\n\nSame as \nfile\n but do for multiple files\n\n\n{\n\"in:files\":\n([\"/path/to/file1\",\n\"/path/to/file2\"],)\n}\n\n\n{{in.in\nasquote}} -\n \"\nindir\n/file1\" \"\nindir\n/file2\"\n{{in._in\nasquote}} -\n \"/path/to/file1\" \"/path/to/file2\"\n\n\n\n\n\n\nOutput\n\n\nvar\n\n\n-\n\n\nSpecify direct value\n\n\n\"out:var:{{job.index}}\"\n\n\n{{out.out}} -\n \njob.index\n\n\n\n\n\n\nOutput\n\n\nfile\n\n\npath\n\n\nJust specify the basename, output file will be generated in \njob.outdir\n\n\n\"out:file:{{in.infile\nfn}}.out\"\n\n\n{{out.out}} == \noutdir\n/\nfilename of infile\n.out\n\n\n\n\n\n\nOutput\n\n\ndir\n\n\nfolder\n\n\nDo the same thing as \nfile\n but will create the directory\n\n\n\"out:dir:{{in.infile\nfn}}-outdir\"\n\n\n{{out.out}} == \noutdir\n/\nfilename of infile\n-outdir\n \n(automatically created)\n\n\n\n\n\n\nOutput\n\n\nstdout\n\n\n-\n\n\nLink \njob.stdout\n file to \noutdir\n\n\nout:stdout:{{in.infile\nfn}}.out\n\n\n{{out.out}} == \noutdir\n/\nfilename of infile\n.out\n\n\n\n\n\n\nOutput\n\n\nstderr\n\n\n-\n\n\nLink \njob.stderr\n file to \noutdir\n\n\nerr:stderr:{{in.infile\nfn}}.err\n\n\n{{out.err}} == \noutdir\n/\nfilename of infile\n.err", 
            "title": "Input and output of a process"
        }, 
        {
            "location": "/specify-input-and-output-of-a-process/#input-and-output-of-a-process", 
            "text": "", 
            "title": "Input and output of a process"
        }, 
        {
            "location": "/specify-input-and-output-of-a-process/#specify-input-of-a-process", 
            "text": "The input of a process of basically a  dict  with keys as the placeholders and the values as the input channels:  p   =   Proc ()  p . input   =   { ph1 :[ 1 , 2 , 3 ],   ph2 :[ 4 , 5 , 6 ]}  # You can also use combined keys and channels  # p.input = { ph1, ph2 : [(1,4), (2,5), (3,6)]}   The complete form of an input key is  key : type . The  type  could be  var ,  file  (a.k.a  path ,  dir  or  folder ) and  files  (a.k.a  paths ,  dirs  or  folders ).  A type of  var  can be omitted.  So  {\"ph1\":[1,2,3], \"ph2\":[4,5,6]}  is the same as  {\"ph1:var\":[1,2,3], \"ph2:var\":[4,5,6]}  You can also use a  str  or a  list  if a process depends on a prior process, it will automatically use the output channel of the prior process, or you want to use the arguments from command line as input channel (in most case for starting processes, which do not depend on any other processes). For example:   Danger  The number of input keys should be no more than that of the output from the prior process. Otherwise, there is not enough data for the keys.    Note  For output,  dict  is not supported. As we need the order of the keys and data to be kept when it's being passed on. But you may use  OrderedDict .    Hint  If you have input keys defined by a string before, for example: p1 . input   =   ph1, ph2  \nYou can then specify the input data/channel directly: p1 . input   =   [( 1 , 4 ),   ( 2 , 5 ),   ( 3 , 6 )]  # same as:  p1 . input    =   { ph1 :[ 1 , 2 , 3 ],   ph2 :[ 4 , 5 , 6 ]}  \nOne thing has to be reminded is that, you can do: p1 . input   =   { in :   a }    # same as p1.input = { in : [ a ]}  \nBut you cannot do: p1 . input   =   in  p1 . input   =   a   # the right way is p.input = [ a ]  # because PyPPL will take  a  as the input key instead of data, as it s a string     Note  When a job is being prepared, the input files (type:  file ,  path ,  dir  or  folder ) will be linked to  indir . In the template, for example, you may use  {{in.infile}}  to get its path. However, it may have different paths:     The original path  The path from  indir    The realpath (if the original file specified to the job is a symbolic link, it will be different from the original path)   Then you are able to swith the value of  {{in.infile}}  using the setting  p.infile :     \"indir\"  (default): The path from  indir  \"origin\" : The original path  \"real\" : The realpath   You may also use them directly by:   {{in.IN_infile}} : The path from  indir  {{in.OR_infile}} : The original path  {{in.RE_infile}} : The realpath    Use  sys.argv  (see details for  Channel.fromArgv ): p3   =   Proc ()  p3 . input   =   in1  # same as p3.input = { in1 : channel.fromArgv ()}  # Run the program:   python test.py 1 2 3  # Then in job#0: {{in.in1}} -  1  # Then in job#1: {{in.in1}} -  2  # Then in job#2: {{in.in1}} -  3  p4   =   Proc ()  p4 . input   =   in1, in2  # same as p4.input = { in1, in2 : channel.fromArgv ()}  # Run the program: python test.py 1,a 2,b 3,c  # Job#0: {{in.in1}} -  1, {{in.in2}} -  a  # Job#1: {{in.in1}} -  2, {{in.in2}} -  b  # Job#2: {{in.in1}} -  3, {{in.in2}} -  c", 
            "title": "Specify input of a process"
        }, 
        {
            "location": "/specify-input-and-output-of-a-process/#specify-files-as-input", 
            "text": "Use a single file:\n  When you specify file as input, you should use  file  (a.k.a  path ,  dir  or  folder ) flag for the type: \n   p . input   =   { infile:file :   channel . fromPattern ( ./*.txt )}  \n  Then  PyPPL  will create symbolic links in  workdir / job.index /input/ .     Note  The  {{in.infile}} \n   will return the path of the link in  indir  pointing to the actual input file. If you want to get the path of the actual path, you may use: \n   {{ in.infile | readlink }} or {{ in._infile }} \n- Use a list of files:\n  Similar as a single file, but you have to specify it as  files :\n   p . input   =   { infiles:files :   [ channel . fromPattern ( ./*.txt ) . flatten ()]}  \n  Then remember  {{in.infiles}}  is a list, so is  {{in._infiles}} \n- Rename input file links\n  When there are input files (different files) with the same basename, later ones will be renamed in  indir . For example:\n   pXXX . input   =   { \n   infile1:file :   /path1/to/theSameBasename.txt ,  \n   infile2:file :   /path2/to/theSameBasename.txt  }  \n  Remember both files will have symblic links created in  indir . To avoid  infile2  being overwritten, the basename of the link will be  theSameBasename[1].txt . If you are using built-in template functions to get the filename ( {{in.file2 | fn}} ), we can still get  theSameBasename.txt  instead of  theSameBasename[1].txt .  bn ,  basename ,  prefix  act similarly.", 
            "title": "Specify files as input"
        }, 
        {
            "location": "/specify-input-and-output-of-a-process/#use-callback-to-modify-the-input-channel", 
            "text": "You can modify the input channel of a process by a callback. For example: p1   =   Proc ()  p1 . input    =   { ph1 :[ 1 , 2 , 3 ],   ph2 :[ 4 , 5 , 6 ]}  p1 . output   =   out1:{{ph1}},out2:{{ph2}}  p1 . script   =   # your logic here  # the output channel is [(1,4), (2,5), (3,6)]  p2 . depends   =   p1  p2 . input     =   { in1, in2 :   lambda   ch :   ch . slice ( 1 )}    # just use the last 2 columns: [(2,5), (3,6)]  # p1.channel keeps intact  \nYou can check more examples in some channel methods:  channel.expand  and  channel.collapse .   Caution  If you use callback to modify the channel, you may combine the keys: in the above case  \"in1, in2\": ... , or specify them independently:  p2.input = {\"in1\": lambda ch: ch.slice(1,1), \"in2\": lambda ch: ch.slice(2)} . But remember,  all channels  from  p2.depends  will be passed to each callback function. For example: p2 . depends   =   [ p0 ,   p1 ]  p2 . input     =   { in1 :   lambda   ch0 ,   ch1 :   ... ,   in2 :   labmda   ch0 ,   ch1 :   ... }  # all channels from p2.depends are passed to each function", 
            "title": "Use callback to modify the input channel"
        }, 
        {
            "location": "/specify-input-and-output-of-a-process/#specify-output-of-a-process", 
            "text": "Different from input, instead of channels, you have to tell  PyPPL  how to compute the output channel. The output can be a  list ,  str  or  OrderedDict  ( but not a  dict , as the order of keys has to be kept ). If it's  str , a comma ( , ) is used to separate different keys: p . input    =   { invar :[ 1 ],   infile:file :   [ /a/b/c.txt ]}  p . output   =   outvar:var:{{in.invar}}2, outfile:file:{{in.infile | bn}}2, outdir:dir:{{in.indir | fn}}-dir  # The type  var  is omitted in the first element.  # The output channel (pXXX.channel) will be:  # [( 12 ,  c.txt2 ,  c-dir )]  \nThe output keys are automatically attached to the output channel, so you may use them to access the columns. In previous example: p . channel . outvar    ==   [( 12 ,   )]  p . channel . outfile   ==   [( outdir /c.txt2 ,   )]  p . channel . outdir    ==   [( outdir /c-dir ,   )]", 
            "title": "Specify output of a process"
        }, 
        {
            "location": "/specify-input-and-output-of-a-process/#types-of-input-and-output", 
            "text": "Input/Output  Type  Aliases  Behavior  Example-assignment ( p.input/output=? )  Example-template-value      Input  var  -  Use the value directly  {\"in:var\": [1]}  {{in.in}} -  1    Input  file  path dir folder  Create link in  indir  and assign the original path to  in._in  {\"in:file\": [\"/path/to/file\"]}  {{in.in}} -   indir /file {{in._in}} -  /path/to/file    Input  files  paths dirs folders  Same as  file  but do for multiple files  { \"in:files\": ([\"/path/to/file1\", \"/path/to/file2\"],) }  {{in.in asquote}} -  \" indir /file1\" \" indir /file2\" {{in._in asquote}} -  \"/path/to/file1\" \"/path/to/file2\"    Output  var  -  Specify direct value  \"out:var:{{job.index}}\"  {{out.out}} -   job.index    Output  file  path  Just specify the basename, output file will be generated in  job.outdir  \"out:file:{{in.infile fn}}.out\"  {{out.out}} ==  outdir / filename of infile .out    Output  dir  folder  Do the same thing as  file  but will create the directory  \"out:dir:{{in.infile fn}}-outdir\"  {{out.out}} ==  outdir / filename of infile -outdir   (automatically created)    Output  stdout  -  Link  job.stdout  file to  outdir  out:stdout:{{in.infile fn}}.out  {{out.out}} ==  outdir / filename of infile .out    Output  stderr  -  Link  job.stderr  file to  outdir  err:stderr:{{in.infile fn}}.err  {{out.err}} ==  outdir / filename of infile .err", 
            "title": "Types of input and output"
        }, 
        {
            "location": "/write-your-script/", 
            "text": "Write and debug your script\n\n\n\n\n\nChoose your language\n\n\nYou can either specify the path of interpreter to \npXXX.lang\n. If the interpreter is in \n$PATH\n, you can directly give the basename of the interpreter.\n\nFor example, if you have your own perl installed at \n/home/user/bin/perl\n, then you need to tell \nPyPPL\n where it is: \npXXX.lang = \"/home/user/bin/perl\"\n. If \n/home/user/bin\n is in your \n$PATH\n, you can simply do: \np.lang = \"perl\"\n\nYou can also use \nshebang\n to specify the interperter:\n\n#!/home/usr/bin/perl\n\n\n# You perl code goes here\n\n\n\n\nUse script from a file\n\n\nYou can also put the script into a file, and use it with a \nfile:\n prefix: \npXXX.script = \"file:/a/b/c.pl\"\n  \n\n\n\n\nNote\n\n\nYou may also use a relative-path template, which is relative to where \npXXX.script\n is defined. For example: \npXXX.script = \"file:./scripts/script.py\"\n is defined in \n/a/b/pipeline.py\n, then the script file refers to \n/a/b/scripts/script.py\n\n\n\n\n\n\nHint\n\n\nIndents are important in python, when you write your scripts, you have to follow exactly the indents in the script string, for example:\n\n\n\n\ndef\n \ntest\n():\n\n    \np\n \n=\n \nProc\n()\n\n    \np\n.\nlang\n \n=\n \npython\n\n    \np\n.\nscript\n \n=\n \n\n\nimport os\n\n\nimport re\n\n\ndef somefunc ():\n\n\n    pass\n\n\n\n\n\nBut with \n'# PYPPL INDENT REMOVE'\n, you can do it more elegantly:\n\ndef\n \ntest\n():\n\n    \np\n \n=\n \nproc\n()\n\n    \np\n.\nlang\n \n=\n \npython\n\n    \np\n.\nscript\n \n=\n \n\n\n    # make sure it\ns not at the beginning of the file\n\n\n    # PYPPL INDENT REMOVE\n\n\n    import os\n\n\n    import re\n\n\n    def somefunc():\n\n\n        pass\n\n\n    \n\n\n\nThe leading white spaces of line \n# PYPPL INDENT REMOVE\n will be removed for each line (including itself) below it. In this case, the extra \ntab\n of pass will be kept.\n\nYou may use \n# PYPPL INDENT KEEP\n to stop removing the white spaces for the following lines.\n\n\n\n\nCaution\n\n\n# PYPPL INDENT REMOVE\n Should not be at the beginning of the file, otherwise the leading spaces will be stripped so we can detect how many spaces should be removed for the following lines.\n\n\n\n\nDebug your script\n\n\nIf you need to debug your script, you just need to find the real running script, which is at: \nworkdir\n/\njob.index\n/job.script\n. The template is rendered already in the file. You can debug it using the tool according to the language you used for the script.\n\n\nYou may also add logs to pyppl's main logs on the screen or in log files. To do that, you just need to print you message starting with \npyppl.log\n to STDERR:\n\n# python\n\n\nimport\n \nsys\n\n\nsys\n.\nstderr\n.\nwrite\n(\npyppl.log: Something for debug.\n)\n\n\n\n\n# bash\n\n\necho\n \npyppl.log: Something for debug.\n \n1\n2\n\n\n\n\n\n# Rscript\n\n\ncat\n(\npyppl.log: Something for debug.\n,\n file \n=\n \nstderr\n())\n\n\n\nEither one of the above will have a log message like:\n\n[2017-01-01 01:01:01][    LOG] Something for debug.\n\n\nYou may also use a different log level (flag):\n\n# python\n\n\nimport\n \nsys\n\n\nsys\n.\nstderr\n.\nwrite\n(\npyppl.log.flag: Something for debug.\n)\n\n\n\nThen the log message will be:\n\n[2017-01-01 01:01:01][   FLAG] Something for debug.\n\n\n\n\n\nNote\n\n\nYou have to tell \nPyPPL\n which jobs to output these logs.\nJust simply by:\n\n# You have to specify an empty string to \ntype\n to disable other outputs, unless you want them.\n\n\npXXX\n.\necho\n \n=\n \n{\njobs\n:\n \n[\n0\n,\n1\n,\n2\n,\n3\n],\n \ntype\n:\n \n}\n\n\n\n\n\n\n\n\nNote\n\n\nThe level name you specified after \npyppl.log\n does not apply to \nnormal log filters or themes\n, because the actual level is \n_FLAG\n in this case. So unless you set \nloglevels\n to \nNone\n, it will be anyway printed out. For themes, the color at the empty string key will be used. \n\n\nYou can define filters or themes for this kind of logs, just remember the actual level name has an \n_\n prefix. See \nhere\n to learn how to define filters and themes.\n\n\n\n\nOutput stdout/stderr to PyPPL logs\n\n\nInstead of log some information, you may also choose to output the stdout/stderr from the jobs to the main \nPyPPL\n log.\n\n\nThis is controlled by setting \np.echo\n, which is set to \nFalse\n by default. The full configuration of the value could be:\n\n{\n\n    \njobs\n:\n \n[\n0\n,\n \n1\n],\n \n# the jobs that are allowed to output\n\n    \n# the regular expression for each type of output\n\n    \ntype\n:\n \n{\nstdout\n:\n \nr\n^STDOUT:\n,\n \nstderr\n:\n \nr\n^STDERR\n}\n \n\n}\n\n\n\nBut there are also some abbrevations for the setting:\n\n\n\n\n\n\n\n\nAbbrevation (\np.echo = ?\n)\n\n\nFull setting\n\n\nMemo\n\n\n\n\n\n\n\n\n\n\nFalse\n\n\nFalse\n\n\nDisable output\n\n\n\n\n\n\nTrue\n\n\n{'jobs':[0], 'type': {'stderr':None, 'stdout':None}}\n\n\nOutput all stdout/stderr of job #0\n\n\n\n\n\n\n'stderr'\n\n\n{'jobs':[0], 'type': {'stderr':None}}\n\n\nOutput all stderr of job #0\n\n\n\n\n\n\n{'jobs':0, 'type': 'stdout'}\n\n\n{'jobs':[0], 'type': {'stdout':None}}\n\n\nOutput all stdout of job #0\n\n\n\n\n\n\n{'type': {'all': r'^output'}}\n\n\n{ 'jobs': [0], 'type': {'stdout': r'^output', 'stderr': r'^output'} }\n\n\nOutput all lines starting with \n\"output\"\n from stdout/stderr of job #0", 
            "title": "The heart: script"
        }, 
        {
            "location": "/write-your-script/#write-and-debug-your-script", 
            "text": "", 
            "title": "Write and debug your script"
        }, 
        {
            "location": "/write-your-script/#choose-your-language", 
            "text": "You can either specify the path of interpreter to  pXXX.lang . If the interpreter is in  $PATH , you can directly give the basename of the interpreter. \nFor example, if you have your own perl installed at  /home/user/bin/perl , then you need to tell  PyPPL  where it is:  pXXX.lang = \"/home/user/bin/perl\" . If  /home/user/bin  is in your  $PATH , you can simply do:  p.lang = \"perl\" \nYou can also use  shebang  to specify the interperter: #!/home/usr/bin/perl  # You perl code goes here", 
            "title": "Choose your language"
        }, 
        {
            "location": "/write-your-script/#use-script-from-a-file", 
            "text": "You can also put the script into a file, and use it with a  file:  prefix:  pXXX.script = \"file:/a/b/c.pl\"      Note  You may also use a relative-path template, which is relative to where  pXXX.script  is defined. For example:  pXXX.script = \"file:./scripts/script.py\"  is defined in  /a/b/pipeline.py , then the script file refers to  /a/b/scripts/script.py    Hint  Indents are important in python, when you write your scripts, you have to follow exactly the indents in the script string, for example:   def   test (): \n     p   =   Proc () \n     p . lang   =   python \n     p . script   =    import os  import re  def somefunc ():      pass   \nBut with  '# PYPPL INDENT REMOVE' , you can do it more elegantly: def   test (): \n     p   =   proc () \n     p . lang   =   python \n     p . script   =        # make sure it s not at the beginning of the file      # PYPPL INDENT REMOVE      import os      import re      def somefunc():          pass        \nThe leading white spaces of line  # PYPPL INDENT REMOVE  will be removed for each line (including itself) below it. In this case, the extra  tab  of pass will be kept. \nYou may use  # PYPPL INDENT KEEP  to stop removing the white spaces for the following lines.   Caution  # PYPPL INDENT REMOVE  Should not be at the beginning of the file, otherwise the leading spaces will be stripped so we can detect how many spaces should be removed for the following lines.", 
            "title": "Use script from a file"
        }, 
        {
            "location": "/write-your-script/#debug-your-script", 
            "text": "If you need to debug your script, you just need to find the real running script, which is at:  workdir / job.index /job.script . The template is rendered already in the file. You can debug it using the tool according to the language you used for the script.  You may also add logs to pyppl's main logs on the screen or in log files. To do that, you just need to print you message starting with  pyppl.log  to STDERR: # python  import   sys  sys . stderr . write ( pyppl.log: Something for debug. )   # bash  echo   pyppl.log: Something for debug.   1 2   # Rscript  cat ( pyppl.log: Something for debug. ,  file  =   stderr ())  \nEither one of the above will have a log message like: [2017-01-01 01:01:01][    LOG] Something for debug. \nYou may also use a different log level (flag): # python  import   sys  sys . stderr . write ( pyppl.log.flag: Something for debug. )  \nThen the log message will be: [2017-01-01 01:01:01][   FLAG] Something for debug.   Note  You have to tell  PyPPL  which jobs to output these logs.\nJust simply by: # You have to specify an empty string to  type  to disable other outputs, unless you want them.  pXXX . echo   =   { jobs :   [ 0 , 1 , 2 , 3 ],   type :   }     Note  The level name you specified after  pyppl.log  does not apply to  normal log filters or themes , because the actual level is  _FLAG  in this case. So unless you set  loglevels  to  None , it will be anyway printed out. For themes, the color at the empty string key will be used.   You can define filters or themes for this kind of logs, just remember the actual level name has an  _  prefix. See  here  to learn how to define filters and themes.", 
            "title": "Debug your script"
        }, 
        {
            "location": "/write-your-script/#output-stdoutstderr-to-pyppl-logs", 
            "text": "Instead of log some information, you may also choose to output the stdout/stderr from the jobs to the main  PyPPL  log.  This is controlled by setting  p.echo , which is set to  False  by default. The full configuration of the value could be: { \n     jobs :   [ 0 ,   1 ],   # the jobs that are allowed to output \n     # the regular expression for each type of output \n     type :   { stdout :   r ^STDOUT: ,   stderr :   r ^STDERR }   }  \nBut there are also some abbrevations for the setting:     Abbrevation ( p.echo = ? )  Full setting  Memo      False  False  Disable output    True  {'jobs':[0], 'type': {'stderr':None, 'stdout':None}}  Output all stdout/stderr of job #0    'stderr'  {'jobs':[0], 'type': {'stderr':None}}  Output all stderr of job #0    {'jobs':0, 'type': 'stdout'}  {'jobs':[0], 'type': {'stdout':None}}  Output all stdout of job #0    {'type': {'all': r'^output'}}  { 'jobs': [0], 'type': {'stdout': r'^output', 'stderr': r'^output'} }  Output all lines starting with  \"output\"  from stdout/stderr of job #0", 
            "title": "Output stdout/stderr to PyPPL logs"
        }, 
        {
            "location": "/export-output-files/", 
            "text": "Export output files\n\n\n\n\n\nOutput files are generated in \noutdir\n(\njob.index\n/output\n) if you specify the basename for a \nfile/path/dir\n for process output. You can export them to a specific directory by specify the directory to \nexdir\n of a process: \npXXX.exdir = \nexdir\n.\n\n\nYou can use different ways (specify it to \nexporthow\n (alias: \nexhow\n) of a process) to export output files:\n\n\n\n\n\n\n\n\nWays to export (\np.exhow=?\n)\n\n\nAliases\n\n\nWhat happens\n\n\n\n\n\n\n\n\n\n\nmove\n(default)\n\n\nmv\n\n\nMove output files to the directory, leave links to them in \noutdir\n (make sure processes depend on this one can run normally).\n\n\n\n\n\n\ncopy\n\n\ncp\n\n\nCopy output files to the directory\n\n\n\n\n\n\nsymlink\n\n\nlink\n, \nsymbol\n\n\nCreate symbolic links to the output files in the directory\n\n\n\n\n\n\ngzip\n\n\ngz\n\n\nIf output is a file, will do \ngzip\n of the file and save the gzipped file in the export directory; if output is a directory, will do \ntar -zcvf\n of the output directory and save the result file in the export directory.\n\n\n\n\n\n\n\n\nYou can export the output files of any process. Note that even though the export directory is specific to a process, the minimum unit is a \njob\n, whose output files are ready to be exported once it finishes successfully.\n\n\nYou can ask \nPyPPL\n whether to overwrite the existing files in the export directory by set \nexow\n as \nTrue\n (overwrite) or \nFalse\n (do not overwrite).\n\n\n\n\nNote\n\n\nif the directory you specified to \npXXX.exdir\n does not exist, it will be created automatically, including those intermediate directories if necessary.\n\n\n\n\nPartial export\n\n\nYou can also partially export the output files by set value to \npXXX.expart\n.\n\nYou have 2 ways to select the files:\n- Output key. For example, for \np.output = \"outfile1:file:a.txt1, outfile2:file:b.txt2\"\n, you can export only \noutfile1\n by: \np.expart = \"outfile1\"\n\n- Glob patterns. In the above example, you can also do: \np.expart = \"*.txt1\"\n\n\nYou can have multiple selectors: \np.expart = [\"*.txt1\", \"outfile2\"]\n to export all files.\n\n\n\n\nNote\n\n\n\n\nExport-caching will not be allowed when using partial export.\n\n\nTemplating is applied for this option.  \n\n\nexpart\n will first match output keys and then be used as a glob pattern. So if you have \n\n# ...\n\n\np\n.\noutput\n \n=\n \noutfile1:file:outfile2, outfile2:file:b.txt2\n\n\n# ...\n\n\np\n.\nexpart\n \n=\n \n[\noutfile2\n]\n\n\n\nthen \nb.txt2\n will be exported instead of \njob.outdir\n/outfile2\n\n\n\n\n\n\nControl of export of cached jobs\n\n\nBy default, if a job is cached, then it will not try to export the output files again (assuming that you have already successfully run the job and exported the output files). But you can force to export them anyway by setting \np.cclean = True", 
            "title": "Output file exporting"
        }, 
        {
            "location": "/export-output-files/#export-output-files", 
            "text": "Output files are generated in  outdir ( job.index /output ) if you specify the basename for a  file/path/dir  for process output. You can export them to a specific directory by specify the directory to  exdir  of a process:  pXXX.exdir =  exdir .  You can use different ways (specify it to  exporthow  (alias:  exhow ) of a process) to export output files:     Ways to export ( p.exhow=? )  Aliases  What happens      move (default)  mv  Move output files to the directory, leave links to them in  outdir  (make sure processes depend on this one can run normally).    copy  cp  Copy output files to the directory    symlink  link ,  symbol  Create symbolic links to the output files in the directory    gzip  gz  If output is a file, will do  gzip  of the file and save the gzipped file in the export directory; if output is a directory, will do  tar -zcvf  of the output directory and save the result file in the export directory.     You can export the output files of any process. Note that even though the export directory is specific to a process, the minimum unit is a  job , whose output files are ready to be exported once it finishes successfully.  You can ask  PyPPL  whether to overwrite the existing files in the export directory by set  exow  as  True  (overwrite) or  False  (do not overwrite).   Note  if the directory you specified to  pXXX.exdir  does not exist, it will be created automatically, including those intermediate directories if necessary.", 
            "title": "Export output files"
        }, 
        {
            "location": "/export-output-files/#partial-export", 
            "text": "You can also partially export the output files by set value to  pXXX.expart . \nYou have 2 ways to select the files:\n- Output key. For example, for  p.output = \"outfile1:file:a.txt1, outfile2:file:b.txt2\" , you can export only  outfile1  by:  p.expart = \"outfile1\" \n- Glob patterns. In the above example, you can also do:  p.expart = \"*.txt1\"  You can have multiple selectors:  p.expart = [\"*.txt1\", \"outfile2\"]  to export all files.   Note   Export-caching will not be allowed when using partial export.  Templating is applied for this option.    expart  will first match output keys and then be used as a glob pattern. So if you have  # ...  p . output   =   outfile1:file:outfile2, outfile2:file:b.txt2  # ...  p . expart   =   [ outfile2 ]  \nthen  b.txt2  will be exported instead of  job.outdir /outfile2", 
            "title": "Partial export"
        }, 
        {
            "location": "/export-output-files/#control-of-export-of-cached-jobs", 
            "text": "By default, if a job is cached, then it will not try to export the output files again (assuming that you have already successfully run the job and exported the output files). But you can force to export them anyway by setting  p.cclean = True", 
            "title": "Control of export of cached jobs"
        }, 
        {
            "location": "/caching/", 
            "text": "Caching and resuming processes\n\n\n\n\n\nProcess caching\n\n\nOnce a job is cached, \nPyPPL\n will skip running this job. But you have to tell a process how to cache its jobs by setting \npXXX.cache\n with a valid caching method:\n\n\n\n\n\n\n\n\nCaching method (\np.cache=?\n)\n\n\nHow\n\n\n\n\n\n\n\n\n\n\nTrue\n\n\nA signature\n*\n of input files, script and output files of a job is cached in \nworkdir\n/\njob.index\n/job.cache\n, compare the signature before a job starts to run.\n\n\n\n\n\n\nFalse\n\n\nDisable caching, always run jobs.\n\n\n\n\n\n\n\"export\"\n\n\nFirst try to find the signatures, if failed, try to restore the files existed (or exported previously in \np.exdir\n).\n\n\n\n\n\n\n\n\n\n\nHint\n: \np.cache = \"export\"\n is extremely useful for a process that you only want it to run successfully once, export the result files and never run the process again. You can even delete the \nworkdir\n of the process, but \nPyPPL\n will find the exported files and use them as the input for processes depending on it, so that you don't need to modify the pipeline.\n\nOne scenario is that you can use it to download some files and never need to download them again.\n\n\n\n\nResuming from processes\n\n\nSometimes, you may not want to start at the very begining of a pipeline. Then you can resume it from some intermediate processes.\n\nTo resume pipeline from a process, you have to make sure that the output files of the processes that this process depends on are already generated. Then you can do:\n\nPyPPL\n()\n.\nstart\n(\n...\n)\n.\nresume\n(\npXXX\n)\n.\nrun\n()\n\n\n\nOr if the process uses the data from other processes, especially the output channel, you may need \nPyPPL\n to infer (not neccessary run the script) the output data for processes that this process depends on. Then you can do:\n\nPyPPL\n()\n.\nstart\n(\n...\n)\n.\nresume2\n(\npXXX\n)\n.\nrun\n()\n\n\n\nYou may also use a common id to set a set of processes:\n\np1\n \n=\n \nProc\n(\nnewid\n \n=\n \np\n,\n \ntag\n \n=\n \n1st\n)\n\n\np2\n \n=\n \nProc\n(\nnewid\n \n=\n \np\n,\n \ntag\n \n=\n \n2nd\n)\n\n\np3\n \n=\n \nProc\n(\nnewid\n \n=\n \np\n,\n \ntag\n \n=\n \n3rd\n)\n\n\n# pipeline will be resumed from p1, p2, p3\n\n\nPyPPL\n()\n.\nstart\n(\n...\n)\n.\nresume\n(\np\n)\n.\nrun\n()\n\n\n\n\nCalculating signatures for caching\n\n\nBy default, \nPyPPL\n uses the last modified time to generate signatures for files and directories. However, for large directories, it may take notably long time to walk over all the files in those directories. If not necessary, you may simply as \nPyPPL\n to get the last modified time for the directories themselves instead of the infiles inside them by setting \np.dirsig = False", 
            "title": "Caching and resuming processes"
        }, 
        {
            "location": "/caching/#caching-and-resuming-processes", 
            "text": "", 
            "title": "Caching and resuming processes"
        }, 
        {
            "location": "/caching/#process-caching", 
            "text": "Once a job is cached,  PyPPL  will skip running this job. But you have to tell a process how to cache its jobs by setting  pXXX.cache  with a valid caching method:     Caching method ( p.cache=? )  How      True  A signature *  of input files, script and output files of a job is cached in  workdir / job.index /job.cache , compare the signature before a job starts to run.    False  Disable caching, always run jobs.    \"export\"  First try to find the signatures, if failed, try to restore the files existed (or exported previously in  p.exdir ).      Hint :  p.cache = \"export\"  is extremely useful for a process that you only want it to run successfully once, export the result files and never run the process again. You can even delete the  workdir  of the process, but  PyPPL  will find the exported files and use them as the input for processes depending on it, so that you don't need to modify the pipeline. \nOne scenario is that you can use it to download some files and never need to download them again.", 
            "title": "Process caching"
        }, 
        {
            "location": "/caching/#resuming-from-processes", 
            "text": "Sometimes, you may not want to start at the very begining of a pipeline. Then you can resume it from some intermediate processes. \nTo resume pipeline from a process, you have to make sure that the output files of the processes that this process depends on are already generated. Then you can do: PyPPL () . start ( ... ) . resume ( pXXX ) . run ()  \nOr if the process uses the data from other processes, especially the output channel, you may need  PyPPL  to infer (not neccessary run the script) the output data for processes that this process depends on. Then you can do: PyPPL () . start ( ... ) . resume2 ( pXXX ) . run ()  \nYou may also use a common id to set a set of processes: p1   =   Proc ( newid   =   p ,   tag   =   1st )  p2   =   Proc ( newid   =   p ,   tag   =   2nd )  p3   =   Proc ( newid   =   p ,   tag   =   3rd )  # pipeline will be resumed from p1, p2, p3  PyPPL () . start ( ... ) . resume ( p ) . run ()", 
            "title": "Resuming from processes"
        }, 
        {
            "location": "/caching/#calculating-signatures-for-caching", 
            "text": "By default,  PyPPL  uses the last modified time to generate signatures for files and directories. However, for large directories, it may take notably long time to walk over all the files in those directories. If not necessary, you may simply as  PyPPL  to get the last modified time for the directories themselves instead of the infiles inside them by setting  p.dirsig = False", 
            "title": "Calculating signatures for caching"
        }, 
        {
            "location": "/runners/", 
            "text": "Runners and running profiles\n\n\nRunning profile\n\n\nA running profile defines the parameters that needed for a pipeline to run. Generally it contains the runner, the parameters for the runner and the common settings for the processes.\nA typical running profile is as follows:\n\n{\n\n    \nrunner\n:\n \nsge\n,\n\n    \nsgeRunner\n:\n \n{\n\n        \nqueue\n:\n \n1-day\n\n    \n},\n\n    \nforks\n:\n \n32\n\n\n}\n\n\n\n\n\n\nCaution\n\n\nYou may also put other settings of processes into a running profile, but keep in mind:\n1. The value will not be overridden if the attribute is set explicitly (i.e: \np.forks = 10\n)\n2. Only set common attributes for all processes in a pipeline to avoid unexprected behavior. For example, you probably don't want this in general cases to set the same script for all processes: \n\n{\n\n    \nscript\n:\n \nfile:/path/to/script\n\n\n}\n\n\n\n\n\n\nDefining running profiles\n\n\nYou may pre-define some profiles so that you can easily swith them by:\n\nPyPPL\n()\n.\nstart\n(\npXXX\n)\n.\nrun\n(\nprofile1\n)\n\n\nPyPPL\n()\n.\nstart\n(\npXXX\n)\n.\nrun\n(\nprofile2\n)\n\n\n\nYou can define profiles in \nPyPPL\n's default configuration files: \n$HOME/.PyPPL.yaml\n, \n$HOME/.PyPPL\n and/or \n$HOME/.PyPPL.json\n. The latter ones have high priorities. \n$HOME/.PyPPL\n should also be in \nJSON\n format. Take \n$HOME/.PyPPL.yaml\n (requiring \npyyaml\n) for example, the content is like:\n\ndefault\n:\n \n    \nrunner\n:\n \nlocal\n\n    \nforks\n:\n \n1\n\n    \necho\n:\n \nstderr\n\n\nprofile1\n:\n\n    \nrunner\n:\n \nsge\n\n    \nsgeRunner\n:\n\n        \nqueue\n:\n \n1-day\n\n\nprofile2\n:\n\n    \nrunner\n:\n \nsge\n\n    \nsgeRunner\n:\n\n        \nqueue\n:\n \n7-days\n\n\n\n\n\n\nNote\n\n\nIf a \nkey\n is not in a profile, then it will be inherited from \ndefault\n.\n\n\n\n\nYou may also define some profiles in a file somewhere else, say \n/path/to/myprofiles.yaml\n. Just pass the file to \nPyPPL\n constructor:\n\nPyPPL\n(\ncfgfile\n \n=\n \n/path/to/myprofiles.yaml\n)\n.\nstart\n(\npXXX\n)\n.\nrun\n(\nprofile1\n)\n\n\n\n\n\n\nNote\n\n\nThis has higher priority than default configuration files.\n\n\n\n\nYou can also pass the profiles to \nPyPPL\n constructor directly:\n\nPyPPL\n({\n\n    \ndefault\n:\n \n{\n\n        \nrunner\n:\n \nlocal\n,\n\n        \nforks\n:\n \n1\n,\n\n        \necho\n:\n \nstderr\n\n    \n},\n\n    \nprofile1\n:\n \n{\n\n        \nrunner\n:\n \nsge\n,\n\n        \nsgeRunner\n:\n \n{\n\n            \nqueue\n:\n \n1-day\n\n        \n}\n\n    \n},\n\n    \nprofile2\n:\n \n{\n\n        \nrunner\n:\n \nsge\n,\n\n        \nsgeRunner\n:\n \n{\n\n            \nqueue\n:\n \n7-days\n\n        \n}\n\n    \n}\n\n\n})\n.\nstart\n(\npXXX\n)\n.\nrun\n(\nprofile1\n)\n\n\n\n\n\n\nNote\n\n\nIn this way, the profiles have higher priorities than the ones defined in configuration files.\n\n\n\n\nOr even, you can also specify a profile to \nrun\n function to ask the pipeline run with the profile directly:\n\nPyPPL\n()\n.\nstart\n(\npXXX\n)\n.\nrun\n({\n\n    \nrunner\n:\n \nsge\n,\n\n    \nsgeRunner\n:\n \n{\n\n        \nqueue\n:\n \n1-day\n\n    \n}\n\n\n})\n\n\n\n\n\n\nNote\n\n\nThis has the highest priority.\n\n\n\n\nBuilt-in runners\n\n\nWe have 5 built-in runners (\nRunnerLocal\n, \nRunnerSsh\n, \nRunnerSge\n, \nRunnerSlurm\n, \nrunnerDry\n), you can also define you own runners.\n\n\nYou can either tell one process to use a runner, or even, you can tell the pipeline to use one runner for all the processes. That means each process can have the same runner or a different one. To tell a process which runner to use, just specify the runner name to \npXXX.runner\n (for example, \npXXX.runner = \"sge\"\n to use the sge runner). Each process may use different configuration for the runner (\npXXX.sgeRunner\n) or the same one by \nconfiguring the pipeline\n.\n\n\nConfigurations for ssh runner\n\n\nSsh runner takes the advantage to use the computing resources from other servers that can be connected via \nssh\n. The \nssh\n command allows us to pass the command to the server and execute it: \nssh [options] [command]\n\n\n\n\nCaution\n\n\n\n\nssh runner only works when the servers share the same file system.\n\n\nyou have to \nconfigure\n so that you don't need a password to log onto the servers, or use a private key to connect to the ssh servers.\n\n\nThe jobs will be distributed equally to the servers.\n\n\n\n\n\n\nTo tell a process the available ssh servers:\n\npXXX\n.\nsshRunner\n \n=\n \n{\n\n    \nservers\n:\n \n[\nserver1\n,\n \nserver2\n,\n \n...\n],\n \n    \nkeys\n:\n \n[\n/path/to/keyfile1\n,\n \n/path/to/keyfile2\n,\n \n...\n]\n\n\n}\n\n\n\n\nYou can have complicated ssh configurations which can be set by the system ssh config subsystem:\n\n\n$HOME/.ssh/config\n:\n\n# contents of $HOME/.ssh/config\nHost dev\n    HostName dev.example.com\n    Port 22000\n    User fooey\n\n\n\nIf you use different usernames to log on the servers, you may also specify the usernames as well:\n\npXXX\n.\nsshRunner\n \n=\n \n{\nservers\n:\n \n[\nuser1@server1\n,\n \nuser2@server2\n,\n \n...\n]}\n\n\n\n\nYou can also add \npreScript\n and \npostScript\n for all jobs:\n\npXXX\n.\nsshRunner\n \n=\n \n{\n\n    \nservers\n:[\n...\n],\n \n    \npreScript\n:\n \nmkdir some/dir/to/be/made\n,\n \n    \npostScript\n:\n \nrm -rf /path/to/job/tmp/dir\n\n\n}\n\n\n\n\nTo make a running profile with it for a pipeline for all processes:\n\nPyPPL\n \n({\n\n    \n# default profile\n\n    \ndefault\n:\n \n{\n\n        \nsshRunner\n:\n \n{\nservers\n:\n \n[\nuser1@server1\n,\n \nuser2@server2\n,\n \n...\n]}\n\n    \n},\n\n    \nssh3\n:\n \n{\n\n        \nrunner\n:\n \nssh\n,\n\n        \nsshRunner\n:\n \n{\n\n            \nservers\n:\n \n[\nserver1\n,\n \nserver2\n,\n \nserver3\n],\n\n            \nkeys\n:\n \n[\n/path/to/key1\n,\n \n/path/to/key2\n,\n \n/path/to/key3\n]\n\n        \n}\n\n    \n}\n\n\n})\n\n\n\nAlso see \"\npipeline configration\n\" for more details.\n\n\nThe constructor of the runner will change the actual script to run the following (\nworkdir\n/0/job.script.ssh\n):\n\n\n#!/usr/bin/env bash\n\n\ntrap\n \nstatus=\\$?; echo \\$status \n \nworkdir\n/scripts/script.0.rc; exit \\$status\n \n1\n \n2\n \n3\n \n6\n \n7\n \n8\n \n9\n \n10\n \n11\n \n12\n \n15\n \n16\n \n17\n EXIT\nssh -i \n/path/to/key1\n user1@server1 \ncd \ncwd\n; \nworkdir\n/0/job.script\n\n\n\n\n\ntrap\n command makes sure a return code file will be generated. \n\n\nConfigurations for sge runner\n\n\nSimilarly, you can also submit your jobs to SGE servers using \nqsub\n. To set the options for a process:\n\npXXX\n.\nsgeRunner\n \n=\n \n{\n\n    \nsge.q\n \n:\n \n1-day\n,\n          \n# the queue\n\n    \nsge.M\n \n:\n \nuser@domain.com\n,\n# The email for notification\n\n    \nsge.l\n \n:\n \nh_vmem=4G\n,\n        \n    \nsge.l \n:\n \nh_stack=512M\n,\n   \n# Remember to add an extra space \n\n                                \n# so that it won\nt override the previous \nsge.l\n\n    \nsge.m\n \n:\n \nabe\n,\n            \n# When to notify\n\n    \nsge.notify\n:\n \nTrue\n,\n\n    \npreScript\n:\n  \nsource /home/user/.bash_profile \n/dev/null; mkdir /tmp/my\n,\n  \n# load the environment and create the temporary directory\n\n    \npostScript\n:\n \nrm -rf /tmp/my\n \n# clean up\n\n\n}\n\n\n\nPlease check \nman qsub\n to find other options. Remember to add a \nsge.\n prefix to the option name.\n\n\nTo make a running profile with it for a pipeline for all processes:\n\nPyPPL\n({\n\n    \nproc\n:\n \n{\n\n        \nsgeRunner\n:\n \n{\n\n            \n#...\n\n        \n}\n\n    \n}\n\n\n})\n\n\n\nAlso see \"\npipeline configuration\n\" for more details.\n\n\nThe constructor of the runner will change the script to run to the following (\nworkdir\n/0/job.script.sge\n):\n\n#!/usr/bin/env bash\n\n\n#$ -N \nid\n.\ntag\n.0\n\n\n#$ -q 1-day\n\n\n#$ -o \nworkdir\n/PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stdout\n\n\n#$ -e \nworkdir\n/PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stderr\n\n\n#$ -cwd\n\n\n#$ -M wang.panwen@mayo.edu\n\n\n#$ -m abe\n\n\n#$ -l h_vmem=4G\n\n\n#$ -l h_stack=512M\n\n\n#$ -notify\n\n\n\ntrap\n \nstatus=\\$?; echo \\$status \n \nworkdir\n/0/job.rc; exit \\$status\n \n1\n \n2\n \n3\n \n6\n \n7\n \n8\n \n9\n \n10\n \n11\n \n12\n \n15\n \n16\n \n17\n EXIT\n\nsource\n /home/whoever/.bash_profile \n/dev/null\n;\n mkdir /tmp/my\n\n\nworkdir\n/0/job.script\nrm -rf /tmp/my\n\n\n\nConfigurations for slurm runner\n\n\nWhere to configure it:\n\nFor single process:\n\npXXX\n.\nslurmRunner\n \n=\n \n{\n...\n}\n\n\n\nFor running profiles:\n\nconfig\n \n=\n \n{\n\n  \nproc\n:\n \n{\n\n    \n...\n \n# other configurations\n\n    \nrunner\n:\n \nslurm\n,\n \n# all processes run with slurm\n\n    \nslurmRunner\n:\n \n{\n\n       \n...\n\n    \n}\n\n  \n},\n \n# or you can also create a profile\n\n  \nrunWithSlurm\n:\n \n{\n\n    \n...\n \n# other configurations\n\n    \nrunner\n:\n \nslurm\n,\n \n    \nslurmRunner\n:\n \n{\n\n       \n...\n\n    \n}\n\n  \n}\n\n\n}\n\n\nPyPPL\n(\nconfig\n)\n.\nstart\n(\n...\n)\n.\nrun\n()\n \n# uses configurations of \nproc\n\n\n# for profile:\n\n\n# PyPPL(config).start(...).run(\nrunWithSlurm\n)\n\n\n\n\nThe full configuration:\n\n\nslurmRunner\n:\n \n{\n\n  \npreScript\n:\n \nexport PATH=$PATH:/path/to/add\n,\n \n// default: \n\n  \npostScript\n:\n \n# some cleanup\n,\n                \n// default: \n\n  \n// commands (some slurm systems have variants of commands)\n\n  \nsbatch\n:\n \nyhbatch\n,\n                           \n// default: sbatch\n\n  \nsrun\n:\n \nyhrun\n,\n                               \n// default: srun\n\n  \nsqueue\n:\n \nyhqueue\n,\n                           \n// default: squeue\n\n  \n// the prefix add to command you want to run\n\n  \n// i.e \nsrun -n8 hostname\n\n  \n// it defaults to the command you specified to slurmRunner[\nsrun\n]\n\n  \n// In this case: \nyhrun\n\n  \ncmdPrefix\n:\n \nsrun -n8\n,\n                       \n// default: slurmRunner[\nsrun\n]\n\n  \n// sbatch options (with prefix \nslurm.\n):\n\n  \nslurm.p\n:\n \nnormal\n,\n\n  \nslurm.mem\n:\n \n1GB\n,\n\n  \n// other options\n\n  \n// ......\n\n  \n// Note that job name (slurm.J), stdout (slurm.o), stderr file (slurm.e) is calculated by the runner.\n\n  \n// Although you can, you are not recommended to set them here.\n\n\n}\n\n\n\n\nDry-run a pipeline\n\n\nYou can use dry runner to dry-run a pipeline. The real script will not be running, instead, it just tries to touch the output files and create the output directories.\n\n\n\n\nWhen \nRunnerDry\n is being used\n\n\n\n\nAll processes are running on local machine.\n\n\nExpectations won't be checked.\n\n\nProcesses won't be cached.\n\n\nOutput files/directories won't be exported.\n\n\nBetter set runner of all processes in a pipeline to \ndry\n. (\npyppl().starts(...).run('dry')\n), since empty file/directory will be created for output. Problems will happen if you have a non-dry-run process depending on dry-run processes.\n\n\n\n\n\n\nDefine your own runner\n\n\nYou are also able to define your own runner, which should be a class extends \nRunner\n (jobs run immediately after submission) or \nRunnerQueue\n (jobs are put into a queue after submission). There are several methods and variables you may need to redefine (You may check the \nAPI documentation\n for all available methods and variables).\n\n\nThe class name \nMUST\n start with \nRunner\n and end with the runner name with first letter capitalized. For example, to define the runner \nmy\n:\n\nfrom\n \npyppl.runners\n \nimport\n \nRunner\n\n\nclass\n \nRunnerMy\n \n(\nRunner\n):\n\n\t\npass\n\n\n\n\nThe base class \nRunner\n defines the runners where the jobs will immediately run after submission; while \nRunnerQueue\n defines the runners where the jobs will be put into a queue and wait for its turn to run (for example, clusters).\n\n\nSome important method to be redefined:\n\n\n\n\nChecklist (What you have to do in the constructor redefinition)\n\n\n\n\nchoose the right base class (\npyppl.runners.Runner\n or \npyppl.runners.RunnerQueue\n)\n\n\nsuper(RunnerMy, self).__init__(job)\n\n\nsetup the right \nself.script\n for submission.\n\n\nMAKE SURE you save the identity of the job to \njob.pidfile\n, rc to \njob.rcfile\n, stdout to \njob.outfile\n and \nstderr\n to \njob.errfile\n\n\n\n\n\n\n\n\n\n\nGet the job identity on the system: \ngetpid()\n\n  Sometimes you cannot determin the job identity (e.g. \npid\n for local jobs) when you are composing the script file. For example, for \nSGE\n runner, only after you submit the job, the job id will be saved in \njob.pidfile\n. In this case, you have to parse the job identity from \njob.outfile\n. Then you may save it by \nself.job.pid(\njobid\n)\n.\n\n  The purpose to save the job identity is to tell whether the job is already running before we submit the job. So you can ignore this, but the same job may be submitted twice. \n  Also see \nisRunning\n below.\n\n\n\n\n\n\nTell whether a job is still running: \nisRunning(self)\n\n    This function is used to detect whether a job is running. \n    Basically, it uses the job id got by \ngetpid()\n to tell whether the job is still running.\n    \nThis function is specially useful when you try to run the pipeline again if some of the jobs are still running but the main thread (pipeline) quite unintentionally.\n\n    But it's optional, you can make the function always return \nFalse\n. Then the jobs are anyway to be submitted. In this case, \ngetpid\n redefinition is not needed.\n\n\n\n\n\n\nHow many jobs to submit at one time (static variable): \nmaxsubmit\n (WILL BE DEPRECATED!)\n    This variable defines how many jobs to submit at one time. It defaults to \nmultiprocessing.cpu_count()/2\n if you don't have the value for your runner, which means it will use half of the cpus to submit the jobs you want to run simultaneously at one time. Then wait for sometime (\ninterval\n, see below), and submit another batch. The purpose is to avoid local machine to get stuck if you have too many jobs to submit.\n\n\n\n\n\n\nKey points in writing your own runner\n:\n\n\n\n\nChoose the right base class (\npyppl.runners.Runner\n or \npyppl.runners.RunnerQueue\n) (required).\n\n\nCompose the right script to submit the job (\nself.script\n) in \n__init__\n(required).\n\n\nUse \ngetpid\n to get the job id (optional).\n\n\nTell \nPyPPL\n how to judge when the jobs are still running (\nself.isRunning()\n) (optional). \n\n\nMAKE SURE you save the identity of the job to \njob.pidfile\n, rc to \njob.rcfile\n, stdout to \njob.outfile\n and \nstderr\n to \njob.errfile\n\n\n\n\nRegister your runner\n\n\nIt very easy to register your runner, just do \nPyPPL.registerRunner (RunnerMy)\n (static method) before you start to run the pipeline.\nThe 5 built-in runners have already been registered: \n\nPyPPL\n.\nregisterRunner\n \n(\nRunnerLocal\n)\n\n\nPyPPL\n.\nregisterRunner\n \n(\nRunnerSsh\n)\n\n\nPyPPL\n.\nregisterRunner\n \n(\nRunnerSge\n)\n\n\nPyPPL\n.\nregisterRunner\n \n(\nRunnerSlurm\n)\n\n\nPyPPL\n.\nregisterRunner\n \n(\nRunnerDry\n)\n\n\n\nTo register yours:\n\nPyPPL\n.\nregisterRunner\n(\nRunnerMy\n)\n\n\n\nAfter registration, you are able to ask a process to use it: \npXXX.runner = \"my\"", 
            "title": "Runners and running profiles"
        }, 
        {
            "location": "/runners/#runners-and-running-profiles", 
            "text": "", 
            "title": "Runners and running profiles"
        }, 
        {
            "location": "/runners/#running-profile", 
            "text": "A running profile defines the parameters that needed for a pipeline to run. Generally it contains the runner, the parameters for the runner and the common settings for the processes.\nA typical running profile is as follows: { \n     runner :   sge , \n     sgeRunner :   { \n         queue :   1-day \n     }, \n     forks :   32  }    Caution  You may also put other settings of processes into a running profile, but keep in mind:\n1. The value will not be overridden if the attribute is set explicitly (i.e:  p.forks = 10 )\n2. Only set common attributes for all processes in a pipeline to avoid unexprected behavior. For example, you probably don't want this in general cases to set the same script for all processes:  { \n     script :   file:/path/to/script  }", 
            "title": "Running profile"
        }, 
        {
            "location": "/runners/#defining-running-profiles", 
            "text": "You may pre-define some profiles so that you can easily swith them by: PyPPL () . start ( pXXX ) . run ( profile1 )  PyPPL () . start ( pXXX ) . run ( profile2 )  \nYou can define profiles in  PyPPL 's default configuration files:  $HOME/.PyPPL.yaml ,  $HOME/.PyPPL  and/or  $HOME/.PyPPL.json . The latter ones have high priorities.  $HOME/.PyPPL  should also be in  JSON  format. Take  $HOME/.PyPPL.yaml  (requiring  pyyaml ) for example, the content is like: default :  \n     runner :   local \n     forks :   1 \n     echo :   stderr  profile1 : \n     runner :   sge \n     sgeRunner : \n         queue :   1-day  profile2 : \n     runner :   sge \n     sgeRunner : \n         queue :   7-days    Note  If a  key  is not in a profile, then it will be inherited from  default .   You may also define some profiles in a file somewhere else, say  /path/to/myprofiles.yaml . Just pass the file to  PyPPL  constructor: PyPPL ( cfgfile   =   /path/to/myprofiles.yaml ) . start ( pXXX ) . run ( profile1 )    Note  This has higher priority than default configuration files.   You can also pass the profiles to  PyPPL  constructor directly: PyPPL ({ \n     default :   { \n         runner :   local , \n         forks :   1 , \n         echo :   stderr \n     }, \n     profile1 :   { \n         runner :   sge , \n         sgeRunner :   { \n             queue :   1-day \n         } \n     }, \n     profile2 :   { \n         runner :   sge , \n         sgeRunner :   { \n             queue :   7-days \n         } \n     }  }) . start ( pXXX ) . run ( profile1 )    Note  In this way, the profiles have higher priorities than the ones defined in configuration files.   Or even, you can also specify a profile to  run  function to ask the pipeline run with the profile directly: PyPPL () . start ( pXXX ) . run ({ \n     runner :   sge , \n     sgeRunner :   { \n         queue :   1-day \n     }  })    Note  This has the highest priority.", 
            "title": "Defining running profiles"
        }, 
        {
            "location": "/runners/#built-in-runners", 
            "text": "We have 5 built-in runners ( RunnerLocal ,  RunnerSsh ,  RunnerSge ,  RunnerSlurm ,  runnerDry ), you can also define you own runners.  You can either tell one process to use a runner, or even, you can tell the pipeline to use one runner for all the processes. That means each process can have the same runner or a different one. To tell a process which runner to use, just specify the runner name to  pXXX.runner  (for example,  pXXX.runner = \"sge\"  to use the sge runner). Each process may use different configuration for the runner ( pXXX.sgeRunner ) or the same one by  configuring the pipeline .", 
            "title": "Built-in runners"
        }, 
        {
            "location": "/runners/#configurations-for-ssh-runner", 
            "text": "Ssh runner takes the advantage to use the computing resources from other servers that can be connected via  ssh . The  ssh  command allows us to pass the command to the server and execute it:  ssh [options] [command]   Caution   ssh runner only works when the servers share the same file system.  you have to  configure  so that you don't need a password to log onto the servers, or use a private key to connect to the ssh servers.  The jobs will be distributed equally to the servers.    To tell a process the available ssh servers: pXXX . sshRunner   =   { \n     servers :   [ server1 ,   server2 ,   ... ],  \n     keys :   [ /path/to/keyfile1 ,   /path/to/keyfile2 ,   ... ]  }   You can have complicated ssh configurations which can be set by the system ssh config subsystem:  $HOME/.ssh/config : # contents of $HOME/.ssh/config\nHost dev\n    HostName dev.example.com\n    Port 22000\n    User fooey  If you use different usernames to log on the servers, you may also specify the usernames as well: pXXX . sshRunner   =   { servers :   [ user1@server1 ,   user2@server2 ,   ... ]}   You can also add  preScript  and  postScript  for all jobs: pXXX . sshRunner   =   { \n     servers :[ ... ],  \n     preScript :   mkdir some/dir/to/be/made ,  \n     postScript :   rm -rf /path/to/job/tmp/dir  }   To make a running profile with it for a pipeline for all processes: PyPPL   ({ \n     # default profile \n     default :   { \n         sshRunner :   { servers :   [ user1@server1 ,   user2@server2 ,   ... ]} \n     }, \n     ssh3 :   { \n         runner :   ssh , \n         sshRunner :   { \n             servers :   [ server1 ,   server2 ,   server3 ], \n             keys :   [ /path/to/key1 ,   /path/to/key2 ,   /path/to/key3 ] \n         } \n     }  })  \nAlso see \" pipeline configration \" for more details.  The constructor of the runner will change the actual script to run the following ( workdir /0/job.script.ssh ):  #!/usr/bin/env bash  trap   status=\\$?; echo \\$status    workdir /scripts/script.0.rc; exit \\$status   1   2   3   6   7   8   9   10   11   12   15   16   17  EXIT\nssh -i  /path/to/key1  user1@server1  cd  cwd ;  workdir /0/job.script   trap  command makes sure a return code file will be generated.", 
            "title": "Configurations for ssh runner"
        }, 
        {
            "location": "/runners/#configurations-for-sge-runner", 
            "text": "Similarly, you can also submit your jobs to SGE servers using  qsub . To set the options for a process: pXXX . sgeRunner   =   { \n     sge.q   :   1-day ,            # the queue \n     sge.M   :   user@domain.com , # The email for notification \n     sge.l   :   h_vmem=4G ,         \n     sge.l  :   h_stack=512M ,     # Remember to add an extra space  \n                                 # so that it won t override the previous  sge.l \n     sge.m   :   abe ,              # When to notify \n     sge.notify :   True , \n     preScript :    source /home/user/.bash_profile  /dev/null; mkdir /tmp/my ,    # load the environment and create the temporary directory \n     postScript :   rm -rf /tmp/my   # clean up  }  \nPlease check  man qsub  to find other options. Remember to add a  sge.  prefix to the option name.  To make a running profile with it for a pipeline for all processes: PyPPL ({ \n     proc :   { \n         sgeRunner :   { \n             #... \n         } \n     }  })  \nAlso see \" pipeline configuration \" for more details.  The constructor of the runner will change the script to run to the following ( workdir /0/job.script.sge ): #!/usr/bin/env bash  #$ -N  id . tag .0  #$ -q 1-day  #$ -o  workdir /PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stdout  #$ -e  workdir /PyPPL.pMuTect2.nothread.51SoVk6Y/0/job.stderr  #$ -cwd  #$ -M wang.panwen@mayo.edu  #$ -m abe  #$ -l h_vmem=4G  #$ -l h_stack=512M  #$ -notify  trap   status=\\$?; echo \\$status    workdir /0/job.rc; exit \\$status   1   2   3   6   7   8   9   10   11   12   15   16   17  EXIT source  /home/whoever/.bash_profile  /dev/null ;  mkdir /tmp/my workdir /0/job.script\nrm -rf /tmp/my", 
            "title": "Configurations for sge runner"
        }, 
        {
            "location": "/runners/#configurations-for-slurm-runner", 
            "text": "Where to configure it: \nFor single process: pXXX . slurmRunner   =   { ... }  \nFor running profiles: config   =   { \n   proc :   { \n     ...   # other configurations \n     runner :   slurm ,   # all processes run with slurm \n     slurmRunner :   { \n        ... \n     } \n   },   # or you can also create a profile \n   runWithSlurm :   { \n     ...   # other configurations \n     runner :   slurm ,  \n     slurmRunner :   { \n        ... \n     } \n   }  }  PyPPL ( config ) . start ( ... ) . run ()   # uses configurations of  proc  # for profile:  # PyPPL(config).start(...).run( runWithSlurm )   The full configuration:  slurmRunner :   { \n   preScript :   export PATH=$PATH:/path/to/add ,   // default:  \n   postScript :   # some cleanup ,                  // default:  \n   // commands (some slurm systems have variants of commands) \n   sbatch :   yhbatch ,                             // default: sbatch \n   srun :   yhrun ,                                 // default: srun \n   squeue :   yhqueue ,                             // default: squeue \n   // the prefix add to command you want to run \n   // i.e  srun -n8 hostname \n   // it defaults to the command you specified to slurmRunner[ srun ] \n   // In this case:  yhrun \n   cmdPrefix :   srun -n8 ,                         // default: slurmRunner[ srun ] \n   // sbatch options (with prefix  slurm. ): \n   slurm.p :   normal , \n   slurm.mem :   1GB , \n   // other options \n   // ...... \n   // Note that job name (slurm.J), stdout (slurm.o), stderr file (slurm.e) is calculated by the runner. \n   // Although you can, you are not recommended to set them here.  }", 
            "title": "Configurations for slurm runner"
        }, 
        {
            "location": "/runners/#dry-run-a-pipeline", 
            "text": "You can use dry runner to dry-run a pipeline. The real script will not be running, instead, it just tries to touch the output files and create the output directories.   When  RunnerDry  is being used   All processes are running on local machine.  Expectations won't be checked.  Processes won't be cached.  Output files/directories won't be exported.  Better set runner of all processes in a pipeline to  dry . ( pyppl().starts(...).run('dry') ), since empty file/directory will be created for output. Problems will happen if you have a non-dry-run process depending on dry-run processes.", 
            "title": "Dry-run a pipeline"
        }, 
        {
            "location": "/runners/#define-your-own-runner", 
            "text": "You are also able to define your own runner, which should be a class extends  Runner  (jobs run immediately after submission) or  RunnerQueue  (jobs are put into a queue after submission). There are several methods and variables you may need to redefine (You may check the  API documentation  for all available methods and variables).  The class name  MUST  start with  Runner  and end with the runner name with first letter capitalized. For example, to define the runner  my : from   pyppl.runners   import   Runner  class   RunnerMy   ( Runner ): \n\t pass   The base class  Runner  defines the runners where the jobs will immediately run after submission; while  RunnerQueue  defines the runners where the jobs will be put into a queue and wait for its turn to run (for example, clusters).  Some important method to be redefined:   Checklist (What you have to do in the constructor redefinition)   choose the right base class ( pyppl.runners.Runner  or  pyppl.runners.RunnerQueue )  super(RunnerMy, self).__init__(job)  setup the right  self.script  for submission.  MAKE SURE you save the identity of the job to  job.pidfile , rc to  job.rcfile , stdout to  job.outfile  and  stderr  to  job.errfile      Get the job identity on the system:  getpid() \n  Sometimes you cannot determin the job identity (e.g.  pid  for local jobs) when you are composing the script file. For example, for  SGE  runner, only after you submit the job, the job id will be saved in  job.pidfile . In this case, you have to parse the job identity from  job.outfile . Then you may save it by  self.job.pid( jobid ) . \n  The purpose to save the job identity is to tell whether the job is already running before we submit the job. So you can ignore this, but the same job may be submitted twice. \n  Also see  isRunning  below.    Tell whether a job is still running:  isRunning(self) \n    This function is used to detect whether a job is running. \n    Basically, it uses the job id got by  getpid()  to tell whether the job is still running.\n     This function is specially useful when you try to run the pipeline again if some of the jobs are still running but the main thread (pipeline) quite unintentionally. \n    But it's optional, you can make the function always return  False . Then the jobs are anyway to be submitted. In this case,  getpid  redefinition is not needed.    How many jobs to submit at one time (static variable):  maxsubmit  (WILL BE DEPRECATED!)\n    This variable defines how many jobs to submit at one time. It defaults to  multiprocessing.cpu_count()/2  if you don't have the value for your runner, which means it will use half of the cpus to submit the jobs you want to run simultaneously at one time. Then wait for sometime ( interval , see below), and submit another batch. The purpose is to avoid local machine to get stuck if you have too many jobs to submit.    Key points in writing your own runner :   Choose the right base class ( pyppl.runners.Runner  or  pyppl.runners.RunnerQueue ) (required).  Compose the right script to submit the job ( self.script ) in  __init__ (required).  Use  getpid  to get the job id (optional).  Tell  PyPPL  how to judge when the jobs are still running ( self.isRunning() ) (optional).   MAKE SURE you save the identity of the job to  job.pidfile , rc to  job.rcfile , stdout to  job.outfile  and  stderr  to  job.errfile", 
            "title": "Define your own runner"
        }, 
        {
            "location": "/runners/#register-your-runner", 
            "text": "It very easy to register your runner, just do  PyPPL.registerRunner (RunnerMy)  (static method) before you start to run the pipeline.\nThe 5 built-in runners have already been registered:  PyPPL . registerRunner   ( RunnerLocal )  PyPPL . registerRunner   ( RunnerSsh )  PyPPL . registerRunner   ( RunnerSge )  PyPPL . registerRunner   ( RunnerSlurm )  PyPPL . registerRunner   ( RunnerDry )  \nTo register yours: PyPPL . registerRunner ( RunnerMy )  \nAfter registration, you are able to ask a process to use it:  pXXX.runner = \"my\"", 
            "title": "Register your runner"
        }, 
        {
            "location": "/error-handling/", 
            "text": "Error handling of processes\n\n\nYou can ask \nPyPPL\n to terminate, retry or ignore a job that fails to run.\n\nWhen a job finishes, it should generate a \njob.rc\n file containing the return code. When compare with the valid return codes \npXXX.rc\n, the error triggered if it is not in \npXXX.rc\n. \npXXX.errhow\n determines what's next if errors happen. \n\n\n\n\n\"terminate\"\n: when errors happen, terminate the entire pipeline\n\n\n\"ignore\"\n: ignore the errors, continuing run the next process\n\n\n\"retry\"\n: re-submit and run the job again. \npXXX.errntry\n defines how many time to retry.\n\n\n\n\nSet expectations of a process results\n\n\nYou can use commands to check whether you have expected output. For example:\n\np\n \n=\n \nProc\n \n()\n\n\np\n.\ninput\n \n=\n \n{\ninput\n:\n \n1\n}\n\n\np\n.\nscript\n \n=\n \necho {{in.input}}\n\n\n# check the stdout\n\n\np\n.\nexpect\n \n=\n \ngrep 1 {{job.outfile}}", 
            "title": "Error handling of processes"
        }, 
        {
            "location": "/error-handling/#error-handling-of-processes", 
            "text": "You can ask  PyPPL  to terminate, retry or ignore a job that fails to run. \nWhen a job finishes, it should generate a  job.rc  file containing the return code. When compare with the valid return codes  pXXX.rc , the error triggered if it is not in  pXXX.rc .  pXXX.errhow  determines what's next if errors happen.    \"terminate\" : when errors happen, terminate the entire pipeline  \"ignore\" : ignore the errors, continuing run the next process  \"retry\" : re-submit and run the job again.  pXXX.errntry  defines how many time to retry.", 
            "title": "Error handling of processes"
        }, 
        {
            "location": "/error-handling/#set-expectations-of-a-process-results", 
            "text": "You can use commands to check whether you have expected output. For example: p   =   Proc   ()  p . input   =   { input :   1 }  p . script   =   echo {{in.input}}  # check the stdout  p . expect   =   grep 1 {{job.outfile}}", 
            "title": "Set expectations of a process results"
        }, 
        {
            "location": "/set-other-properties-of-a-process/", 
            "text": "Other attributes of a process\n\n\nCurrently we introduced in previous chapters a set of attributes of a process and we will introduce the rest of them in this chapter:\n\n\n\n\n\n\n\n\nAttribute\n\n\nMeaning\n\n\nPossibile values/types\n\n\nDefault value\n\n\nWhere it's first mentioned\n\n\n\n\n\n\n\n\n\n\nid\n\n\nThe id of the process\n\n\nstr\n\n\nthe variable name\n\n\nLink\n\n\n\n\n\n\ntag\n\n\nThe tag of the process, makes it possible to have two processes with the same \nid\n but different \ntag\n.\n\n\nstr\n\n\n\"notag\"\n\n\nLink\n\n\n\n\n\n\ndesc\n\n\nThe description of the process.\n\n\nstr\n\n\n\"No description\"\n\n\n\n\n\n\n\n\necho\n\n\nWhether to print out the \nstdout\n and \nstderr\n\n\nbool\n/\ndict\n\n\nFalse\n\n\nLink\n\n\n\n\n\n\ninput\n\n\nThe input of the process\n\n\ndict\n/\nlist\n/\nstr\n\n\n\n\nLink\n\n\n\n\n\n\noutput\n\n\nThe output of the process\n\n\nlist\n/\nstr\n/\nOrderedDict\n\n\n\n\nLink\n\n\n\n\n\n\nscript\n\n\nThe script of the process\n\n\nstr\n\n\n\n\nLink\n\n\n\n\n\n\nlang\n\n\nThe language for the script\n\n\nstr\n\n\n\"bash\"\n\n\nLink\n\n\n\n\n\n\nexdir\n\n\nThe export directory\n\n\nstr\n\n\n\n\nLink\n\n\n\n\n\n\nexhow\n\n\nHow to export\n\n\n\"move\"\n, \n\"copy\"\n, \n\"symlink\"\n, \n\"gzip\"\n\n\n\"move\"\n\n\nLink\n\n\n\n\n\n\nexow\n\n\nWhether to overwrite existing files when export\n\n\nbool\n\n\nTrue\n\n\nLink\n\n\n\n\n\n\ncache\n\n\nWhether to cache the process\n\n\nTrue\n, \nFalse\n, \n\"export\"\n\n\nTrue\n\n\nLink\n\n\n\n\n\n\nrunner\n\n\nWhich runner to use\n\n\nstr\n\n\n\"local\"\n\n\nLink\n\n\n\n\n\n\nppldir\n\n\nThe directory to store \nworkdir\ns\n for all processes in this pipeline\n\n\nstr\n\n\n\"./workdir\"\n\n\nLink\n\n\n\n\n\n\nworkdir\n\n\nThe work directory of the process\n\n\nstr\n\n\n\"\nid\n.\ntag\n.\nuid\n\"\n\n\nLink\n\n\n\n\n\n\nexpart\n\n\nPartial export\n\n\nstr\n/\nlist\n\n\n\n\nLink\n\n\n\n\n\n\nbrings\n\n\nDefinition of bring-in files\n\n\nstr\n/\nlist\n\n\n\n\nLink\n\n\n\n\n\n\ntemplate\n\n\nThe name of the template engine\n\n\nstr\n\n\nPyPPL\n\n\nLink\n\n\n\n\n\n\nenvs\n\n\nEnvironments for the template engine\n\n\ndict\n\n\n\n\nLink\n\n\n\n\n\n\ncclean\n\n\nWhether do cleanup (output checking/exporting) if a job was cached.\n\n\nbool\n\n\nFalse\n\n\nLink\n\n\n\n\n\n\ndirsig\n\n\nGet the modified time for directory recursively (taking into account the dirs and files in it) for cache checking\n\n\nbool\n\n\nTrue\n\n\nLink\n\n\n\n\n\n\nerrhow\n\n\nWhat's next if jobs fail\n\n\n\"terminate\"\n, \n\"retry\"\n, \n\"ignore\"\n\n\n\"terminate\"\n\n\nLink\n\n\n\n\n\n\nerrntry\n\n\nIf \nerrhow\n is \n\"retry\"\n, how many time to re-try?\n\n\nint\n\n\n3\n\n\nLink\n\n\n\n\n\n\nexpect\n\n\nA command to check whether expected results generated\n\n\nstr\n\n\n\n\nLink\n\n\n\n\n\n\nnthread\n\n\nNumber of theads used for job construction and submission\n\n\nint\n\n\nmin(int(cpu_count() / 2), 16)\n\n\n-\n\n\n\n\n\n\nargs\n\n\nThe arguments for the process\n\n\ndict\n\n\n{}\n\n\nThis chapter\n\n\n\n\n\n\nrc\n\n\nValid return codes\n\n\nstr\n/\nlist\n/\nint\n\n\n0\n\n\nThis chapter\n\n\n\n\n\n\nbeforeCmd\n\n\nThe command to run before jobs run\n\n\nstr\n\n\n\n\nThis chapter\n\n\n\n\n\n\nafterCmd\n\n\nThe command to run after jobs finish\n\n\nstr\n\n\n\n\nThis chapter\n\n\n\n\n\n\ndepends\n\n\nThe processes the process depends on\n\n\nproc\n/\nlist\n\n\n\n\nThis chapter\n\n\n\n\n\n\ncallback\n\n\nThe callback, called after the process finishes\n\n\ncallable\n\n\n\n\nThis chapter\n\n\n\n\n\n\ncallfront\n\n\nThe callfront, called after properties are computed\n\n\ncallable\n\n\n\n\nThis chapter\n\n\n\n\n\n\n\n\nSet arguments of a process \npXXX.args\n\n\nIt is a \ndict\n used to set some common arguments shared within the process (different jobs). For example, all jobs use the same program: \nbedtools\n. but to make the process portable and shareable, you may want others can give a different path of \nbedtools\n as well. Then you can use \npXXX.args\n:\n\npXXX\n \n=\n \nProc\n()\n\n\npXXX\n.\ninput\n \n=\n \n{\ninfile1:file, infile2:file\n:\n \n[(\nfile1.bed\n,\n \nfile2.bed\n)]}\n\n\npXXX\n.\noutput\n \n=\n \noutfile:file:{{in.infile1 | fn}}.out\n\n\npXXX\n.\nargs\n \n=\n \n{\nbedtools\n:\n \n/path/to/bedtools\n}\n\n\n# You can also do:\n\n\n# pXXX.args.bedtools = \n/path/to/bedtools\n\n\npXXX\n.\nscript\n \n=\n \n\n\n{{args.bedtools}} intersect -a {{in.infile1}} -b {{in.infile2}} \n {{out.outfile}}\n\n\n\n\n\nThat's \nNOT\n recommended that you put it in the input channel:\n\npXXX\n \n=\n \nproc\n()\n\n\npXXX\n.\ninput\n \n=\n \n{\ninfile1:file, infile2:file, bedtools\n:\n \n[(\nfile1.bed\n,\n \nfile2.bed\n,\n \n/path/to/bedtools\n)]}\n\n\npXXX\n.\noutput\n \n=\n \noutfile:file:{{infile.fn}}.out\n\n\npXXX\n.\nscript\n \n=\n \n\n\n{{bedtools}} intersect -a {{infile1}} -b {{infile2}} \n {{outfile}}\n\n\n\n\n\nOf course, you can do that, but a common argument is not usually generated from prior processes, then you have to modify the input channels. If the argument is a file, and you put it in \ninput\n with type \nfile\n, \nPyPPL\n will try to create a link in \nindir\n. If you have 100 jobs, we need to do that 100 times or to determine whether the link exists for 100 times. You may not want that to happen.  \n\n\n\n\nCaution\n\n\nNever use a key with dot \n.\n in \npXXX.args\n, since we use \n{{args.key}}\n to access it. \n\n\n\n\n\n\nHint\n\n\nPyPPL\n uses a built-in class \nBox\n to allow dot to be used to refer the attributes. So you can set the value of \nargs\n like this: \n\npXXX\n.\nargs\n.\nbedtools\n \n=\n \nbedtools\n\n\n\n\n\n\nSet the valid return/exit codes \npXXX.rc\n\n\nWhen a program exits, it will return a code (or \nexit status\n), usually a small integer to exhibit it's status. Generally if a program finishes successfully, it will return \n0\n, which is the default value of \np.rc\n. \npyppl\n relies on this return code to determine whether a job finishes successfully.  If not, \np.errorhow\n will be triggered. You can set multiple valid return codes for a process: \n\np\n.\nrc\n \n=\n \n[\n0\n,\n \n1\n]\n \n#or \n0,1\n\n\n# exit code with 0 or 1 will be both regarded as success\n\n\n\n\n\n\nCaution\n\n\nbeforeCmd\n/\nafterCmd\n only run locally, no matter which runner you choose to run the jobs.\n\n\n\n\nSet the processes current process depends on \npXXX.depends\n\n\nA process can not only depend on a single process: \n\np2\n.\ndepends\n \n=\n \np1\n\n\n\nbut also multiple processes \n\np2\n.\ndepends\n \n=\n \np1\n,\n \np0\n\n\n\nTo set prior processes not only let the process use the output channel as input for current process, but also determines when the process starts to run (right after the prior processes finish).\n\n\n\n\nCaution\n\n\nYou can copy a process by \np2 = p.copy()\n, but remember \ndepends\n will not be copied, you have to specify it for the copied processes.  \n\n\nWhen you specify new dependents for a process, its original ones will be removed, which means each time \npXXX.depends\n will overwrite the previous setting.\n\n\n\n\nUse callback to modify the process \npXXX.callback\n\n\nThe processes \nNOT\n initialized until it's ready to run. So you may not be able to modify some of the values until it is initialized. For example, you may want to change the output channel before it passes to the its dependent process:\n\npSingle\n \n=\n \nProc\n \n()\n\n\npSingle\n.\ninput\n    \n=\n \n{\ninfile:file\n:\n \n[\nfile1.txt\n,\n \nfile2.txt\n,\n \nfile3.txt\n]}\n\n\npSingle\n.\noutput\n   \n=\n \noutfile:file:{{in.infile | fn}}.sorted\n\n\npSingle\n.\nscript\n   \n=\n \n# Sort {{in. infile}} and save to {{out.outfile}}.sorted\n\n\n# pSingle.channel == [(\nfile1.sorted\n,), (\nfile2.sorted\n,), (\nfile3.sorted\n,)]\n\n\n# BUT NOT NOW!! the output channel is only generated after the process finishes\n\n\n\npCombine\n \n=\n \nProc\n \n()\n\n\npCombine\n.\ndepends\n \n=\n \npSingle\n\n\npCombine\n.\ninput\n   \n=\n \nindir:file\n   \n\n# the directory contains \nfile1.sorted\n, \nfile2.sorted\n, \nfile3.sorted\n\n\npCombine\n.\noutput\n  \n=\n \noutfile:{{in.indir | fn}}.combined\n\n\npCombine\n.\nscript\n  \n=\n \n# combine files to {{out.outfile}}.combined\n\n\n\n# To use the directory of \nfile1.sorted\n, \nfile2.sorted\n, \nfile3.sorted\n as the input channel for pCombine\n\n\n# You can use callback\n\n\ndef\n \npSingleCallback\n(\np\n):\n\n    \np\n.\nchannel\n \n=\n \np\n.\nchannel\n.\ncollapse\n()\n\n\npSingle\n.\ncallback\n \n=\n \npSingleCallback\n\n\n\nPyPPL\n()\n.\nstart\n \n(\npSingle\n)\n.\nrun\n()\n\n\n\nYou can also use a callback in \npCombine.input\n to modify the channel, see \nhere\n, which is recommended. Because \np.callback\n will change the original output channel of \npSingle\n, but the \ninput\n callback will keep the output channel intact. However, \np.callback\n can not only change the output channel, but also change other properties of current process or even set the properties of coming processes.\n\n\nUse callfront to modify the process \npXXX.callfront\n\n\nOne possible scenario is that, value in  \npXXX.args\n depends on the other process.\nFor example:\n\n# generate bam files\n\n\n# ...\n\n\npBam\n.\noutput\n \n=\n \nbamfile:file:{{in.infile | fn}}.bam\n\n\n\n# generate/download reference file, and index it\n\n\n# ...\n\n\npRef\n.\noutput\n \n=\n \nreffile:file:{{in.in}}.fa\n\n\n\n# call variance\n\n\n# ...\n\n\npCall\n.\ndepends\n   \n=\n \npBam\n,\n \npRef\n\n\npCall\n.\ncallfront\n \n=\n \nlambda\n \np\n:\n \np\n.\nargs\n.\nupdate\n({\nreffile\n:\n \npRef\n.\nchannel\n.\nreffile\n[\n0\n][\n0\n]})\n\n\n\n# pCall also depends on pRef, as it needs the reference file to run. \n\n\n# But you may not want to put it in input.\n\n\n\nPyPPL\n()\n.\nstart\n(\npBam\n,\n \npRef\n)\n.\nrun\n()", 
            "title": "Other attributes of a process"
        }, 
        {
            "location": "/set-other-properties-of-a-process/#other-attributes-of-a-process", 
            "text": "Currently we introduced in previous chapters a set of attributes of a process and we will introduce the rest of them in this chapter:     Attribute  Meaning  Possibile values/types  Default value  Where it's first mentioned      id  The id of the process  str  the variable name  Link    tag  The tag of the process, makes it possible to have two processes with the same  id  but different  tag .  str  \"notag\"  Link    desc  The description of the process.  str  \"No description\"     echo  Whether to print out the  stdout  and  stderr  bool / dict  False  Link    input  The input of the process  dict / list / str   Link    output  The output of the process  list / str / OrderedDict   Link    script  The script of the process  str   Link    lang  The language for the script  str  \"bash\"  Link    exdir  The export directory  str   Link    exhow  How to export  \"move\" ,  \"copy\" ,  \"symlink\" ,  \"gzip\"  \"move\"  Link    exow  Whether to overwrite existing files when export  bool  True  Link    cache  Whether to cache the process  True ,  False ,  \"export\"  True  Link    runner  Which runner to use  str  \"local\"  Link    ppldir  The directory to store  workdir s  for all processes in this pipeline  str  \"./workdir\"  Link    workdir  The work directory of the process  str  \" id . tag . uid \"  Link    expart  Partial export  str / list   Link    brings  Definition of bring-in files  str / list   Link    template  The name of the template engine  str  PyPPL  Link    envs  Environments for the template engine  dict   Link    cclean  Whether do cleanup (output checking/exporting) if a job was cached.  bool  False  Link    dirsig  Get the modified time for directory recursively (taking into account the dirs and files in it) for cache checking  bool  True  Link    errhow  What's next if jobs fail  \"terminate\" ,  \"retry\" ,  \"ignore\"  \"terminate\"  Link    errntry  If  errhow  is  \"retry\" , how many time to re-try?  int  3  Link    expect  A command to check whether expected results generated  str   Link    nthread  Number of theads used for job construction and submission  int  min(int(cpu_count() / 2), 16)  -    args  The arguments for the process  dict  {}  This chapter    rc  Valid return codes  str / list / int  0  This chapter    beforeCmd  The command to run before jobs run  str   This chapter    afterCmd  The command to run after jobs finish  str   This chapter    depends  The processes the process depends on  proc / list   This chapter    callback  The callback, called after the process finishes  callable   This chapter    callfront  The callfront, called after properties are computed  callable   This chapter", 
            "title": "Other attributes of a process"
        }, 
        {
            "location": "/set-other-properties-of-a-process/#set-arguments-of-a-process-pxxxargs", 
            "text": "It is a  dict  used to set some common arguments shared within the process (different jobs). For example, all jobs use the same program:  bedtools . but to make the process portable and shareable, you may want others can give a different path of  bedtools  as well. Then you can use  pXXX.args : pXXX   =   Proc ()  pXXX . input   =   { infile1:file, infile2:file :   [( file1.bed ,   file2.bed )]}  pXXX . output   =   outfile:file:{{in.infile1 | fn}}.out  pXXX . args   =   { bedtools :   /path/to/bedtools }  # You can also do:  # pXXX.args.bedtools =  /path/to/bedtools  pXXX . script   =    {{args.bedtools}} intersect -a {{in.infile1}} -b {{in.infile2}}   {{out.outfile}}   \nThat's  NOT  recommended that you put it in the input channel: pXXX   =   proc ()  pXXX . input   =   { infile1:file, infile2:file, bedtools :   [( file1.bed ,   file2.bed ,   /path/to/bedtools )]}  pXXX . output   =   outfile:file:{{infile.fn}}.out  pXXX . script   =    {{bedtools}} intersect -a {{infile1}} -b {{infile2}}   {{outfile}}   \nOf course, you can do that, but a common argument is not usually generated from prior processes, then you have to modify the input channels. If the argument is a file, and you put it in  input  with type  file ,  PyPPL  will try to create a link in  indir . If you have 100 jobs, we need to do that 100 times or to determine whether the link exists for 100 times. You may not want that to happen.     Caution  Never use a key with dot  .  in  pXXX.args , since we use  {{args.key}}  to access it.     Hint  PyPPL  uses a built-in class  Box  to allow dot to be used to refer the attributes. So you can set the value of  args  like this:  pXXX . args . bedtools   =   bedtools", 
            "title": "Set arguments of a process pXXX.args"
        }, 
        {
            "location": "/set-other-properties-of-a-process/#set-the-valid-returnexit-codes-pxxxrc", 
            "text": "When a program exits, it will return a code (or  exit status ), usually a small integer to exhibit it's status. Generally if a program finishes successfully, it will return  0 , which is the default value of  p.rc .  pyppl  relies on this return code to determine whether a job finishes successfully.  If not,  p.errorhow  will be triggered. You can set multiple valid return codes for a process:  p . rc   =   [ 0 ,   1 ]   #or  0,1  # exit code with 0 or 1 will be both regarded as success    Caution  beforeCmd / afterCmd  only run locally, no matter which runner you choose to run the jobs.", 
            "title": "Set the valid return/exit codes pXXX.rc"
        }, 
        {
            "location": "/set-other-properties-of-a-process/#set-the-processes-current-process-depends-on-pxxxdepends", 
            "text": "A process can not only depend on a single process:  p2 . depends   =   p1  \nbut also multiple processes  p2 . depends   =   p1 ,   p0  \nTo set prior processes not only let the process use the output channel as input for current process, but also determines when the process starts to run (right after the prior processes finish).   Caution  You can copy a process by  p2 = p.copy() , but remember  depends  will not be copied, you have to specify it for the copied processes.    When you specify new dependents for a process, its original ones will be removed, which means each time  pXXX.depends  will overwrite the previous setting.", 
            "title": "Set the processes current process depends on pXXX.depends"
        }, 
        {
            "location": "/set-other-properties-of-a-process/#use-callback-to-modify-the-process-pxxxcallback", 
            "text": "The processes  NOT  initialized until it's ready to run. So you may not be able to modify some of the values until it is initialized. For example, you may want to change the output channel before it passes to the its dependent process: pSingle   =   Proc   ()  pSingle . input      =   { infile:file :   [ file1.txt ,   file2.txt ,   file3.txt ]}  pSingle . output     =   outfile:file:{{in.infile | fn}}.sorted  pSingle . script     =   # Sort {{in. infile}} and save to {{out.outfile}}.sorted  # pSingle.channel == [( file1.sorted ,), ( file2.sorted ,), ( file3.sorted ,)]  # BUT NOT NOW!! the output channel is only generated after the process finishes  pCombine   =   Proc   ()  pCombine . depends   =   pSingle  pCombine . input     =   indir:file     # the directory contains  file1.sorted ,  file2.sorted ,  file3.sorted  pCombine . output    =   outfile:{{in.indir | fn}}.combined  pCombine . script    =   # combine files to {{out.outfile}}.combined  # To use the directory of  file1.sorted ,  file2.sorted ,  file3.sorted  as the input channel for pCombine  # You can use callback  def   pSingleCallback ( p ): \n     p . channel   =   p . channel . collapse ()  pSingle . callback   =   pSingleCallback  PyPPL () . start   ( pSingle ) . run ()  \nYou can also use a callback in  pCombine.input  to modify the channel, see  here , which is recommended. Because  p.callback  will change the original output channel of  pSingle , but the  input  callback will keep the output channel intact. However,  p.callback  can not only change the output channel, but also change other properties of current process or even set the properties of coming processes.", 
            "title": "Use callback to modify the process pXXX.callback"
        }, 
        {
            "location": "/set-other-properties-of-a-process/#use-callfront-to-modify-the-process-pxxxcallfront", 
            "text": "One possible scenario is that, value in   pXXX.args  depends on the other process.\nFor example: # generate bam files  # ...  pBam . output   =   bamfile:file:{{in.infile | fn}}.bam  # generate/download reference file, and index it  # ...  pRef . output   =   reffile:file:{{in.in}}.fa  # call variance  # ...  pCall . depends     =   pBam ,   pRef  pCall . callfront   =   lambda   p :   p . args . update ({ reffile :   pRef . channel . reffile [ 0 ][ 0 ]})  # pCall also depends on pRef, as it needs the reference file to run.   # But you may not want to put it in input.  PyPPL () . start ( pBam ,   pRef ) . run ()", 
            "title": "Use callfront to modify the process pXXX.callfront"
        }, 
        {
            "location": "/configure-your-logs/", 
            "text": "Configure your logs\n\n\nPyPPL\n has fancy logs. You can define how they look like (theme) and what messages to show (levels).\n\n\nBuilt-in log themes\n\n\nWe have some built-in themes:\n\n\ngreenOnBlack (default):\n\n\n\ngreenOnWhite:\n\n\nblueOnBlack:\n\n\n\nblueOnWhite:\n\n\n\nmagentaOnBlack:\n\n\n\nmagentaOnWhite:\n\n\n\nIf you don't like them, you can also disable them:\n\n\n\n\n\nTo use them, just specify the name in your pipeline configuration file:\n\n{\n\n    \n_log\n:\n \n{\n\n        \ntheme\n:\n \nmagentaOnWhite\n\n    \n}\n\n\n}\n\n\n\nOr when you initialize a pipeline:\n\nPyPPL\n({\n_log\n:\n \n{\ntheme\n:\n \nmagentaOnWhite\n}})\n.\nstart\n(\n...\n)\n.\nrun\n()\n\n\n\nIf you want to disable the theme, just set \n\"theme\"\n to \nFalse\n (\nfalse\n for \njson\n)\nIf you set \ntheme\n to \nTrue\n, then default theme \ngreenOnBlack\n is used.\n\n\nLevels of pyppl logs\n\n\nPlease note that the levels are different from those of python's \nlogging\n module. For \nlogging\n module has \n6 levels\n, with different int values. However, pyppl's log has many levels, or more suitable, flags, which don't have corresponding values. They are somehow equal, but some of them always print out unless you ask them not to.\n\n\n\n\nNote\n\n\nThe log levels are a little bit different from here, please see \ndebug your script\n.\n\n\n\n\nYou may also specify the group name in your pipeline configuration file:\n\n{\n\n    \n_log\n:\n \n{\n\n        \nlevels\n:\n \nnodebug\n\n    \n},\n\n    \n//\n \nrunning\n \nprofiles\n \n...\n\n\n}\n\n\n\nOr when you initialize a pipeline:\n\nPyPPL\n({\n_log\n:\n \n{\nlevels\n:\n \nnodebug\n}})\n.\nstart\n(\n...\n)\n.\nrun\n()\n\n\n\n\nYou can also explicitly define a set of messages with different levels to show in the logs:\n\n{\n\n    \n_log\n:\n \n{\nlevels\n:\n \n[\nPROCESS\n,\n \nRUNNING\n,\n \nCACHED\n]}\n\n\n}\n\n\n\n\nEven you can modify the base groups:\n\n{\n\n    \n_log\n:\n \n{\n\n        \nlevels\n:\n \nnormal\n,\n\n        \nlvldiff\n:\n \n[\n+DEBUG\n,\n \nP.ARGS\n,\n \n-SUBMIT\n]\n\n    \n}\n\n\n}\n\n\n\nThen the \nDEBUG\n, \nP.ARGS\n messages will show, and \nSUBMIT\n will hide.\n\n\nDefine your theme\n\n\nLet's see how the built-in theme looks like first:\nin \npyppl/logger.py\n:\n\nthemes\n \n=\n \n{\n\n  \ngreenOnBlack\n:\n \n{\n\n    \nPROCESS\n \n:\n \n[\ncolors\n.\nbold\n \n+\n \ncolors\n.\ncyan\n,\n \ncolors\n.\nbold\n \n+\n \ncolors\n.\nunderline\n \n+\n \ncolors\n.\ncyan\n],\n\n    \nDONE\n    \n:\n \ncolors\n.\nbold\n \n+\n \ncolors\n.\ngreen\n,\n\n    \nDEBUG\n   \n:\n \ncolors\n.\nbold\n \n+\n \ncolors\n.\nblack\n,\n\n    \nDEPENDS\n \n:\n \ncolors\n.\nmagenta\n,\n\n    \nPROCESS\n \n:\n \n[\ncolors\n.\nbold\n \n+\n \ncolors\n.\ncyan\n,\n \ncolors\n.\nbold\n \n+\n \ncolors\n.\nunderline\n \n+\n \ncolors\n.\ncyan\n],\n\n    \nin:SUBMIT,JOBDONE,INFO,P.PROPS,OUTPUT,EXPORT,INPUT,P.ARGS,BRINGS\n:\n \ncolors\n.\ngreen\n,\n\n    \nhas:ERR\n \n:\n \ncolors\n.\nred\n,\n\n    \nin:WARNING,RETRY\n \n:\n \ncolors\n.\nbold\n \n+\n \ncolors\n.\nyellow\n,\n\n    \nin:CACHED,RUNNING\n:\n \ncolors\n.\nyellow\n,\n\n    \n        \n:\n \ncolors\n.\nwhite\n\n  \n},\n\n  \n# other themes\n\n\n}\n\n\n\nFor the keys, you may either directly use the level name or have some prefix to define how to match the level names:\n- \nin:\n matches the messages with level name in the following list, which is separated by comma (\n,\n).\n- \nhas:\n matches the messages with level name containing the following string.\n- \nstarts:\n matches the messages with level name starting with the following string.\n- \nre\n: uses the following string as regular expression to match\n\nThen empty string key (\n''\n) defines the colors to use for the messages that can not match any of the above rules.\n\n\nFor the values, basically it's a 2-element list, where the first one defines the color to show the level name; and the second is the color to render the message. If only one color offered, it will be used for both level name and message.\n\n\nIf you just want to modify the built-in themes, you can do it before you specify it to the pyppl constructor:\n\nfrom\n \nPyPPL\n \nimport\n \nlogger\n,\n \nPyPPL\n\n\nlogger\n.\nthemes\n[\ngreenOnBlack\n][\nDONE\n]\n \n=\n \nlogger\n.\ncolors\n.\ncyan\n\n\n# ... define some procs\n\n\nPyPPL\n({\n_log\n:{\ntheme\n:\n \ngreenOnBlack\n}})\n.\nstart\n(\n...\n)\n.\nrun\n()\n\n\n\n\nYes, of course, you can also define a completely new theme:\n\nfrom\n \npyppl\n \nimport\n \nlogger\n,\n \nPyPPL\n\n\n# ... define procs\n\n\nPyPPL\n({\n_log\n:\n \n    \n{\ntheme\n:\n \n{\n\n        \nDONE\n:\n \nlogger\n.\ncolors\n.\ngreen\n,\n\n        \nDEBUG\n:\n \nlogger\n.\ncolors\n.\nblack\n,\n\n        \nstarts:LOG\n:\n \nlogger\n.\ncolors\n.\nbgwhite\n \n+\n \nlogger\n.\ncolors\n.\nblack\n,\n\n        \n# ...\n\n    \n}}\n\n\n})\n.\nstart\n(\n...\n)\n.\nrun\n()\n\n\n\n\nAvailable colors in \nlogger.colors\n:\n\n\n\n\n\n\n\n\nKey\n\n\nColor\n\n\nKey\n\n\nColor\n\n\nKey\n\n\nColor\n\n\nKey\n\n\nColor\n\n\nKey\n\n\nColor\n\n\n\n\n\n\n\n\n\n\nnone\n\n\n''\n1\n\n\nblack\n\n\n\n\nred\n\n\n\n\ngreen\n\n\n\n\nyellow\n\n\n\n\n\n\n\n\nend\n\n\n2\n\n\nblue\n\n\n\n\nmagenta\n\n\n\n\ncyan\n\n\n\n\nwhite\n\n\n\n\n\n\n\n\nbold\n\n\nA\n3\n\n\nbgblack\n\n\n\n\nbgred\n\n\n\n\nbggreen\n\n\n\n\nbgyellow\n\n\n\n\n\n\n\n\nunderline\n\n\n_\n4\n\n\nbgblue\n\n\n\n\nbgmagenta\n\n\n\n\nbgcyan\n\n\n\n\nbgwhite\n\n\n\n\n\n\n\n\n\n\n\n\nAn empty string; 2. End of coloring; 3. Show bold characters; 4. Show underline characters.\n\n\n\n\nYou can also use the directly terminal escape sequences, like \n\\033[30m\n for black (check \nhere\n).\n\n\nIf you define a theme in a configuration file, you may use the escape sequences or also use the color names:\n\n{\n\n    \n_log\n:\n \n{\ntheme\n:\n \n{\n\n        \nDONE\n:\n \n{{colors.green}}\n,\n\n        \nDEBUG\n:\n \n{{colors.black}}\n,\n\n        \nstarts:LOG\n:\n \n{{colors.bgwhite}}{{colors.black}}\n,\n\n        \n#\n \n...\n\n    \n}}\n\n\n}\n\n\n\n\nLog to file\n\n\nBy default, pyppl will not log to a file until you set a file path to \n{\"_log\": {\"file\": \"/path/to/logfile\"}}\n in the configuration. Or you can specfiy \nFalse\n to it to disable logging to file. If you set it to \nTrue\n, a default log file will be used, which is: \n\"./pipeline.pyppl.log\"\n if your pipeline is from file: \n./pipeline.py\n\n\n\n\nNote\n\n\nFilters and themes are not applied to handler to log to file. So you can always find all logs in the log file if your have it enabled.\n\n\n\n\nProgress bar\n\n\nJob status and progress are indicated in the log with progress bar:\n\n[==============================XXXXX!!!!!\n-----]\n\n\nThe bar length defaults to \n50\n. You can change it in your code:\n\nfrom\n \npyppl\n \nimport\n \nJobmgr\n\n\nJobmgr\n.\nPBAR_SIZE\n \n=\n \n80\n\n\n\nHere is an explanation about how a cell (one sign or element of the bar) corresponds to job(s):\nLet's say \nJobmgr.PBAR_SIZE = 9\n and we have 5 jobs, then every two cells represent 1 job for first 8 cells, and last one represents job #5. The rule is trying to equally distributed the cells to jobs:\n\n 1 2 3 4 5         1  2  345      1    2345\n[==XX!!\n-]  NOT  [===XXX!\n-] OR [=====X!\n-]\n\n\nWhile if you have more jobs than cells, we try to equally distribute jobs to cells:\nSay we have 9 jobs but \nJobmgr.PBAR_SIZE = 5\n:\n\n 1357        \n 24689        \n[=X!\n-] \n\n\nFirst cell represents job #1, #2; second job #3, #4; ...; the last one represents job #9.\n\n\nThe meaning of each sign in the cell:\n- \n-\n: Job initiated\n- \n!\n: Job failed to submit\n- \n: Job running\n- \nX\n: Job failed\n- \n=\n: Job done\n\n\nNote that if a cell represents multiple jobs, it has a priority as above listed. For example, in the second situation, if job #1 is done, however, job #2 is running, then the sign should be \n.\n\n\nBut if the progress bar belongs to a job (shown when a job is submitted or done), the status of the job has the highest priority. So in the above example, if the progress bar belongs to job #1:\n\n[JOBDONE] [1/9] [=----] Done:  x.x% | Running: x\n           ^ Indicating current job\n\n\nSo even job #2 belongs to the first cell and it's running, the sign is still \n=\n.", 
            "title": "Log configuration"
        }, 
        {
            "location": "/configure-your-logs/#configure-your-logs", 
            "text": "PyPPL  has fancy logs. You can define how they look like (theme) and what messages to show (levels).", 
            "title": "Configure your logs"
        }, 
        {
            "location": "/configure-your-logs/#built-in-log-themes", 
            "text": "We have some built-in themes:  greenOnBlack (default):  greenOnWhite: \nblueOnBlack:  blueOnWhite:  magentaOnBlack:  magentaOnWhite:  If you don't like them, you can also disable them:   To use them, just specify the name in your pipeline configuration file: { \n     _log :   { \n         theme :   magentaOnWhite \n     }  }  \nOr when you initialize a pipeline: PyPPL ({ _log :   { theme :   magentaOnWhite }}) . start ( ... ) . run ()  \nIf you want to disable the theme, just set  \"theme\"  to  False  ( false  for  json )\nIf you set  theme  to  True , then default theme  greenOnBlack  is used.", 
            "title": "Built-in log themes"
        }, 
        {
            "location": "/configure-your-logs/#levels-of-pyppl-logs", 
            "text": "Please note that the levels are different from those of python's  logging  module. For  logging  module has  6 levels , with different int values. However, pyppl's log has many levels, or more suitable, flags, which don't have corresponding values. They are somehow equal, but some of them always print out unless you ask them not to.   Note  The log levels are a little bit different from here, please see  debug your script .   You may also specify the group name in your pipeline configuration file: { \n     _log :   { \n         levels :   nodebug \n     }, \n     //   running   profiles   ...  }  \nOr when you initialize a pipeline: PyPPL ({ _log :   { levels :   nodebug }}) . start ( ... ) . run ()   You can also explicitly define a set of messages with different levels to show in the logs: { \n     _log :   { levels :   [ PROCESS ,   RUNNING ,   CACHED ]}  }   Even you can modify the base groups: { \n     _log :   { \n         levels :   normal , \n         lvldiff :   [ +DEBUG ,   P.ARGS ,   -SUBMIT ] \n     }  }  \nThen the  DEBUG ,  P.ARGS  messages will show, and  SUBMIT  will hide.", 
            "title": "Levels of pyppl logs"
        }, 
        {
            "location": "/configure-your-logs/#define-your-theme", 
            "text": "Let's see how the built-in theme looks like first:\nin  pyppl/logger.py : themes   =   { \n   greenOnBlack :   { \n     PROCESS   :   [ colors . bold   +   colors . cyan ,   colors . bold   +   colors . underline   +   colors . cyan ], \n     DONE      :   colors . bold   +   colors . green , \n     DEBUG     :   colors . bold   +   colors . black , \n     DEPENDS   :   colors . magenta , \n     PROCESS   :   [ colors . bold   +   colors . cyan ,   colors . bold   +   colors . underline   +   colors . cyan ], \n     in:SUBMIT,JOBDONE,INFO,P.PROPS,OUTPUT,EXPORT,INPUT,P.ARGS,BRINGS :   colors . green , \n     has:ERR   :   colors . red , \n     in:WARNING,RETRY   :   colors . bold   +   colors . yellow , \n     in:CACHED,RUNNING :   colors . yellow , \n              :   colors . white \n   }, \n   # other themes  }  \nFor the keys, you may either directly use the level name or have some prefix to define how to match the level names:\n-  in:  matches the messages with level name in the following list, which is separated by comma ( , ).\n-  has:  matches the messages with level name containing the following string.\n-  starts:  matches the messages with level name starting with the following string.\n-  re : uses the following string as regular expression to match \nThen empty string key ( '' ) defines the colors to use for the messages that can not match any of the above rules.  For the values, basically it's a 2-element list, where the first one defines the color to show the level name; and the second is the color to render the message. If only one color offered, it will be used for both level name and message.  If you just want to modify the built-in themes, you can do it before you specify it to the pyppl constructor: from   PyPPL   import   logger ,   PyPPL  logger . themes [ greenOnBlack ][ DONE ]   =   logger . colors . cyan  # ... define some procs  PyPPL ({ _log :{ theme :   greenOnBlack }}) . start ( ... ) . run ()   Yes, of course, you can also define a completely new theme: from   pyppl   import   logger ,   PyPPL  # ... define procs  PyPPL ({ _log :  \n     { theme :   { \n         DONE :   logger . colors . green , \n         DEBUG :   logger . colors . black , \n         starts:LOG :   logger . colors . bgwhite   +   logger . colors . black , \n         # ... \n     }}  }) . start ( ... ) . run ()   Available colors in  logger.colors :     Key  Color  Key  Color  Key  Color  Key  Color  Key  Color      none  '' 1  black   red   green   yellow     end  2  blue   magenta   cyan   white     bold  A 3  bgblack   bgred   bggreen   bgyellow     underline  _ 4  bgblue   bgmagenta   bgcyan   bgwhite       An empty string; 2. End of coloring; 3. Show bold characters; 4. Show underline characters.   You can also use the directly terminal escape sequences, like  \\033[30m  for black (check  here ).  If you define a theme in a configuration file, you may use the escape sequences or also use the color names: { \n     _log :   { theme :   { \n         DONE :   {{colors.green}} , \n         DEBUG :   {{colors.black}} , \n         starts:LOG :   {{colors.bgwhite}}{{colors.black}} , \n         #   ... \n     }}  }", 
            "title": "Define your theme"
        }, 
        {
            "location": "/configure-your-logs/#log-to-file", 
            "text": "By default, pyppl will not log to a file until you set a file path to  {\"_log\": {\"file\": \"/path/to/logfile\"}}  in the configuration. Or you can specfiy  False  to it to disable logging to file. If you set it to  True , a default log file will be used, which is:  \"./pipeline.pyppl.log\"  if your pipeline is from file:  ./pipeline.py   Note  Filters and themes are not applied to handler to log to file. So you can always find all logs in the log file if your have it enabled.", 
            "title": "Log to file"
        }, 
        {
            "location": "/configure-your-logs/#progress-bar", 
            "text": "Job status and progress are indicated in the log with progress bar: [==============================XXXXX!!!!! -----] \nThe bar length defaults to  50 . You can change it in your code: from   pyppl   import   Jobmgr  Jobmgr . PBAR_SIZE   =   80  \nHere is an explanation about how a cell (one sign or element of the bar) corresponds to job(s):\nLet's say  Jobmgr.PBAR_SIZE = 9  and we have 5 jobs, then every two cells represent 1 job for first 8 cells, and last one represents job #5. The rule is trying to equally distributed the cells to jobs:  1 2 3 4 5         1  2  345      1    2345\n[==XX!! -]  NOT  [===XXX! -] OR [=====X! -] \nWhile if you have more jobs than cells, we try to equally distribute jobs to cells:\nSay we have 9 jobs but  Jobmgr.PBAR_SIZE = 5 :  1357        \n 24689        \n[=X! -]  \nFirst cell represents job #1, #2; second job #3, #4; ...; the last one represents job #9.  The meaning of each sign in the cell:\n-  - : Job initiated\n-  ! : Job failed to submit\n-  : Job running\n-  X : Job failed\n-  = : Job done  Note that if a cell represents multiple jobs, it has a priority as above listed. For example, in the second situation, if job #1 is done, however, job #2 is running, then the sign should be  .  But if the progress bar belongs to a job (shown when a job is submitted or done), the status of the job has the highest priority. So in the above example, if the progress bar belongs to job #1: [JOBDONE] [1/9] [=----] Done:  x.x% | Running: x\n           ^ Indicating current job \nSo even job #2 belongs to the first cell and it's running, the sign is still  = .", 
            "title": "Progress bar"
        }, 
        {
            "location": "/draw-flowchart-of-a-pipeline/", 
            "text": "Draw flowchart of a pipeline\n\n\n\n\n\nPyPPL\n will generate a graph in \nDOT language\n, according to the process dependencies. \nYou can have multiple \nrenderers\n to visualize to graph. A typical one is \nGraphviz\n. With its python port \ngraphviz\n installed, you can output the flowchart to an svg figure. \n\n\nGenerate the flowchart\n\n\nFor example, if we have a pipeline written in \npipeline.py\n:\n\nfrom\n \npyppl\n \nimport\n \nPyPPL\n,\n \nProc\n\n\n\np1\n \n=\n \nProc\n()\n\n\np2\n \n=\n \nProc\n()\n\n\np3\n \n=\n \nProc\n()\n\n\np4\n \n=\n \nProc\n()\n\n\np5\n \n=\n \nProc\n()\n\n\np6\n \n=\n \nProc\n()\n\n\np7\n \n=\n \nProc\n()\n\n\np8\n \n=\n \nProc\n()\n\n\np9\n \n=\n \nProc\n()\n\n\n\n\n\t\t   p1         p8\n\n\n\t\t/      \\      /\n\n\n\t p2           p3\n\n\n\t\t\\      /\n\n\n\t\t   p4         p9\n\n\n\t\t/      \\      /\n\n\n\t p5          p6 (export)\n\n\n\t\t\\      /\n\n\n\t\t  p7 (export)\n\n\n\n\np2\n.\ndepends\n \n=\n \np1\n\n\np3\n.\ndepends\n \n=\n \np1\n,\n \np8\n\n\np4\n.\ndepends\n \n=\n \np2\n,\n \np3\n\n\np4\n.\nexdir\n   \n=\n \n./export\n\n\np5\n.\ndepends\n \n=\n \np4\n\n\np6\n.\ndepends\n \n=\n \np4\n,\n \np9\n\n\np6\n.\nexdir\n   \n=\n \n./export\n\n\np7\n.\ndepends\n \n=\n \np5\n,\n \np6\n\n\np7\n.\nexdir\n   \n=\n \n./export\n\n\n\n# make sure at least one job is created.\n\n\np1\n.\ninput\n \n=\n \n{\nin\n:\n \n[\n0\n]}\n\n\np8\n.\ninput\n \n=\n \n{\nin\n:\n \n[\n0\n]}\n\n\np9\n.\ninput\n \n=\n \n{\nin\n:\n \n[\n0\n]}\n\n\n\nPyPPL\n()\n.\nstar\n(\np1\n,\n \np8\n,\n \np9\n)\n.\nflowchart\n()\n.\nrun\n()\n\n\n\n\nYou can specify different files to store the dot and svg file:\n\npyppl\n()\n.\nstarts\n(\np1\n,\n \np8\n,\n \np9\n)\n.\nflowchart\n(\n/another/dot/file\n,\n \n/another/svg/file\n)\n\n\n\n\n\n\nNote\n The svg file will be only generated if you specify the right command to do it.\n\n\n\n\nFor example, if you have \nGraphviz\n installed, you will have \ndot\n available to convert the dot file to svg file:\n\nPyPPL\n()\n.\nstart\n(\np1\n,\n \np8\n,\n \np9\n)\n.\nflowchart\n(\n\n\t\n/another/dot/file\n,\n \n\t\n/another/svg/file\n\n\n)\n\n\n\n\nThe graph (\nsvgfile\n) will be like:\n\n\n\n\nThe green processes are the starting processes; ones with purple text are processes that will export the output files; and nodes in red are the end processes of the pipeline.\n\n\nUse the dark theme\n\n\nPyPPL\n({\n\n\t\n_flowchart\n:\n \n{\ntheme\n:\n \ndark\n}\n\n\n})\n.\nstar\n(\np1\n,\n \np8\n,\n \np9\n)\n.\nflowchart\n()\n.\nrun\n()\n\n\n\n\n\n\nDefine your own theme\n\n\nYou just need to define the style for each type of nodes (refer \nDOT node shapes\n for detailed styles):\nYou may also put the definition in the default configuration file (\n~/.PyPPL.json\n)\n\nPyPPL\n({\n\n\t\n_flowchart\n:\n \n{\n\n\t\t\ntheme\n:\n \n{\n\n\t\t\t\nbase\n:\n  \n{\n\n\t\t\t\t\nshape\n:\n     \nbox\n,\n\n\t\t\t\t\nstyle\n:\n     \nrounded,filled\n,\n\n\t\t\t\t\nfillcolor\n:\n \n#555555\n,\n\n\t\t\t\t\ncolor\n:\n     \n#ffffff\n,\n\n\t\t\t\t\nfontcolor\n:\n \n#ffffff\n,\n\n\t\t\t\n},\n\n\t\t\t\nstart\n:\n \n{\n\n\t\t\t\t\nstyle\n:\n \nfilled\n,\n\n\t\t\t\t\ncolor\n:\n \n#59b95d\n,\n \n# green\n\n\t\t\t\t\npenwidth\n:\n \n2\n,\n\n\t\t\t\n},\n\n\t\t\t\nend\n:\n \n{\n\n\t\t\t\t\nstyle\n:\n \nfilled\n,\n\n\t\t\t\t\ncolor\n:\n \n#ea7d75\n,\n \n# red\n\n\t\t\t\t\npenwidth\n:\n \n2\n,\n\n\t\t\t\n},\n\n\t\t\t\nexport\n:\n \n{\n\n\t\t\t\t\nfontcolor\n:\n \n#db95e6\n,\n \n# purple\n\n\t\t\t\n},\n\n\t\t\t\nskip\n:\n \n{\n\n\t\t\t\t\nfillcolor\n:\n \n#eaeaea\n,\n \n# gray\n\n\t\t\t\n},\n\n\t\t\t\nskip+\n:\n \n{\n\n\t\t\t\t\nfillcolor\n:\n \n#e9e9e9\n,\n \n# gray\n\n\t\t\t\n},\n\n\t\t\t\nresume\n:\n \n{\n\n\t\t\t\t\nfillcolor\n:\n \n#1b5a2d\n,\n \n# light green\n\n\t\t\t\n},\n\n\t\t\t\naggr\n:\n \n{\n\n\t\t\t\t\nstyle\n:\n \nfilled\n,\n\n\t\t\t\t\ncolor\n:\n \n#eeeeee\n,\n \n# almost white\n\n\t\t\t\n}\n\n\t\t\n}\n\n\t\n}\n\n\n})\n.\nstar\n(\np1\n,\n \np8\n,\n \np9\n)\n.\nflowchart\n()\n.\nrun\n()\n\n\n\n\nExplanations of node types:\n\n\n\n\nbase\n: The base node style\n\n\nstart\n: The style for starting processes\n\n\nend\n: The style for starting processes\n\n\nexport\n: The style for processes have output file to be exported\n\n\nskip\n: The style for processes to be skiped\n\n\nskip+\n: The style for processes to be skiped but ouput channel will be computed\n\n\nresume\n: The style for the processes to be resumed\n\n\naggr\n: The style for the group, where all processes belong to the same aggregation", 
            "title": "Pipeline flowchart"
        }, 
        {
            "location": "/draw-flowchart-of-a-pipeline/#draw-flowchart-of-a-pipeline", 
            "text": "PyPPL  will generate a graph in  DOT language , according to the process dependencies. \nYou can have multiple  renderers  to visualize to graph. A typical one is  Graphviz . With its python port  graphviz  installed, you can output the flowchart to an svg figure.", 
            "title": "Draw flowchart of a pipeline"
        }, 
        {
            "location": "/draw-flowchart-of-a-pipeline/#generate-the-flowchart", 
            "text": "For example, if we have a pipeline written in  pipeline.py : from   pyppl   import   PyPPL ,   Proc  p1   =   Proc ()  p2   =   Proc ()  p3   =   Proc ()  p4   =   Proc ()  p5   =   Proc ()  p6   =   Proc ()  p7   =   Proc ()  p8   =   Proc ()  p9   =   Proc ()   \t\t   p1         p8  \t\t/      \\      /  \t p2           p3  \t\t\\      /  \t\t   p4         p9  \t\t/      \\      /  \t p5          p6 (export)  \t\t\\      /  \t\t  p7 (export)   p2 . depends   =   p1  p3 . depends   =   p1 ,   p8  p4 . depends   =   p2 ,   p3  p4 . exdir     =   ./export  p5 . depends   =   p4  p6 . depends   =   p4 ,   p9  p6 . exdir     =   ./export  p7 . depends   =   p5 ,   p6  p7 . exdir     =   ./export  # make sure at least one job is created.  p1 . input   =   { in :   [ 0 ]}  p8 . input   =   { in :   [ 0 ]}  p9 . input   =   { in :   [ 0 ]}  PyPPL () . star ( p1 ,   p8 ,   p9 ) . flowchart () . run ()   You can specify different files to store the dot and svg file: pyppl () . starts ( p1 ,   p8 ,   p9 ) . flowchart ( /another/dot/file ,   /another/svg/file )    Note  The svg file will be only generated if you specify the right command to do it.   For example, if you have  Graphviz  installed, you will have  dot  available to convert the dot file to svg file: PyPPL () . start ( p1 ,   p8 ,   p9 ) . flowchart ( \n\t /another/dot/file ,  \n\t /another/svg/file  )   The graph ( svgfile ) will be like:   The green processes are the starting processes; ones with purple text are processes that will export the output files; and nodes in red are the end processes of the pipeline.", 
            "title": "Generate the flowchart"
        }, 
        {
            "location": "/draw-flowchart-of-a-pipeline/#use-the-dark-theme", 
            "text": "PyPPL ({ \n\t _flowchart :   { theme :   dark }  }) . star ( p1 ,   p8 ,   p9 ) . flowchart () . run ()", 
            "title": "Use the dark theme"
        }, 
        {
            "location": "/draw-flowchart-of-a-pipeline/#define-your-own-theme", 
            "text": "You just need to define the style for each type of nodes (refer  DOT node shapes  for detailed styles):\nYou may also put the definition in the default configuration file ( ~/.PyPPL.json ) PyPPL ({ \n\t _flowchart :   { \n\t\t theme :   { \n\t\t\t base :    { \n\t\t\t\t shape :       box , \n\t\t\t\t style :       rounded,filled , \n\t\t\t\t fillcolor :   #555555 , \n\t\t\t\t color :       #ffffff , \n\t\t\t\t fontcolor :   #ffffff , \n\t\t\t }, \n\t\t\t start :   { \n\t\t\t\t style :   filled , \n\t\t\t\t color :   #59b95d ,   # green \n\t\t\t\t penwidth :   2 , \n\t\t\t }, \n\t\t\t end :   { \n\t\t\t\t style :   filled , \n\t\t\t\t color :   #ea7d75 ,   # red \n\t\t\t\t penwidth :   2 , \n\t\t\t }, \n\t\t\t export :   { \n\t\t\t\t fontcolor :   #db95e6 ,   # purple \n\t\t\t }, \n\t\t\t skip :   { \n\t\t\t\t fillcolor :   #eaeaea ,   # gray \n\t\t\t }, \n\t\t\t skip+ :   { \n\t\t\t\t fillcolor :   #e9e9e9 ,   # gray \n\t\t\t }, \n\t\t\t resume :   { \n\t\t\t\t fillcolor :   #1b5a2d ,   # light green \n\t\t\t }, \n\t\t\t aggr :   { \n\t\t\t\t style :   filled , \n\t\t\t\t color :   #eeeeee ,   # almost white \n\t\t\t } \n\t\t } \n\t }  }) . star ( p1 ,   p8 ,   p9 ) . flowchart () . run ()   Explanations of node types:   base : The base node style  start : The style for starting processes  end : The style for starting processes  export : The style for processes have output file to be exported  skip : The style for processes to be skiped  skip+ : The style for processes to be skiped but ouput channel will be computed  resume : The style for the processes to be resumed  aggr : The style for the group, where all processes belong to the same aggregation", 
            "title": "Define your own theme"
        }, 
        {
            "location": "/configure-a-pipeline/", 
            "text": "Full pipeline configuration\n\n\nTo configure your pipeline, you just pass the configurations (a \ndict\n) to the constructor:\n\nppl\n \n=\n \nPyPPL\n(\nconfig\n)\n\n\n\nHere is the full structure of the configurations (\nyaml\n configuration file is also supported since \n0.9.4\n):\n\n{\n\n    \n_log\n:\n \n{\n\n        \nlevels\n:\n \nbasic\n,\n  \n//\n \nthe\n \nlog\n \nlevels\n\n        \ntheme\n:\n \ntrue\n,\n    \n//\n \nuse\n \ncolored\n \nlog\n \ninformation\n\n        \nlvldiff\n:\n \n[\n+DEBUG\n],\n  \n//\n \nmodify\n \nthe\n \nloglevels\n \ngroup\n\n        \nfile\n:\n \nfalse\n,\n    \n//\n \ndisable\n \nlogfile,\n \nor\n \nspecify\n \na\n \ndifferent\n \nlogfile\n\n    \n},\n\n    \n_flowchart\n:\n \n{\n\n        \ntheme\n:\n \ndefault\n,\n\n        \ndot\n:\n \ndot -Tsvg {{dotfile}} -o {{fcfile}}\n\n    \n},\n\n    \nproc\n:\n \n{\n            \n//\n \nshared\n \nconfiguration\n \nof\n \nprocesses\n\n        \nforks\n:\n \n10\n,\n\n        \nrunner\n:\n \nsge\n,\n\n        \nsgeRunner\n:\n \n{\n\n            \n//\n \nsge\n \noptions\n\n        \n}\n\n    \n},\n\n    \nprofile1\n \n:\n \n{\n\n        \nforks\n:\n \n20\n,\n\n        \nrunner\n:\n \nssh\n,\n\n        \nsshRunner\n:\n \n{\n\n            \n//\n \nssh\n \noptions\n\n        \n}\n\n    \n},\n\n    \nprofile2\n:\n \n{\n...\n},\n\n    \nprofile3\n:\n \n{\n...\n},\n\n    \n...\n\n\n}\n\n\n\n- For log configuration please refer to \nconfigure your logs\n\n- For flowchart configuration please refer to \npipeline flowchart\n\n- \nproc\n defines the base running profile for processes in this pipeline. \nAll the properties\n of a process can be set here, but just some common one are recommended. Obviously, \ninput\n is not suitable to be set here, except some extreme cases.\n- \nprofiles\n defines some profiles that may be shared by the processes. To use a profile, just specify the profile name to \nrun\n: \nPyPPL(config).start(process).run(\nprofile\n)\n.\n\n\n\n\nNote\n\n\nYou may also use the runner name as a profile. That means, following profiles are implied in the configuration:\n\n{\n\n    \nsge\n  \n:\n \n{\nrunner\n:\n \nsge\n},\n\n    \nssh\n  \n:\n \n{\nrunner\n:\n \nssh\n},\n\n    \nslurm\n:\n \n{\nrunner\n:\n \nslurm\n},\n\n    \nlocal\n:\n \n{\nrunner\n:\n \nlocal\n},\n\n    \ndry\n  \n:\n \n{\nrunner\n:\n \ndry\n},\n\n\n}\n\n\n\n\n\n\n\n\nCaution\n\n\nYou cannot define profiles with names \n_flowchart\n and \n_log\n\n\n\n\nPriority of configuration options\n\n\nSee \nhere\n for use of configuration files.\n\nNow you have 3 ways to set attributes for a process: \n- directly set the process attributes \n(1)\n, \n- set in the first argument (\nconfig\n) of \nPyPPL\n constructor \n(2)\n, \n- set in a configuration file \n/a/b/pyppl.config.json\n \n(3)\n,\n- set in configuration file \n~/.PyPPL.json\n \n(4)\n, and\n- set in configuration file \n~/.PyPPL\n \n(5)\n\n\nThe priority is: (1) \n (2) \n (3) \n (4) \n (5).\n\nOnce you set the property of the process, it will never be changed by \nPyPPL\n constructor or the configuration file. But the first argument can overwrite the options in configuration files.\nHere are an examples to illustrate the priority:\n\n\n Example 1:\n\n\n# ~/.PyPPL.json\n\n\n\n\n{\n\n\n    \nproc\n: {\nforks\n: 5}\n\n\n}\n\n\n\n\n\np\n \n=\n \nProc\n()\n\n\np\n.\nforks\n \n=\n \n1\n\n\n\nppl\n \n=\n \nPyPPL\n({\nproc\n:\n \n{\nforks\n:\n \n10\n}})\n\n\n\n# p.forks == 1\n\n\n\n\n Example 2:\n\n\n# we also have ~/.PyPPL.json as previous example\n\n\np\n \n=\n \nProc\n()\n\n\n\nppl\n \n=\n \nPyPPL\n({\nproc\n:\n \n{\nforks\n:\n \n10\n}})\n\n\n\n# p.forks == 10\n\n\n\n\n Example 3:\n\n\n# we also have ~/.PyPPL.json as previous example\n\n\np\n \n=\n \nProc\n()\n\n\n\nppl\n \n=\n \nPyPPL\n()\n\n\n\n# p.forks == 5\n\n\n\n\n\n\nCaution\n\n\n\n\nIf a process is depending on other processes, you are not supposed to set it as starting process. Of course you can, but make sure the input channel can be normally constructed.\n\n\nIf a process is not depending on any other processes, you have to set it as starting process. Otherwise, it won't start to run.", 
            "title": "Pipeline configuration"
        }, 
        {
            "location": "/configure-a-pipeline/#full-pipeline-configuration", 
            "text": "To configure your pipeline, you just pass the configurations (a  dict ) to the constructor: ppl   =   PyPPL ( config )  \nHere is the full structure of the configurations ( yaml  configuration file is also supported since  0.9.4 ): { \n     _log :   { \n         levels :   basic ,    //   the   log   levels \n         theme :   true ,      //   use   colored   log   information \n         lvldiff :   [ +DEBUG ],    //   modify   the   loglevels   group \n         file :   false ,      //   disable   logfile,   or   specify   a   different   logfile \n     }, \n     _flowchart :   { \n         theme :   default , \n         dot :   dot -Tsvg {{dotfile}} -o {{fcfile}} \n     }, \n     proc :   {              //   shared   configuration   of   processes \n         forks :   10 , \n         runner :   sge , \n         sgeRunner :   { \n             //   sge   options \n         } \n     }, \n     profile1   :   { \n         forks :   20 , \n         runner :   ssh , \n         sshRunner :   { \n             //   ssh   options \n         } \n     }, \n     profile2 :   { ... }, \n     profile3 :   { ... }, \n     ...  }  \n- For log configuration please refer to  configure your logs \n- For flowchart configuration please refer to  pipeline flowchart \n-  proc  defines the base running profile for processes in this pipeline.  All the properties  of a process can be set here, but just some common one are recommended. Obviously,  input  is not suitable to be set here, except some extreme cases.\n-  profiles  defines some profiles that may be shared by the processes. To use a profile, just specify the profile name to  run :  PyPPL(config).start(process).run( profile ) .   Note  You may also use the runner name as a profile. That means, following profiles are implied in the configuration: { \n     sge    :   { runner :   sge }, \n     ssh    :   { runner :   ssh }, \n     slurm :   { runner :   slurm }, \n     local :   { runner :   local }, \n     dry    :   { runner :   dry },  }     Caution  You cannot define profiles with names  _flowchart  and  _log", 
            "title": "Full pipeline configuration"
        }, 
        {
            "location": "/configure-a-pipeline/#priority-of-configuration-options", 
            "text": "See  here  for use of configuration files. \nNow you have 3 ways to set attributes for a process: \n- directly set the process attributes  (1) , \n- set in the first argument ( config ) of  PyPPL  constructor  (2) , \n- set in a configuration file  /a/b/pyppl.config.json   (3) ,\n- set in configuration file  ~/.PyPPL.json   (4) , and\n- set in configuration file  ~/.PyPPL   (5)  The priority is: (1)   (2)   (3)   (4)   (5). \nOnce you set the property of the process, it will never be changed by  PyPPL  constructor or the configuration file. But the first argument can overwrite the options in configuration files.\nHere are an examples to illustrate the priority:   Example 1:  # ~/.PyPPL.json   {       proc : { forks : 5}  }   p   =   Proc ()  p . forks   =   1  ppl   =   PyPPL ({ proc :   { forks :   10 }})  # p.forks == 1    Example 2:  # we also have ~/.PyPPL.json as previous example  p   =   Proc ()  ppl   =   PyPPL ({ proc :   { forks :   10 }})  # p.forks == 10    Example 3:  # we also have ~/.PyPPL.json as previous example  p   =   Proc ()  ppl   =   PyPPL ()  # p.forks == 5    Caution   If a process is depending on other processes, you are not supposed to set it as starting process. Of course you can, but make sure the input channel can be normally constructed.  If a process is not depending on any other processes, you have to set it as starting process. Otherwise, it won't start to run.", 
            "title": "Priority of configuration options"
        }, 
        {
            "location": "/command-line-argument-parser/", 
            "text": "Command line argument parser\n\n\n\n\n\nThis module is just for your convenience. You are free to use Python's built-in \nargparse\n module or just use \nsys.argv\n.\n\n\nTo start with, just import it from \nPyPPL\n:\n\nfrom\n \npyppl\n \nimport\n \nparams\n\n\n\n\nAdd an option\n\n\nparams\n.\nopt\n \n=\n \na\n\n\n# or\n\n\n# params.opt.setValue(\na\n)\n\n\nparams\n.\nopt2\n.\nsetType\n(\nlist\n)\n.\nsetRequired\n()\n\n\n\n# then don\nt forget to parse the command line arguments:\n\n\nparams\n.\nparse\n()\n\n\n\nThen \npython pipeline.py -opt b -opt2 1 2 3\n will overwrite the value.\n\n\nlist\n option can also be specified as \n-opt2 1 -opt2 2 -opt2 3\n\nTo use the value:\n\nvar\n \n=\n \nparams\n.\nopt\n.\nvalue\n \n+\n \n2\n\n\n# var is \nb2\n\n\nvar2\n \n=\n \nparams\n.\nopt2\n.\nvalue\n \n+\n \n[\n4\n]\n\n\n# var2 is [1, 2, 3, 4]\n\n\n\n\nIf you feel annoying using \nparams.xxx.value\n to have the value of the option, you can convert the \nparams\n object to a \nBox\n object (inherited from \nOrderedDict\n):\n\nparams\n.\nparse\n()\n\n\nps\n \n=\n \nparams\n.\nasDict\n()\n\n\n# or just\n\n\n# ps = params.parse()\n\n\n\nThen you can use \nparams.xxx\n to get the value directly:\n\nvar\n \n=\n \nps\n.\nopt\n \n+\n \n2\n\n\nvar2\n \n=\n \nps\n.\nopt2\n \n+\n \n[\n4\n]\n\n\n\n\nSet attributes of an option\n\n\nAn option has server properties:\n- \ndesc\n: The description of the option, shows in the help page. Default: \n[]\n\n- \nrequired\n: Whether the option is required. Default: \nFalse\n\n- \nshow\n: Whether this option should be shown in the help page. Default: \nTrue\n\n- \ntype\n: The type of the option value. Default: \nstr\n\n- \nname\n: The name of the option, then in command line, \n-\nname\n refers to the option.\n- \nvalue\n: The default value of the option.\n\n\nYou can either set the value of an option by \n\nparams\n.\nopt\n.\nrequired\n \n=\n \nTrue\n\n\n\nor\n\nparams\n.\nopt\n.\nsetRequired\n(\nTrue\n)\n\n\n# params.opt.setDesc(\nDesctipion\n)\n\n\n# params.opt.setShow(False)\n\n\n# params.opt.setType(list)\n\n\n# params.opt.setName(\nopt1\n)\n\n\n# params.opt.setValue([1,2,3])\n\n\n# or they can be chained:\n\n\n# params.opt.setDesc(\nDesctipion\n) \\\n\n\n#           .setShow(False)        \\\n\n\n#           .setType(list)         \\\n\n\n#           .setName(\nopt1\n)       \\\n\n\n#           .setValue([1,2,3])\n\n\n\n\nAbout \ntype\n\n\nDeclear the type of an option\n\n\nYou can use either the type itself or the type name:\n\n\nparams.opt.type = int\n or \nparams.opt.type = 'int'\n\n\nInfer type when option initialized\n\n\nWhen you initialize an option:\n\n# with nothing specified\n\n\nparams\n.\nopt\n\n\n# with a default value\n\n\nparams\n.\nopt\n \n=\n \n1\n\n\n\nThe type will be inferred from the value. In the first case, the type is \nNone\n, will in the second, it's \n'int'\n\n\n\n\nNote\n\n\nLet initialize an option first: \nparams.opt\n, then when the value is set explictly:\n\n\nparams.opt.value = \"a\"\n\nOr when the value replaced (set value after initialization):\n\n\nparams.opt = \"a\"\n\nThe type will not be changed (it's \nNone\n in this case).\n\n\n\n\nAllowed types\n\n\nLiterally, allowed types are \nstr\n, \nint\n, \nfloat\n, \nbool\n and \nlist\n. But we allow subtypes (types of elements) for \nlist\n. By default, the value of \nlist\n elements will be recognized automatically. For example, \n'1'\n will be recognized as an integer \n1\n, and \n\"True\"\n will be recognized as bool value \nTrue\n. You can avoid this by specified the subtypes explictly: \nparams.opt.type = 'list:str'\n, then \n'1'\n will be kept as \n'1'\n rather than \n1\n, and \n\"True\"\n will be \n\"True\"\n instead of \nTrue\n.\n\n\nYou can use shortcuts for the types:\n\ni     -\n \nint\n\nf     -\n \nfloat\n\nb     -\n \nbool\n\ns     -\n \nstr\n\nl     -\n \nlist\n\narray -\n \nlist\n\n\n\nFor subtypes, you can also do \nparams.opt.type = 'l:s'\n indicating \nlist:str\n. Or you can even mix them: \nparams.opt.type = 'l:str'\n or \nparams.opt.type = 'list:s'\n\n\nOverwrite the type from command line\n\n\nEven though we may declear the type of an option by \nparams.opt.type = 'list:str'\n, the users are able to overwrite it by pass the type through command argument:\n\n\n program -opt:list:int 1 2 3\n\nThen we will get: \nparams.opt.value == [1,2,3]\n instead of \nparams.opt.value == ['1', '2', '3']\n when we do:\n\n\n program -opt 1 2 3\n\nFlexibly, we can have mixed types of elements in a list option:\n\n\n program -opt:list:bool 1 -opt:list:int 2 -opt:list:str 3\n\nWe will get: \nparams.opt.value == [True, 2, '3']\n\n\nLoad params from a dict\n\n\nYou can define a \ndict\n, and then load it to \nparams\n:\n\nd2load\n \n=\n \n{\n\n    \np1\n:\n \n1\n,\n\n    \np1.required\n:\n \nTrue\n,\n\n    \np2\n:\n \n[\n1\n,\n2\n,\n3\n],\n\n    \np2.show\n:\n \nTrue\n,\n\n    \np3\n:\n \n2.3\n,\n\n    \np3.desc\n:\n \nThe p3 params\n,\n\n    \np3.required\n:\n \nTrue\n,\n\n    \np3.show\n:\n \nTrue\n\n\n}\n\n\nparams\n.\nloadDict\n \n(\nd2load\n)\n\n\n# params.p1.value    == 1\n\n\n# params.p1.required == True\n\n\n# params.p1.show     == False\n\n\n# params.p1.type     == int\n\n\n# params.p2.value    == [1,2,3]\n\n\n# params.p2.required == False\n\n\n# params.p2.show     == True\n\n\n# params.p2.type     == list\n\n\n# params.p3.value    == 2.3\n\n\n# params.p3.required == True\n\n\n# params.p3.show     == False\n\n\n# params.p3.type     == float\n\n\n\nNote that by default, the options from the \ndict\n will be hidden from the help page. If you want to show some of them, just set \np2.show = True\n, or you want to show them all: \nparams.loadDict(d2load, show = True)\n\n\nLoad params from a configuration file\n\n\nIf the configuration file has a name ending with '.json', then \njson.load\n will be used to load the file into a \ndict\n, and \nparams.loadDict\n will be used to load it to \nparams\n\n\nElse python's \nConfigParse\n will be used. All params in the configuration file should be under one section with whatever name:\n\n[Config]\n\n\np1: 1\n\n\np1.required: True\n\n\np2: [1,2,3]\n\n\np2.show: True\n\n\np3: 2.3\n\n\np3.desc\n \n=\n \nThe p3 params\n\n\np3.required: True\n\n\np3.show: True\n\n\n\n\nSimilarly, you can set the default value for \nshow\n property by: \nparams.loadCfgfile(\"params.config\", show=True)\n\n\n\n\nHint\n\n\nparams\n is a singleton of \nParameters\n. It's a combination of configuration and command line arguments. So you are able to load the configurations from files, which will be used as default values, before you parse the command line arguments. You are also able to choose some of the options for the users to pass value to, and some hidden from the users.\n\n\n\n\nPreseved option names\n\n\nWe have a certain convention of the option names used with \nparams\n:\n- Make sure it's only composed of alphabetics, underscores and hyphens.\n- Make sure it starts with alphabetics.\n- Make sure it's not one of these words (\nhelp\n, \nloadDict\n, \nloadFile\n and \nasDict\n)\n\n\nSet other attributes of params\n\n\nShow the usages/example/description of the program\n\n\nparams('usage', [\"{prog} -h\", \"{prog} -infile \ninfile\n [options]\"])\n\n\nparams('example', [\"{prog} -h\", \"{prog} -infile path/to/infile -out /path/to/outfile\"])\n\n\nparams('desc', [\"This program does this.\", \"This program also does that.\"])\n\nMultiple lines can also be passed as a string with \n\"\\n\"\n:\n\n\nparams('desc', [\"This program does this.\\nThis program also does that.\"])\n\n\nSet the prefix of option names\n\n\nBy default, the option names are recognized from the command line if they follow the convention and start with \n-\n. You may change it, let's say you want the option names to start with \n--\n:\n\n\nparams('prefix', '--')\n\nThe only \nprog --opt 1\n will be parsed, instead of \nprog -opt 1\n to get the value by \nparams.opt.value\n  \n\n\n\n\nNote\n\n\nYou cannot use an empty prefix\n\n\n\n\nSet the default help options\n\n\nBy default, the help options are \n['-h', '--help', '-H', '-?', '']\n. The empty string (\n''\n) enables the program to print the help page without arguments be passed from the command line. If you don't want it, you can remove it from the default help options:\n\n\nparams('hopts', '-h, --help, -H, -?')\n or\n\n\nparams('hopts', ['-h', '--help', '-H', '-?'])\n\nOr you can just simply remove some of them by:\n\n\nparams('hopts', ['-H', '-?', ''], excl = True)\n\nThen you will only have \n['-h', '--help']\n\n\n\n\nHint\n\n\nThese statements can also be chained:\n\n\nparams('usage', [\"{prog} -h\", \"{prog} -infile \ninfile\n [options]\"])('prefix', '--')('hopts', ['-H', '-?', ''], excl = True)", 
            "title": "Command line argument parser"
        }, 
        {
            "location": "/command-line-argument-parser/#command-line-argument-parser", 
            "text": "This module is just for your convenience. You are free to use Python's built-in  argparse  module or just use  sys.argv .  To start with, just import it from  PyPPL : from   pyppl   import   params", 
            "title": "Command line argument parser"
        }, 
        {
            "location": "/command-line-argument-parser/#add-an-option", 
            "text": "params . opt   =   a  # or  # params.opt.setValue( a )  params . opt2 . setType ( list ) . setRequired ()  # then don t forget to parse the command line arguments:  params . parse ()  \nThen  python pipeline.py -opt b -opt2 1 2 3  will overwrite the value.  list  option can also be specified as  -opt2 1 -opt2 2 -opt2 3 \nTo use the value: var   =   params . opt . value   +   2  # var is  b2  var2   =   params . opt2 . value   +   [ 4 ]  # var2 is [1, 2, 3, 4]   If you feel annoying using  params.xxx.value  to have the value of the option, you can convert the  params  object to a  Box  object (inherited from  OrderedDict ): params . parse ()  ps   =   params . asDict ()  # or just  # ps = params.parse()  \nThen you can use  params.xxx  to get the value directly: var   =   ps . opt   +   2  var2   =   ps . opt2   +   [ 4 ]", 
            "title": "Add an option"
        }, 
        {
            "location": "/command-line-argument-parser/#set-attributes-of-an-option", 
            "text": "An option has server properties:\n-  desc : The description of the option, shows in the help page. Default:  [] \n-  required : Whether the option is required. Default:  False \n-  show : Whether this option should be shown in the help page. Default:  True \n-  type : The type of the option value. Default:  str \n-  name : The name of the option, then in command line,  - name  refers to the option.\n-  value : The default value of the option.  You can either set the value of an option by  params . opt . required   =   True  \nor params . opt . setRequired ( True )  # params.opt.setDesc( Desctipion )  # params.opt.setShow(False)  # params.opt.setType(list)  # params.opt.setName( opt1 )  # params.opt.setValue([1,2,3])  # or they can be chained:  # params.opt.setDesc( Desctipion ) \\  #           .setShow(False)        \\  #           .setType(list)         \\  #           .setName( opt1 )       \\  #           .setValue([1,2,3])", 
            "title": "Set attributes of an option"
        }, 
        {
            "location": "/command-line-argument-parser/#about-type", 
            "text": "", 
            "title": "About type"
        }, 
        {
            "location": "/command-line-argument-parser/#declear-the-type-of-an-option", 
            "text": "You can use either the type itself or the type name:  params.opt.type = int  or  params.opt.type = 'int'", 
            "title": "Declear the type of an option"
        }, 
        {
            "location": "/command-line-argument-parser/#infer-type-when-option-initialized", 
            "text": "When you initialize an option: # with nothing specified  params . opt  # with a default value  params . opt   =   1  \nThe type will be inferred from the value. In the first case, the type is  None , will in the second, it's  'int'   Note  Let initialize an option first:  params.opt , then when the value is set explictly:  params.opt.value = \"a\" \nOr when the value replaced (set value after initialization):  params.opt = \"a\" \nThe type will not be changed (it's  None  in this case).", 
            "title": "Infer type when option initialized"
        }, 
        {
            "location": "/command-line-argument-parser/#allowed-types", 
            "text": "Literally, allowed types are  str ,  int ,  float ,  bool  and  list . But we allow subtypes (types of elements) for  list . By default, the value of  list  elements will be recognized automatically. For example,  '1'  will be recognized as an integer  1 , and  \"True\"  will be recognized as bool value  True . You can avoid this by specified the subtypes explictly:  params.opt.type = 'list:str' , then  '1'  will be kept as  '1'  rather than  1 , and  \"True\"  will be  \"True\"  instead of  True .  You can use shortcuts for the types: i     -   int \nf     -   float \nb     -   bool \ns     -   str \nl     -   list \narray -   list  \nFor subtypes, you can also do  params.opt.type = 'l:s'  indicating  list:str . Or you can even mix them:  params.opt.type = 'l:str'  or  params.opt.type = 'list:s'", 
            "title": "Allowed types"
        }, 
        {
            "location": "/command-line-argument-parser/#overwrite-the-type-from-command-line", 
            "text": "Even though we may declear the type of an option by  params.opt.type = 'list:str' , the users are able to overwrite it by pass the type through command argument:   program -opt:list:int 1 2 3 \nThen we will get:  params.opt.value == [1,2,3]  instead of  params.opt.value == ['1', '2', '3']  when we do:   program -opt 1 2 3 \nFlexibly, we can have mixed types of elements in a list option:   program -opt:list:bool 1 -opt:list:int 2 -opt:list:str 3 \nWe will get:  params.opt.value == [True, 2, '3']", 
            "title": "Overwrite the type from command line"
        }, 
        {
            "location": "/command-line-argument-parser/#load-params-from-a-dict", 
            "text": "You can define a  dict , and then load it to  params : d2load   =   { \n     p1 :   1 , \n     p1.required :   True , \n     p2 :   [ 1 , 2 , 3 ], \n     p2.show :   True , \n     p3 :   2.3 , \n     p3.desc :   The p3 params , \n     p3.required :   True , \n     p3.show :   True  }  params . loadDict   ( d2load )  # params.p1.value    == 1  # params.p1.required == True  # params.p1.show     == False  # params.p1.type     == int  # params.p2.value    == [1,2,3]  # params.p2.required == False  # params.p2.show     == True  # params.p2.type     == list  # params.p3.value    == 2.3  # params.p3.required == True  # params.p3.show     == False  # params.p3.type     == float  \nNote that by default, the options from the  dict  will be hidden from the help page. If you want to show some of them, just set  p2.show = True , or you want to show them all:  params.loadDict(d2load, show = True)", 
            "title": "Load params from a dict"
        }, 
        {
            "location": "/command-line-argument-parser/#load-params-from-a-configuration-file", 
            "text": "If the configuration file has a name ending with '.json', then  json.load  will be used to load the file into a  dict , and  params.loadDict  will be used to load it to  params  Else python's  ConfigParse  will be used. All params in the configuration file should be under one section with whatever name: [Config]  p1: 1  p1.required: True  p2: [1,2,3]  p2.show: True  p3: 2.3  p3.desc   =   The p3 params  p3.required: True  p3.show: True   Similarly, you can set the default value for  show  property by:  params.loadCfgfile(\"params.config\", show=True)   Hint  params  is a singleton of  Parameters . It's a combination of configuration and command line arguments. So you are able to load the configurations from files, which will be used as default values, before you parse the command line arguments. You are also able to choose some of the options for the users to pass value to, and some hidden from the users.", 
            "title": "Load params from a configuration file"
        }, 
        {
            "location": "/command-line-argument-parser/#preseved-option-names", 
            "text": "We have a certain convention of the option names used with  params :\n- Make sure it's only composed of alphabetics, underscores and hyphens.\n- Make sure it starts with alphabetics.\n- Make sure it's not one of these words ( help ,  loadDict ,  loadFile  and  asDict )", 
            "title": "Preseved option names"
        }, 
        {
            "location": "/command-line-argument-parser/#set-other-attributes-of-params", 
            "text": "", 
            "title": "Set other attributes of params"
        }, 
        {
            "location": "/command-line-argument-parser/#show-the-usagesexampledescription-of-the-program", 
            "text": "params('usage', [\"{prog} -h\", \"{prog} -infile  infile  [options]\"])  params('example', [\"{prog} -h\", \"{prog} -infile path/to/infile -out /path/to/outfile\"])  params('desc', [\"This program does this.\", \"This program also does that.\"]) \nMultiple lines can also be passed as a string with  \"\\n\" :  params('desc', [\"This program does this.\\nThis program also does that.\"])", 
            "title": "Show the usages/example/description of the program"
        }, 
        {
            "location": "/command-line-argument-parser/#set-the-prefix-of-option-names", 
            "text": "By default, the option names are recognized from the command line if they follow the convention and start with  - . You may change it, let's say you want the option names to start with  -- :  params('prefix', '--') \nThe only  prog --opt 1  will be parsed, instead of  prog -opt 1  to get the value by  params.opt.value      Note  You cannot use an empty prefix", 
            "title": "Set the prefix of option names"
        }, 
        {
            "location": "/command-line-argument-parser/#set-the-default-help-options", 
            "text": "By default, the help options are  ['-h', '--help', '-H', '-?', ''] . The empty string ( '' ) enables the program to print the help page without arguments be passed from the command line. If you don't want it, you can remove it from the default help options:  params('hopts', '-h, --help, -H, -?')  or  params('hopts', ['-h', '--help', '-H', '-?']) \nOr you can just simply remove some of them by:  params('hopts', ['-H', '-?', ''], excl = True) \nThen you will only have  ['-h', '--help']   Hint  These statements can also be chained:  params('usage', [\"{prog} -h\", \"{prog} -infile  infile  [options]\"])('prefix', '--')('hopts', ['-H', '-?', ''], excl = True)", 
            "title": "Set the default help options"
        }, 
        {
            "location": "/aggregations/", 
            "text": "Aggregations\n\n\nImagine that you have a set of processes predefined, and every time when you deal with similar problems (i.e. format a file and plot the data or some next generation sequencing data analysis), you will consistently use those processes, then you have to configure and call them every time. \n\n\nAggregations are designed for this kind of situations, you can just define an aggregations with those processes, and adjust the dependencies, input and arguments, you will be able to re-use the aggregation with very less configuration.\n\n\nFor example:\n\npTrimmomaticPE\n.\ninput\n             \n=\n \ninput\n \nchannel\n\n\npAlignPEByBWA\n.\ndepends\n            \n=\n \npTrimmomaticPE\n\n\npSortSam\n.\ndepends\n                 \n=\n \npAlignPEByBWA\n\n\npMarkDuplicates\n.\ndepends\n          \n=\n \npSortSam\n\n\npIndexBam\n.\ndepends\n                \n=\n \npMarkDuplicates\n\n\npRealignerTargetCreator\n.\ndepends\n  \n=\n \npIndexBam\n\n\npIndelRealigner\n.\ndepends\n          \n=\n \npIndexBam\n,\n \npRealignerTargetCreator\n\n\npBaseRecalibrator\n.\ndepends\n        \n=\n \npIndelRealigner\n\n\npPrintReads\n.\ndepends\n              \n=\n \npIndelRealigner\n,\n \npBaseRecalibrator\n\n\npPrintReads\n.\nexportdir\n            \n=\n \nexdir\n\n\n\npMarkDuplicates\n.\nargs\n.\nparams\n.\ntmpdir\n   \n=\n \n/local2/tmp/\n\n\npAlignPEByBWA\n.\nargs\n.\nreffile\n           \n=\n \nreffile\n\n\npRealignerTargetCreator\n.\nargs\n.\nreffile\n \n=\n \nreffile\n\n\npRealignerTargetCreator\n.\nargs\n.\nparams\n.\ntmpdir\n=\n/local2/tmp/\n\n\npIndelRealigner\n.\nargs\n.\nreffile\n         \n=\n \nreffile\n\n\npIndelRealigner\n.\nargs\n.\nparams\n.\ntmpdir\n   \n=\n \n/local2/tmp/\n\n\npBaseRecalibrator\n.\nargs\n.\nreffile\n       \n=\n \nreffile\n\n\npBaseRecalibrator\n.\nargs\n.\nknownSites\n    \n=\n \ndbsnp\n\n\npBaseRecalibrator\n.\nargs\n.\nparams\n.\ntmpdir\n \n=\n/local2/tmp/\n\n\npPrintReads\n.\nargs\n.\nreffile\n             \n=\n \nreffile\n\n\npPrintReads\n.\nargs\n.\nparams\n              \n=\n \n/local2/tmp/\n\n\n\nPyPPL\n({\n\n    \nproc\n:\n \n{\n\n        \nforks\n:\n \n100\n,\n\n        \nrunner\n:\n \nsge\n,\n\n        \nsgeRunner\n:\n \n{\n\n            \nsge.q\n:\n \nlg-mem\n\n        \n}\n\n    \n}\n\n\n})\n.\nstart\n(\npTrimmomaticPE\n)\n.\nrun\n()\n\n\n\nThis is a very commonly used Whole Genome Sequencing data cleanup pipeline from the raw reads according to the \nGATK best practice\n. And it will be used pretty much every time when the raw read files come. \n\n\nWith an aggregation defined, you don't need to configure and call those processes every time:\n\nfrom\n \npyppl\n \nimport\n \nAggr\n\n\nfrom\n \nbioprocs\n \nimport\n \nparams\n\n\n# some paramters defined in params\n\n\naFastqPE2Bam\n \n=\n \nAggr\n \n(\n\n\n\u00a0\u00a0\u00a0\u00a0\npTrimmomaticPE\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npAlignPEByBWA\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npSortSam\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npMarkDuplicates\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npIndexBam\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npRealignerTargetCreator\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npIndelRealigner\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npBaseRecalibrator\n,\n\n\n\u00a0\u00a0\u00a0\u00a0\npPrintReads\n\n\n)\n\n\n# dependency adjustment\n\n\naFastqPE2Bam\n.\npIndelRealigner\n.\ndepends\n   \n=\n \naFastqPE2Bam\n.\npIndexBam\n,\n \naFastqPE2Bam\n.\npRealignerTargetCreator\n\n\naFastqPE2Bam\n.\npPrintReads\n.\ndepends\n       \n=\n \naFastqPE2Bam\n.\npIndelRealigner\n,\n \naFastqPE2Bam\n.\npBaseRecalibrator\n\n\n# input adjustment\n\n\n# args adjustment\n\n\naFastqPE2Bam\n.\npMarkDuplicates\n.\nargs\n.\nparams\n.\ntmpdir\n       \n=\n \nparams\n.\ntmpdir\n\n\naFastqPE2Bam\n.\npAlignPEByBWA\n.\nargs\n.\nreffile\n               \n=\n \nparams\n.\nhg19fa\n\n\naFastqPE2Bam\n.\npRealignerTargetCreator\n.\nargs\n.\nreffile\n     \n=\n \nparams\n.\nhg19fa\n\n\naFastqPE2Bam\n.\npRealignerTargetCreator\n.\nargs\n.\nparams\n.\ntmpdir\n=\n \nparams\n.\ntmpdir\n\n\naFastqPE2Bam\n.\npIndelRealigner\n.\nargs\n.\nreffile\n             \n=\n \nparams\n.\nhg19fa\n\n\naFastqPE2Bam\n.\npIndelRealigner\n.\nargs\n.\nparams\n.\ntmpdir\n       \n=\n \nparams\n.\ntmpdir\n\n\naFastqPE2Bam\n.\npBaseRecalibrator\n.\nargs\n.\nreffile\n           \n=\n \nparams\n.\nhg19fa\n\n\naFastqPE2Bam\n.\npBaseRecalibrator\n.\nargs\n.\nknownSites\n        \n=\n \nparams\n.\ndbsnp\n\n\naFastqPE2Bam\n.\npBaseRecalibrator\n.\nargs\n.\nparams\n.\ntmpdir\n     \n=\n \nparams\n.\ntmpdir\n\n\naFastqPE2Bam\n.\npPrintReads\n.\nargs\n.\nreffile\n                 \n=\n \nparams\n.\nhg19fa\n\n\naFastqPE2Bam\n.\npPrintReads\n.\nargs\n.\nparams\n.\ntmpdir\n           \n=\n \nparams\n.\ntmpdir\n\n\n\n\nThen every time you just need to call the aggregation:\n\naFastqPE2Bam\n.\ninput\n \n=\n \nchannel\n.\nfromPairs\n \n(\n \ndatadir\n \n+\n \n/*.fastq.gz\n \n)\n\n\naFastqPE2Bam\n.\nexdir\n \n=\n \nexdir\n\n\nPyPPL\n({\n\n\n\u00a0\u00a0\u00a0\u00a0\nproc\n:\n \n{\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nsgeRunner\n:\n \n{\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\nsge.q\n \n:\n \n1-day\n\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n}\n\n\n\u00a0\u00a0\u00a0\u00a0\n}\n\n\n})\n.\nstart\n(\naFastqPE2Bam\n)\n.\nrun\n()\n\n\n\n\nInitialize an aggregation\n\n\nLike previous example shows, you just need to give the constructor all the processes to construct an aggretation. However, there are several things need to be noticed:\n\n\n\n\nThe dependencies are automatically constructed by the order of the processes. \n   \na\n \n=\n \nAggr\n \n(\np1\n,\n \np2\n,\n \np3\n)\n\n\n# The dependencies will be p1 -\n p2 -\n p3\n\n\n\n\nThe starting and ending processes are defined as the first and last processes, respectively. If you need to modify the dependencies, keep that in mind whether the starting and ending processes are changed.\n    \na\n \n=\n \nAggr\n \n(\np1\n,\n \np2\n,\n \np3\n)\n\n\n# a.starts == [p1]\n\n\n# a.ends   == [p3]\n\n\n#                                / p2\n\n\n# change the dependencies to  p1      \n\n\n#                                \\ p3\n\n\n# both p2, p3 depend on p1, and p3 depends on p2\n\n\na\n.\np3\n.\ndepends\n \n=\n \np1\n,\n \np2\n\n\n# but remember the ending processes are changed from [p3] to [p2, p3]\n\n\na\n.\nends\n \n=\n \n[\np2\n,\n \np3\n]\n\n\n\n    You can also specify the dependencies manually:\n    \na\n \n=\n \nAggr\n \n(\np1\n,\n \np2\n,\n \np3\n,\n \ndepends\n \n=\n \nFalse\n)\n\n\na\n.\np2\n.\ndepends\n \n=\n \np1\n\n\na\n.\np3\n.\ndepends\n \n=\n \np1\n,\n \np2\n\n\na\n.\nstarts\n \n=\n \n[\np1\n]\n\n\na\n.\nends\n   \n=\n \n[\np2\n,\n \np3\n]\n\n\n\n\nIf you have one process used twice in the aggregation, copy it with a different id:\n   \na\n \n=\n \nAggr\n(\n\n    \np1\n,\n\n    \np2\n,\n\n    \np1\n.\ncopy\n(\nid\n \n=\n \np1copy\n)\n\n\n)\n\n\n# then to access the 2nd p1: a.p1copy\n\n\n\n\nEach process is copied by aggregation, so the original one can still be used.\n\n\nThe tag of each process is regenerated by the id of the aggregation.\n\n\n\n\nAdd a process on the run\n\n\naggr.addProc(p, where=None)\n\nYou can add a process, and also define whether to put it in \nstarts\n, \nends\n, \nboth\n or \nNone\n.\n\n\nDelegate attributes of processes to an aggregation\n\n\nYou can Delegate the attributes directly for a process to an aggregation:\n\naFastqPE2Bam\n.\ndelegate\n(\nargs.reffile\n,\n \npAlignPEByBWA\n)\n\n\n\nThen when you want to set \nargs.reffile\n for \npAlignPEByBWA\n, you can just do:\n\naFastqPE2Bam\n.\nargs\n.\nreffile\n \n=\n \n/path/to/hg19.fa\n\n\n\nYou may use \nstarts/ends\n represents the start/end processes.  \n\n\nDelegate an attribute to multiple processes:\n\naFastqPE2Bam\n.\ndelegate\n(\nargs.reffile\n,\n \npAlignPEByBWA, pPrintReads\n)\n\n\n# or\n\n\naFastqPE2Bam\n.\ndelegate\n(\nargs.reffile\n,\n \n[\npAlignPEByBWA\n,\n \npPrintReads\n])\n\n\n# or\n\n\naFastqPE2Bam\n.\ndelegate\n(\nargs.reffile\n,\n \n[\naFastqPE2Bam\n.\npAlignPEByBWA\n,\n \naFastqPE2Bam\n.\npPrintReads\n])\n\n\n\n\nDelegate multiple attributes at one time:\n\naFastqPE2Bam\n.\ndelegate\n(\nargs.reffile, args.tmpdir\n,\n \npAlignPEByBWA, pPrintReads\n)\n\n\n# or\n\n\naFastqPE2Bam\n.\ndelegate\n([\nargs.reffile\n,\n \nargs.tmpdir\n],\n \npAlignPEByBWA, pPrintReads\n)\n\n\n\n\n\n\nCaution\n\n\nUndelegated attributes will be delegated to all processes. But remember if an attribute has been set before, say \np.runner = 'local'\n then, \naggr.runner\n will not overwrite it if \nrunner\n is not delegated. If it is, then it'll be overwritten.\n\n\n\n\n\n\nNote\n\n\nFor attributes that have sub-attributes (i.e. \np.args.params.inopts.cnames\n), you may just delegate the first parts, then the full assignment of the attribute will still follow the delegation. For example:\n\n\naggr\n.\ndelegate\n(\nargs.params\n,\n \np1,p2\n)\n\n\naggr\n.\nargs\n.\nparams\n.\ninopts\n.\ncnames\n \n=\n \nTrue\n \n# only affects p1, p2\n\n\n\nKeep in mind that shorter delegations always come first. In the above case, if we have another delegation: \naggr.delegate('args.params.inopts', 'p3')\n, then the assignment will still affect \np1, p2\n (the first delegation) and \np3\n (the second delegation).\n\n\n\n\nDefault delegations\n\n\nBy default, \ninput/depends\n are delegated for start processes, and \nexdir/exhow/exow/expart\n for end processes. Importantly, as \naggr.starts\n is a list, the values for \ninput/depends\n must be a list as well, with elements corresponing to each start process. \nBesides, we have two special attributes for aggregations: \ninput2\n and \ndepends2\n. Unlike \ninput\n and \ndepends\n, \ninput2\n and \ndepends2\n try to pass everything it gets to each process, instead of passing corresponind element to each process. For example:\n\n# aggr.starts = [aggr.p1, aggr.p2]\n\n\naggr\n.\ninput\n \n=\n \n[[\na\n],\n \n[\nb\n]]\n\n\n# then:\n\n\n# aggr.p1.config[\ninput\n] == [\na\n]\n\n\n# aggr.p2.config[\ninput\n] == [\nb\n]\n\n\n\naggr\n.\ninput2\n \n=\n \n[[\na\n],\n \n[\nb\n]]\n\n\n# then:\n\n\n# aggr.p1.config[\ninput\n] == [[\na\n], [\nb\n]]\n\n\n# aggr.p2.config[\ninput\n] == [[\na\n], [\nb\n]]\n\n\n\n\nSet attribute value for specific processes of an aggregation\n\n\nThere are several ways to do that:\n\n# refer to the process directly\n\n\naFastqPE2Bam\n.\npPrintReads\n.\nargs\n.\ntmpdir\n \n=\n \n/tmp\n\n\naFastqPE2Bam\n.\npPrintReads\n.\nrunner\n \n=\n \nlocal\n\n\n# refer to the index of the process\n\n\naFastqPE2Bam\n.\nargs\n[\n8\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\naFastqPE2Bam\n.\nrunner\n[\n8\n]\n \n=\n \nlocal\n\n\n# refer to the name of the process\n\n\naFastqPE2Bam\n.\nargs\n[\npPrintReads\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\naFastqPE2Bam\n.\nrunner\n[\npPrintReads\n]\n \n=\n \nlocal\n\n\n\n# for multiple processes\n\n\naFastqPE2Bam\n.\nargs\n[:\n3\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\naFastqPE2Bam\n.\nargs\n[\n0\n,\n1\n,\n3\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\naFastqPE2Bam\n.\nargs\n[\naFastqPE2Bam\n,\n \npPrintReads\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\naFastqPE2Bam\n.\nargs\n[\naFastqPE2Bam, pPrintReads\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\n\n# or you may use starts/ends to refer to the start/end processes\n\n\n# has to be done after aggr.starts/aggr.ends assigned \n\n\n# or initialized with depends = True\n\n\naFastqPE2Bam\n.\nargs\n[\nstarts\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\naFastqPE2Bam\n.\nargs\n[\nends\n]\n.\ntmpdir\n \n=\n \n/tmp\n\n\n\n\n\n\nHint\n\n\nIf an attribute is delegated for other processes, you can still set the value of it by the above methods.\n\n\n\n\nSet an aggregation as start aggregation for a pipeline\n\n\nYou can do it just like setting a process as the starting process of pipeline (see \nhere\n). Actually the starting processes in the aggregation (\naggr.starts\n) will be set as the starting processes of the pipeline.\n\n\nThe dependency of aggregations and processes\n\n\nAn aggregation can depend on aggregations and/or processes, you just treat the aggregations as processes. A process can also depend on aggregations and/or processes. \n\n\n\n\n\n\n\n\nWhat am I?\n\n\nWhom I am depending on?\n\n\nReal relations\n\n\n\n\n\n\n\n\n\n\nAggr\n (\na1\n)\n\n\nAggr\n (\na2\n)\n\n\na1.starts\n depends on \na2.ends\n\n\n\n\n\n\nProc\n (\np\n)\n\n\nAggr\n (\na\n)\n\n\np\n depends on \na.ends\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nYou have to specify \ndepends\n for start processes of an aggregation.\n\n\n\n\nCopy an aggregation\n\n\nAggr.copy(tag = 'notag', copyDeps = True, newid = None)\n\nYou may copy an aggregation, all the processes in the aggregation will be copied, and the dependencies will be switched to the corresponding copied processes, as well as the starting and ending processes, if \ncopyDeps == True\n. \n\n\nYou can keep the ids of processes unchanged but give a new tag and also give the aggregation an new id instead of the variable name:\n\na\n \n=\n \nAggr\n(\np1\n,\n \np2\n,\n \np3\n)\n\n\n# access the processes:\n\n\n# a.p1, a.p2, a.p3\n\n\na2\n \n=\n \na\n.\ncopy\n(\ncopied\n)\n\n\n# a2.procs == [\n\n\n# \nproc with id \np1\n and tag \ncopied\n,\n\n\n# \nproc with id \np2\n and tag \ncopied\n,\n\n\n# \nproc with id \np3\n and tag \ncopied\n,\n\n\n# ]\n\n\n# a2.id == \na2\n\n\n# to access the processes:\n\n\n# a2.p1, a2.p2, a2.p3\n\n\na2\n \n=\n \na\n.\ncopy\n(\ncopied\n,\n \nid\n \n=\n \nnewAggr\n)\n\n\n# a2.id == \nnewAggr", 
            "title": "Aggregations"
        }, 
        {
            "location": "/aggregations/#aggregations", 
            "text": "Imagine that you have a set of processes predefined, and every time when you deal with similar problems (i.e. format a file and plot the data or some next generation sequencing data analysis), you will consistently use those processes, then you have to configure and call them every time.   Aggregations are designed for this kind of situations, you can just define an aggregations with those processes, and adjust the dependencies, input and arguments, you will be able to re-use the aggregation with very less configuration.  For example: pTrimmomaticPE . input               =   input   channel  pAlignPEByBWA . depends              =   pTrimmomaticPE  pSortSam . depends                   =   pAlignPEByBWA  pMarkDuplicates . depends            =   pSortSam  pIndexBam . depends                  =   pMarkDuplicates  pRealignerTargetCreator . depends    =   pIndexBam  pIndelRealigner . depends            =   pIndexBam ,   pRealignerTargetCreator  pBaseRecalibrator . depends          =   pIndelRealigner  pPrintReads . depends                =   pIndelRealigner ,   pBaseRecalibrator  pPrintReads . exportdir              =   exdir  pMarkDuplicates . args . params . tmpdir     =   /local2/tmp/  pAlignPEByBWA . args . reffile             =   reffile  pRealignerTargetCreator . args . reffile   =   reffile  pRealignerTargetCreator . args . params . tmpdir = /local2/tmp/  pIndelRealigner . args . reffile           =   reffile  pIndelRealigner . args . params . tmpdir     =   /local2/tmp/  pBaseRecalibrator . args . reffile         =   reffile  pBaseRecalibrator . args . knownSites      =   dbsnp  pBaseRecalibrator . args . params . tmpdir   = /local2/tmp/  pPrintReads . args . reffile               =   reffile  pPrintReads . args . params                =   /local2/tmp/  PyPPL ({ \n     proc :   { \n         forks :   100 , \n         runner :   sge , \n         sgeRunner :   { \n             sge.q :   lg-mem \n         } \n     }  }) . start ( pTrimmomaticPE ) . run ()  \nThis is a very commonly used Whole Genome Sequencing data cleanup pipeline from the raw reads according to the  GATK best practice . And it will be used pretty much every time when the raw read files come.   With an aggregation defined, you don't need to configure and call those processes every time: from   pyppl   import   Aggr  from   bioprocs   import   params  # some paramters defined in params  aFastqPE2Bam   =   Aggr   (  \u00a0\u00a0\u00a0\u00a0 pTrimmomaticPE ,  \u00a0\u00a0\u00a0\u00a0 pAlignPEByBWA ,  \u00a0\u00a0\u00a0\u00a0 pSortSam ,  \u00a0\u00a0\u00a0\u00a0 pMarkDuplicates ,  \u00a0\u00a0\u00a0\u00a0 pIndexBam ,  \u00a0\u00a0\u00a0\u00a0 pRealignerTargetCreator ,  \u00a0\u00a0\u00a0\u00a0 pIndelRealigner ,  \u00a0\u00a0\u00a0\u00a0 pBaseRecalibrator ,  \u00a0\u00a0\u00a0\u00a0 pPrintReads  )  # dependency adjustment  aFastqPE2Bam . pIndelRealigner . depends     =   aFastqPE2Bam . pIndexBam ,   aFastqPE2Bam . pRealignerTargetCreator  aFastqPE2Bam . pPrintReads . depends         =   aFastqPE2Bam . pIndelRealigner ,   aFastqPE2Bam . pBaseRecalibrator  # input adjustment  # args adjustment  aFastqPE2Bam . pMarkDuplicates . args . params . tmpdir         =   params . tmpdir  aFastqPE2Bam . pAlignPEByBWA . args . reffile                 =   params . hg19fa  aFastqPE2Bam . pRealignerTargetCreator . args . reffile       =   params . hg19fa  aFastqPE2Bam . pRealignerTargetCreator . args . params . tmpdir =   params . tmpdir  aFastqPE2Bam . pIndelRealigner . args . reffile               =   params . hg19fa  aFastqPE2Bam . pIndelRealigner . args . params . tmpdir         =   params . tmpdir  aFastqPE2Bam . pBaseRecalibrator . args . reffile             =   params . hg19fa  aFastqPE2Bam . pBaseRecalibrator . args . knownSites          =   params . dbsnp  aFastqPE2Bam . pBaseRecalibrator . args . params . tmpdir       =   params . tmpdir  aFastqPE2Bam . pPrintReads . args . reffile                   =   params . hg19fa  aFastqPE2Bam . pPrintReads . args . params . tmpdir             =   params . tmpdir   Then every time you just need to call the aggregation: aFastqPE2Bam . input   =   channel . fromPairs   (   datadir   +   /*.fastq.gz   )  aFastqPE2Bam . exdir   =   exdir  PyPPL ({  \u00a0\u00a0\u00a0\u00a0 proc :   {  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 sgeRunner :   {  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 sge.q   :   1-day  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }  \u00a0\u00a0\u00a0\u00a0 }  }) . start ( aFastqPE2Bam ) . run ()", 
            "title": "Aggregations"
        }, 
        {
            "location": "/aggregations/#initialize-an-aggregation", 
            "text": "Like previous example shows, you just need to give the constructor all the processes to construct an aggretation. However, there are several things need to be noticed:   The dependencies are automatically constructed by the order of the processes. \n    a   =   Aggr   ( p1 ,   p2 ,   p3 )  # The dependencies will be p1 -  p2 -  p3   The starting and ending processes are defined as the first and last processes, respectively. If you need to modify the dependencies, keep that in mind whether the starting and ending processes are changed.\n     a   =   Aggr   ( p1 ,   p2 ,   p3 )  # a.starts == [p1]  # a.ends   == [p3]  #                                / p2  # change the dependencies to  p1        #                                \\ p3  # both p2, p3 depend on p1, and p3 depends on p2  a . p3 . depends   =   p1 ,   p2  # but remember the ending processes are changed from [p3] to [p2, p3]  a . ends   =   [ p2 ,   p3 ]  \n    You can also specify the dependencies manually:\n     a   =   Aggr   ( p1 ,   p2 ,   p3 ,   depends   =   False )  a . p2 . depends   =   p1  a . p3 . depends   =   p1 ,   p2  a . starts   =   [ p1 ]  a . ends     =   [ p2 ,   p3 ]   If you have one process used twice in the aggregation, copy it with a different id:\n    a   =   Aggr ( \n     p1 , \n     p2 , \n     p1 . copy ( id   =   p1copy )  )  # then to access the 2nd p1: a.p1copy   Each process is copied by aggregation, so the original one can still be used.  The tag of each process is regenerated by the id of the aggregation.", 
            "title": "Initialize an aggregation"
        }, 
        {
            "location": "/aggregations/#add-a-process-on-the-run", 
            "text": "aggr.addProc(p, where=None) \nYou can add a process, and also define whether to put it in  starts ,  ends ,  both  or  None .", 
            "title": "Add a process on the run"
        }, 
        {
            "location": "/aggregations/#delegate-attributes-of-processes-to-an-aggregation", 
            "text": "You can Delegate the attributes directly for a process to an aggregation: aFastqPE2Bam . delegate ( args.reffile ,   pAlignPEByBWA )  \nThen when you want to set  args.reffile  for  pAlignPEByBWA , you can just do: aFastqPE2Bam . args . reffile   =   /path/to/hg19.fa  \nYou may use  starts/ends  represents the start/end processes.    Delegate an attribute to multiple processes: aFastqPE2Bam . delegate ( args.reffile ,   pAlignPEByBWA, pPrintReads )  # or  aFastqPE2Bam . delegate ( args.reffile ,   [ pAlignPEByBWA ,   pPrintReads ])  # or  aFastqPE2Bam . delegate ( args.reffile ,   [ aFastqPE2Bam . pAlignPEByBWA ,   aFastqPE2Bam . pPrintReads ])   Delegate multiple attributes at one time: aFastqPE2Bam . delegate ( args.reffile, args.tmpdir ,   pAlignPEByBWA, pPrintReads )  # or  aFastqPE2Bam . delegate ([ args.reffile ,   args.tmpdir ],   pAlignPEByBWA, pPrintReads )    Caution  Undelegated attributes will be delegated to all processes. But remember if an attribute has been set before, say  p.runner = 'local'  then,  aggr.runner  will not overwrite it if  runner  is not delegated. If it is, then it'll be overwritten.    Note  For attributes that have sub-attributes (i.e.  p.args.params.inopts.cnames ), you may just delegate the first parts, then the full assignment of the attribute will still follow the delegation. For example:  aggr . delegate ( args.params ,   p1,p2 )  aggr . args . params . inopts . cnames   =   True   # only affects p1, p2  \nKeep in mind that shorter delegations always come first. In the above case, if we have another delegation:  aggr.delegate('args.params.inopts', 'p3') , then the assignment will still affect  p1, p2  (the first delegation) and  p3  (the second delegation).", 
            "title": "Delegate attributes of processes to an aggregation"
        }, 
        {
            "location": "/aggregations/#default-delegations", 
            "text": "By default,  input/depends  are delegated for start processes, and  exdir/exhow/exow/expart  for end processes. Importantly, as  aggr.starts  is a list, the values for  input/depends  must be a list as well, with elements corresponing to each start process. \nBesides, we have two special attributes for aggregations:  input2  and  depends2 . Unlike  input  and  depends ,  input2  and  depends2  try to pass everything it gets to each process, instead of passing corresponind element to each process. For example: # aggr.starts = [aggr.p1, aggr.p2]  aggr . input   =   [[ a ],   [ b ]]  # then:  # aggr.p1.config[ input ] == [ a ]  # aggr.p2.config[ input ] == [ b ]  aggr . input2   =   [[ a ],   [ b ]]  # then:  # aggr.p1.config[ input ] == [[ a ], [ b ]]  # aggr.p2.config[ input ] == [[ a ], [ b ]]", 
            "title": "Default delegations"
        }, 
        {
            "location": "/aggregations/#set-attribute-value-for-specific-processes-of-an-aggregation", 
            "text": "There are several ways to do that: # refer to the process directly  aFastqPE2Bam . pPrintReads . args . tmpdir   =   /tmp  aFastqPE2Bam . pPrintReads . runner   =   local  # refer to the index of the process  aFastqPE2Bam . args [ 8 ] . tmpdir   =   /tmp  aFastqPE2Bam . runner [ 8 ]   =   local  # refer to the name of the process  aFastqPE2Bam . args [ pPrintReads ] . tmpdir   =   /tmp  aFastqPE2Bam . runner [ pPrintReads ]   =   local  # for multiple processes  aFastqPE2Bam . args [: 3 ] . tmpdir   =   /tmp  aFastqPE2Bam . args [ 0 , 1 , 3 ] . tmpdir   =   /tmp  aFastqPE2Bam . args [ aFastqPE2Bam ,   pPrintReads ] . tmpdir   =   /tmp  aFastqPE2Bam . args [ aFastqPE2Bam, pPrintReads ] . tmpdir   =   /tmp  # or you may use starts/ends to refer to the start/end processes  # has to be done after aggr.starts/aggr.ends assigned   # or initialized with depends = True  aFastqPE2Bam . args [ starts ] . tmpdir   =   /tmp  aFastqPE2Bam . args [ ends ] . tmpdir   =   /tmp    Hint  If an attribute is delegated for other processes, you can still set the value of it by the above methods.", 
            "title": "Set attribute value for specific processes of an aggregation"
        }, 
        {
            "location": "/aggregations/#set-an-aggregation-as-start-aggregation-for-a-pipeline", 
            "text": "You can do it just like setting a process as the starting process of pipeline (see  here ). Actually the starting processes in the aggregation ( aggr.starts ) will be set as the starting processes of the pipeline.", 
            "title": "Set an aggregation as start aggregation for a pipeline"
        }, 
        {
            "location": "/aggregations/#the-dependency-of-aggregations-and-processes", 
            "text": "An aggregation can depend on aggregations and/or processes, you just treat the aggregations as processes. A process can also depend on aggregations and/or processes.      What am I?  Whom I am depending on?  Real relations      Aggr  ( a1 )  Aggr  ( a2 )  a1.starts  depends on  a2.ends    Proc  ( p )  Aggr  ( a )  p  depends on  a.ends      Note  You have to specify  depends  for start processes of an aggregation.", 
            "title": "The dependency of aggregations and processes"
        }, 
        {
            "location": "/aggregations/#copy-an-aggregation", 
            "text": "Aggr.copy(tag = 'notag', copyDeps = True, newid = None) \nYou may copy an aggregation, all the processes in the aggregation will be copied, and the dependencies will be switched to the corresponding copied processes, as well as the starting and ending processes, if  copyDeps == True .   You can keep the ids of processes unchanged but give a new tag and also give the aggregation an new id instead of the variable name: a   =   Aggr ( p1 ,   p2 ,   p3 )  # access the processes:  # a.p1, a.p2, a.p3  a2   =   a . copy ( copied )  # a2.procs == [  #  proc with id  p1  and tag  copied ,  #  proc with id  p2  and tag  copied ,  #  proc with id  p3  and tag  copied ,  # ]  # a2.id ==  a2  # to access the processes:  # a2.p1, a2.p2, a2.p3  a2   =   a . copy ( copied ,   id   =   newAggr )  # a2.id ==  newAggr", 
            "title": "Copy an aggregation"
        }, 
        {
            "location": "/command-line-tool/", 
            "text": "Command line tool\n\n\n\n\n\nWhen you are debuggin a processes, specially when you are modify input, output and script, the suffix a process will change. Then there will be several \nworkdir\n created in \nppldir\n. The command line tool helps to maintain and clean up the \nworkdir\ns\n\n\n bin/pyppl --help\nusage: pyppl \n[\n-h\n]\n \n[\n-w WORKDIR\n]\n \n{\nclean,list,compare\n}\n ...\n\nA \nset\n of CLI tools \nfor\n pyppl.\n\npositional arguments:\n  \n{\nclean,list,compare\n}\n\n    clean               Clean a workdir\n    list                List the status of a workdir\n    compare             Compare the settings of two processes.\n\noptional arguments:\n  -h, --help            show this \nhelp\n message and \nexit\n\n  -w WORKDIR, --workdir WORKDIR\n                        The path of workdir, default: ./workdir\n\n\n\n\npyppl\n has a common option \n-w\n or \n--workdir\n to specify the work directory with the  processes you want to list, clean or compare. By default, it looks at the processes in \n./workdir\n\n\nList processes\n\n\n\n\npyppl list\n command will list the processes in\n./workdir\n. It will group the processes with same \nid\n and \ntag\n, and compare their time start to run. The latest one will show at the first place, follows the second latest, ... If a \nproc.settings\n cannot be found in the process directory, it will be shown in red.\n\n\nClean processes\n\n\n\n\npyppl list\n command will ask whether you want to remove the process directory for the older processes with the same \nid\n and \ntag\n.\n\n\nYou can remove all those older process directories without confirmation by \npyppl clean --force\n\n\n\n\nCaution\n\n\nBe careful when you have multiple pipelines in \n./workdir\n. \n\n\nFor a single pipeline, it does allow you have processes with same \nid\n and \ntag\n. However, for multiple pipelines, you may have. And if the two processes have the same \nppldir\n (i.e. \n./workdir\n), they will have directories with the same \nid\n and \ntag\n, but different \nsuffix\n. In this case, if you use \npyppl clean --force\n, it will only try to keep only the latest one.\n\n\nSo the best way is to set different tags for the processes with same ids in different pipelines.\nOr you can do it without \n--force\n and keep the ones you want.\n\n\n\n\nCompare the settings of two pipeines\n\n\n\n\npyppl compare\n uses python's \ndifflib\n to compare the \nproc.settings\n files in the directories of two processes. it can take a process group name (i.e. \n-p pSort.notag\n, in this case, actually, the tag can be omitted if it is \nnotag\n, so you can use \n-p pSort\n) to compare the top 2 latest processes or two process names with suffices (i.e. \n-p1 pSort.notag.4HIhyVbp -p2 pSort.notag.7hNBe2uT\n. \n\n\nYou can also specify the direct path of process groups/directories:\n\npyppl -p ./workdir/PyPPL.pSort.notag \n\n# or mixed\n\npyppl -p1 ./workdir/PyPPL.pSort.notag.4HIhyVbp -p2 pSort.notag.7hNBe2uT \n\n\nThe direct path will ignore the \nworkdir\n specified by \n-w\n.", 
            "title": "Command line tool"
        }, 
        {
            "location": "/command-line-tool/#command-line-tool", 
            "text": "When you are debuggin a processes, specially when you are modify input, output and script, the suffix a process will change. Then there will be several  workdir  created in  ppldir . The command line tool helps to maintain and clean up the  workdir s   bin/pyppl --help\nusage: pyppl  [ -h ]   [ -w WORKDIR ]   { clean,list,compare }  ...\n\nA  set  of CLI tools  for  pyppl.\n\npositional arguments:\n   { clean,list,compare } \n    clean               Clean a workdir\n    list                List the status of a workdir\n    compare             Compare the settings of two processes.\n\noptional arguments:\n  -h, --help            show this  help  message and  exit \n  -w WORKDIR, --workdir WORKDIR\n                        The path of workdir, default: ./workdir  pyppl  has a common option  -w  or  --workdir  to specify the work directory with the  processes you want to list, clean or compare. By default, it looks at the processes in  ./workdir", 
            "title": "Command line tool"
        }, 
        {
            "location": "/command-line-tool/#list-processes", 
            "text": "pyppl list  command will list the processes in ./workdir . It will group the processes with same  id  and  tag , and compare their time start to run. The latest one will show at the first place, follows the second latest, ... If a  proc.settings  cannot be found in the process directory, it will be shown in red.", 
            "title": "List processes"
        }, 
        {
            "location": "/command-line-tool/#clean-processes", 
            "text": "pyppl list  command will ask whether you want to remove the process directory for the older processes with the same  id  and  tag .  You can remove all those older process directories without confirmation by  pyppl clean --force   Caution  Be careful when you have multiple pipelines in  ./workdir .   For a single pipeline, it does allow you have processes with same  id  and  tag . However, for multiple pipelines, you may have. And if the two processes have the same  ppldir  (i.e.  ./workdir ), they will have directories with the same  id  and  tag , but different  suffix . In this case, if you use  pyppl clean --force , it will only try to keep only the latest one.  So the best way is to set different tags for the processes with same ids in different pipelines.\nOr you can do it without  --force  and keep the ones you want.", 
            "title": "Clean processes"
        }, 
        {
            "location": "/command-line-tool/#compare-the-settings-of-two-pipeines", 
            "text": "pyppl compare  uses python's  difflib  to compare the  proc.settings  files in the directories of two processes. it can take a process group name (i.e.  -p pSort.notag , in this case, actually, the tag can be omitted if it is  notag , so you can use  -p pSort ) to compare the top 2 latest processes or two process names with suffices (i.e.  -p1 pSort.notag.4HIhyVbp -p2 pSort.notag.7hNBe2uT .   You can also specify the direct path of process groups/directories: pyppl -p ./workdir/PyPPL.pSort.notag  # or mixed \npyppl -p1 ./workdir/PyPPL.pSort.notag.4HIhyVbp -p2 pSort.notag.7hNBe2uT  \nThe direct path will ignore the  workdir  specified by  -w .", 
            "title": "Compare the settings of two pipeines"
        }, 
        {
            "location": "/api/", 
            "text": "API\n\n\n\n\n\nModule \nPyPPL\n\n\n\n\nThe PyPPL class\n\n\n\n\n@static variables:\n    `TIPS`: The tips for users\n    `RUNNERS`: Registered runners\n    `DEFAULT_CFGFILES`: Default configuration file\n\n\n\n__init__ (self, config, cfgfile)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\nconfig\n: the configurations for the pipeline, default: {}\n\n\ncfgfile\n:  the configuration file for the pipeline, default: \n~/.PyPPL.json\n or \n./.PyPPL\n  \n\n\n\n\n_any2procs (*args) [@staticmethod]\n\n\nGet procs from anything (aggr.starts, proc, procs, proc names)  \n\n\n\n\n\n\nparams:\n\n\narg\n: anything  \n\n\n\n\n\n\nreturns:\n\nA set of procs  \n\n\n\n\n\n\n_checkProc (proc) [@staticmethod]\n\n\nCheck processes, whether 2 processes have the same id and tag  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The process  \n\n\n\n\n\n\nreturns:\n\nIf there are 2 processes with the same id and tag, raise \nValueError\n.  \n\n\n\n\n\n\n_registerProc (proc) [@staticmethod]\n\n\nRegister the process  \n\n\n\n\nparams:\n\n\nproc\n: The process  \n\n\n\n\n_resume (self, *args, **kwargs)\n\n\nMark processes as to be resumed  \n\n\n\n\nparams:\n\n\nargs\n: the processes to be marked. The last element is the mark for processes to be skipped.  \n\n\n\n\nflowchart (self, fcfile, dotfile)\n\n\nGenerate graph in dot language and visualize it.  \n\n\n\n\nparams:\n\n\ndotfile\n: Where to same the dot graph. Default: \nNone\n (\npath.splitext(sys.argv[0])[0] + \".pyppl.dot\"\n)\n\n\nfcfile\n:  The flowchart file. Default: \nNone\n (\npath.splitext(sys.argv[0])[0] + \".pyppl.svg\"\n)  \n\n\n\n\nFor example: run \npython pipeline.py\n will save it to \npipeline.pyppl.svg\n\n\ndot\n:     The dot visulizer. Default: \"dot -Tsvg {{dotfile}} \n {{fcfile}}\"  \n\n\n\n\n\n\nreturns:\n\nThe pipeline object itself.  \n\n\n\n\n\n\nregisterRunner (runner) [@staticmethod]\n\n\nRegister a runner  \n\n\n\n\nparams:\n\n\nrunner\n: The runner to be registered.  \n\n\n\n\nresume (self, *args)\n\n\nMark processes as to be resumed  \n\n\n\n\n\n\nparams:\n\n\nargs\n: the processes to be marked  \n\n\n\n\n\n\nreturns:\n\nThe pipeline object itself.  \n\n\n\n\n\n\nresume2 (self, *args)\n\n\nMark processes as to be resumed  \n\n\n\n\n\n\nparams:\n\n\nargs\n: the processes to be marked  \n\n\n\n\n\n\nreturns:\n\nThe pipeline object itself.  \n\n\n\n\n\n\nrun (self, profile)\n\n\nRun the pipeline  \n\n\n\n\n\n\nparams:\n\n\nprofile\n: the profile used to run, if not found, it'll be used as runner name. default: 'default'  \n\n\n\n\n\n\nreturns:\n\nThe pipeline object itself.  \n\n\n\n\n\n\nshowAllRoutes (self)\n\n\nstart (self, *args)\n\n\nSet the starting processes of the pipeline  \n\n\n\n\n\n\nparams:\n\n\nargs\n: the starting processes  \n\n\n\n\n\n\nreturns:\n\nThe pipeline object itself.  \n\n\n\n\n\n\nModule \nProc\n\n\n\n\nThe Proc class defining a process\n\n\n\n\n@static variables:\n    `RUNNERS`:       The regiested runners\n    `ALIAS`:         The alias for the properties\n    `LOG_NLINE`:     The limit of lines of logging information of same type of messages\n\n@magic methods:\n    `__getattr__(self, name)`: get the value of a property in `self.props`\n    `__setattr__(self, name, value)`: set the value of a property in `self.config`\n\n\n\n__init__ (self, tag, desc, id)\n\n\nConstructor  \n\n\n\n\n\n\nparams:\n\n\ntag\n:  The tag of the process\n\n\ndesc\n: The description of the process\n\n\nid\n:   The identify of the process  \n\n\n\n\n\n\nconfig:\n\nid, input, output, ppldir, forks, cache, cclean, rc, echo, runner, script, depends, tag, desc, dirsig\n\nexdir, exhow, exow, errhow, errntry, lang, beforeCmd, afterCmd, workdir, args, aggr\n\ncallfront, callback, expect, expart, template, tplenvs, resume, nthread  \n\n\n\n\n\n\nprops\n\ninput, output, rc, echo, script, depends, beforeCmd, afterCmd, workdir, expect\n\nexpart, template, channel, jobs, ncjobids, size, sets, procvars, suffix, logs  \n\n\n\n\n\n\n_buildInput (self)\n\n\nBuild the input data\n\nInput could be:\n\n1. list: ['input', 'infile:file'] \n=\n ['input:var', 'infile:path']\n\n2. str : \"input, infile:file\" \n=\n input:var, infile:path\n\n3. dict: {\"input\": channel1, \"infile:file\": channel2}\n\nor    {\"input:var, input:file\" : channel3}\n\nfor 1,2 channels will be the combined channel from dependents, if there is not dependents, it will be sys.argv[1:]  \n\n\n_buildJobs (self)\n\n\nBuild the jobs.  \n\n\n_buildOutput (self)\n\n\nBuild the output data templates waiting to be rendered.  \n\n\n_buildProcVars (self)\n\n\nBuild proc attribute values for template rendering,\n\nand also echo some out.  \n\n\n_buildProps (self)\n\n\nCompute some properties  \n\n\n_buildScript (self)\n\n\nBuild the script template waiting to be rendered.  \n\n\n_checkCached (self)\n\n\nTell whether the jobs are cached  \n\n\n\n\nreturns:\n\nTrue if all jobs are cached, otherwise False  \n\n\n\n\n_readConfig (self, profile, profiles)\n\n\nRead the configuration  \n\n\n\n\nparams:\n\n\nconfig\n: The configuration  \n\n\n\n\n_runCmd (self, key)\n\n\nRun the \nbeforeCmd\n or \nafterCmd\n  \n\n\n\n\n\n\nparams:\n\n\nkey\n: \"beforeCmd\" or \"afterCmd\"  \n\n\n\n\n\n\nreturns:\n\nThe return code of the command  \n\n\n\n\n\n\n_runJobs (self)\n\n\nSubmit and run the jobs  \n\n\n_saveSettings (self)\n\n\nSave all settings in proc.settings, mostly for debug  \n\n\n_suffix (self)\n\n\nCalcuate a uid for the process according to the configuration\n\nThe philosophy:\n\n1. procs from different script must have different suffix (sys.argv[0])\n\n2. procs from the same script:\n\n- procs with different id or tag have different suffix\n\n- procs with different input have different suffix (depends, input)  \n\n\n\n\nreturns:\n\nThe uniq id of the process  \n\n\n\n\n_tidyAfterRun (self)\n\n\nDo some cleaning after running jobs\n\nself.resume can only be:\n\n- '': normal process\n\n- skip+: skipped process but required workdir and data exists\n\n- resume: resume pipeline from this process, no requirement\n\n- resume+: get data from workdir/proc.settings, and resume  \n\n\n_tidyBeforeRun (self)\n\n\nDo some preparation before running jobs  \n\n\ncopy (self, tag, desc, id)\n\n\nCopy a process  \n\n\n\n\n\n\nparams:\n\n\nid\n: The new id of the process, default: \nNone\n (use the varname)\n\n\ntag\n:   The tag of the new process, default: \nNone\n (used the old one)\n\n\ndesc\n:  The desc of the new process, default: \nNone\n (used the old one)  \n\n\n\n\n\n\nreturns:\n\nThe new process  \n\n\n\n\n\n\nlog (self, msg, level, key)\n\n\nThe log function with aggregation name, process id and tag integrated.  \n\n\n\n\nparams:\n\n\nmsg\n:   The message to log\n\n\nlevel\n: The log level\n\n\nkey\n:   The type of messages  \n\n\n\n\nname (self, aggr)\n\n\nGet my name include \naggr\n, \nid\n, \ntag\n  \n\n\n\n\nreturns:\n\nthe name  \n\n\n\n\nrun (self, profile, profiles)\n\n\nRun the jobs with a configuration  \n\n\n\n\nparams:\n\n\nconfig\n: The configuration  \n\n\n\n\nModule \nChannel\n\n\n\n\nThe channen class, extended from \nlist\n\n\n\n\n_tuplize (tu) [@staticmethod]\n\n\nA private method, try to convert an element to tuple\n\nIf it's a string, convert it to \n(tu, )\n\nElse if it is iterable, convert it to \ntuple(tu)\n\nOtherwise, convert it to \n(tu, )\n\nNotice that string is also iterable.  \n\n\n\n\n\n\nparams:\n\n\ntu\n: the element to be converted  \n\n\n\n\n\n\nreturns:\n\nThe converted element  \n\n\n\n\n\n\nattach (self, *names, **kwargs)\n\n\nAttach columns to names of Channel, so we can access each column by:\n\n\nch.col0\n == ch.colAt(0)  \n\n\n\n\nparams:\n\n\nnames\n: The names. Have to be as length as channel's width. None of them should be Channel's property name\n\n\nflatten\n: Whether flatten the channel for the name being attached  \n\n\n\n\ncbind (self, *cols)\n\n\nAdd columns to the channel  \n\n\n\n\n\n\nparams:\n\n\ncols\n: The columns  \n\n\n\n\n\n\nreturns:\n\nThe channel with the columns inserted.  \n\n\n\n\n\n\ncolAt (self, index)\n\n\nFetch one column of a Channel  \n\n\n\n\n\n\nparams:\n\n\nindex\n: which column to fetch  \n\n\n\n\n\n\nreturns:\n\nThe Channel with that column  \n\n\n\n\n\n\ncollapse (self, col)\n\n\nDo the reverse of expand\n\nlength: N -\n 1\n\nwidth:  M -\n M  \n\n\n\n\n\n\nparams:\n\n\ncol\n:     the index of the column used to collapse  \n\n\n\n\n\n\nreturns:\n\nThe collapsed Channel  \n\n\n\n\n\n\ncopy (self)\n\n\nCopy a Channel using \ncopy.copy\n  \n\n\n\n\nreturns:\n\nThe copied Channel  \n\n\n\n\ncreate (l) [@staticmethod]\n\n\nCreate a Channel from a list  \n\n\n\n\n\n\nparams:\n\n\nl\n: The list, default: []  \n\n\n\n\n\n\nreturns:\n\nThe Channel created from the list  \n\n\n\n\n\n\nexpand (self, col, pattern, t, sortby, reverse)\n\n\nexpand the Channel according to the files in \n, other cols will keep the same\n\n\n[(dir1/dir2, 1)].expand (0, \"*\")\n will expand to\n\n\n[(dir1/dir2/file1, 1), (dir1/dir2/file2, 1), ...]\n\nlength: 1 -\n N\n\nwidth:  M -\n M  \n\n\n\n\nparams:\n\n\ncol\n:     the index of the column used to expand\n\n\npattern\n: use a pattern to filter the files/dirs, default: \n*\n\n\nt\n:       the type of the files/dirs to include  \n\n\n'dir', 'file', 'link' or 'any' (default)\n\n\nsortby\n:  how the list is sorted  \n\n\n\n\n'name' (default), 'mtime', 'size'\n\n\nreverse\n: reverse sort. Default: False  \n\n\n\n\n\n\nreturns:\n\nThe expanded Channel  \n\n\n\n\n\n\nfilter (self, func)\n\n\nAlias of python builtin \nfilter\n  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: the function. Default: None  \n\n\n\n\n\n\nreturns:\n\nThe filtered Channel  \n\n\n\n\n\n\nfilterCol (self, func, col)\n\n\nJust filter on the first column  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: the function\n\n\ncol\n: the column to filter  \n\n\n\n\n\n\nreturns:\n\nThe filtered Channel  \n\n\n\n\n\n\nflatten (self, col)\n\n\nConvert a single-column Channel to a list (remove the tuple signs)\n\n\n[(a,), (b,)]\n to \n[a, b]\n  \n\n\n\n\n\n\nparams:\n\n\ncol\n: The column to flat. None for all columns (default)  \n\n\n\n\n\n\nreturns:\n\nThe list converted from the Channel.  \n\n\n\n\n\n\nfold (self, n)\n\n\nFold a Channel. Make a row to n-length chunk rows\n\n\na1\ta2\ta3\ta4  \nb1\tb2\tb3\tb4  \nif n==2, fold(2) will change it to:  \na1\ta2  \na3\ta4  \nb1\tb2  \nb3\tb4  \n\n\n\n\n\n\n\nparams:\n\n\nn\n: the size of the chunk  \n\n\n\n\n\n\nreturns\n\nThe new Channel  \n\n\n\n\n\n\nfromArgv () [@staticmethod]\n\n\nCreate a Channel from \nsys.argv[1:]\n\n\"python test.py a b c\" creates a width=1 Channel\n\n\"python test.py a,1 b,2 c,3\" creates a width=2 Channel  \n\n\n\n\nreturns:\n\nThe Channel created from the command line arguments  \n\n\n\n\nfromChannels (*args) [@staticmethod]\n\n\nCreate a Channel from Channels  \n\n\n\n\n\n\nparams:\n\n\nargs\n: The Channels  \n\n\n\n\n\n\nreturns:\n\nThe Channel merged from other Channels  \n\n\n\n\n\n\nfromFile (fn, header, skip, delimit) [@staticmethod]\n\n\nCreate Channel from the file content\n\nIt's like a matrix file, each row is a row for a Channel.\n\nAnd each column is a column for a Channel.  \n\n\n\n\nparams:\n\n\nfn\n:      the file\n\n\nheader\n:  Whether the file contains header. If True, will attach the header  \n\n\n\n\nSo you can use \nchannel.\nheader\n to fetch the column\n\n\nskip\n:    first lines to skip\n\n\ndelimit\n: the delimit for columns  \n\n\n\n\n\n\nreturns:\n\nA Channel created from the file  \n\n\n\n\n\n\nfromPairs (pattern) [@staticmethod]\n\n\nCreate a width = 2 Channel from a pattern  \n\n\n\n\n\n\nparams:\n\n\npattern\n: the pattern  \n\n\n\n\n\n\nreturns:\n\nThe Channel create from every 2 files match the pattern  \n\n\n\n\n\n\nfromParams (*pnames) [@staticmethod]\n\n\nCreate a Channel from params  \n\n\n\n\n\n\nparams:\n\n\n*pnames\n: The names of the option  \n\n\n\n\n\n\nreturns:\n\nThe Channel  \n\n\n\n\n\n\nfromPattern (pattern, t, sortby, reverse) [@staticmethod]\n\n\nCreate a Channel from a path pattern  \n\n\n\n\nparams:\n\n\npattern\n: the pattern with wild cards\n\n\nt\n:       the type of the files/dirs to include  \n\n\n'dir', 'file', 'link' or 'any' (default)\n\n\nsortby\n:  how the list is sorted  \n\n\n\n\n'name' (default), 'mtime', 'size'\n\n\nreverse\n: reverse sort. Default: False  \n\n\n\n\n\n\nreturns:\n\nThe Channel created from the path  \n\n\n\n\n\n\nget (self, idx)\n\n\nGet the element of a flattened channel  \n\n\n\n\n\n\nparams:\n\n\nidx\n: The index of the element to get. Default: 0  \n\n\n\n\n\n\nreturn:\n\nThe element  \n\n\n\n\n\n\ninsert (self, cidx, *cols)\n\n\nInsert columns to a channel  \n\n\n\n\n\n\nparams:\n\n\ncidx\n: Insert into which index of column?\n\n\ncols\n: the columns to be bound to Channel  \n\n\n\n\n\n\nreturns:\n\nThe combined Channel\n\nNote, self is also changed  \n\n\n\n\n\n\nlength (self)\n\n\nGet the length of a Channel\n\nIt's just an alias of \nlen(chan)\n  \n\n\n\n\nreturns:\n\nThe length of the Channel  \n\n\n\n\nmap (self, func)\n\n\nAlias of python builtin \nmap\n  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: the function  \n\n\n\n\n\n\nreturns:\n\nThe transformed Channel  \n\n\n\n\n\n\nmapCol (self, func, col)\n\n\nMap for a column  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: the function\n\n\ncol\n: the index of the column. Default: 0  \n\n\n\n\n\n\nreturns:\n\nThe transformed Channel  \n\n\n\n\n\n\nrbind (self, *rows)\n\n\nThe multiple-argument versoin of \nrbind\n  \n\n\n\n\n\n\nparams:\n\n\nrows\n: the rows to be bound to Channel  \n\n\n\n\n\n\nreturns:\n\nThe combined Channel\n\nNote, self is also changed  \n\n\n\n\n\n\nreduce (self, func)\n\n\nAlias of python builtin \nreduce\n  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: the function  \n\n\n\n\n\n\nreturns:\n\nThe reduced value  \n\n\n\n\n\n\nreduceCol (self, func, col)\n\n\nReduce a column  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: the function\n\n\ncol\n: the column to reduce  \n\n\n\n\n\n\nreturns:\n\nThe reduced value  \n\n\n\n\n\n\nrepCol (self, n)\n\n\nRepeat column and return a new channel  \n\n\n\n\n\n\nparams:\n\n\nn\n: how many times to repeat.  \n\n\n\n\n\n\nreturns:\n\nThe new channel with repeated columns  \n\n\n\n\n\n\nrepRow (self, n)\n\n\nRepeat row and return a new channel  \n\n\n\n\n\n\nparams:\n\n\nn\n: how many times to repeat.  \n\n\n\n\n\n\nreturns:\n\nThe new channel with repeated rows  \n\n\n\n\n\n\nrowAt (self, index)\n\n\nFetch one row of a Channel  \n\n\n\n\n\n\nparams:\n\n\nindex\n: which row to fetch  \n\n\n\n\n\n\nreturns:\n\nThe Channel with that row  \n\n\n\n\n\n\nslice (self, start, length)\n\n\nFetch some columns of a Channel  \n\n\n\n\n\n\nparams:\n\n\nstart\n:  from column to start\n\n\nlength\n: how many columns to fetch, default: None (from start to the end)  \n\n\n\n\n\n\nreturns:\n\nThe Channel with fetched columns  \n\n\n\n\n\n\nsplit (self, flatten)\n\n\nSplit a Channel to single-column Channels  \n\n\n\n\nreturns:\n\nThe list of single-column Channels  \n\n\n\n\nunfold (self, n)\n\n\nDo the reverse thing as self.fold does  \n\n\n\n\n\n\nparams:\n\n\nn\n: How many rows to combind each time. default: 2  \n\n\n\n\n\n\nreturns:\n\nThe unfolded Channel  \n\n\n\n\n\n\nunique (self)\n\n\nMake the channel unique, remove duplicated rows\n\nTry to keep the order  \n\n\nwidth (self)\n\n\nGet the width of a Channel  \n\n\n\n\nreturns:\n\nThe width of the Channel  \n\n\n\n\nModule \nJob\n\n\n\n\nJob class, defining a job in a process\n\n\n\n\n__init__ (self, index, proc)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\nindex\n:   The index of the job in a process\n\n\nproc\n:    The process  \n\n\n\n\n_indexIndicator (self)\n\n\nGet the index indicator in the log  \n\n\n\n\nreturns:\n\nThe \"[001/100]\" like indicator  \n\n\n\n\n_linkInfile (self, orgfile)\n\n\nCreate links for input files  \n\n\n\n\n\n\nparams:\n\n\norgfile\n: The original input file  \n\n\n\n\n\n\nreturns:\n\nThe link to the original file.  \n\n\n\n\n\n\n_prepInput (self)\n\n\nPrepare input, create link to input files and set other placeholders  \n\n\n_prepOutput (self)\n\n\nBuild the output data.\n\nOutput could be:\n\n1. list: \n['output:var:{{input}}', 'outfile:file:{{infile.bn}}.txt']\n\nor you can ignore the name if you don't put it in script:\n\n\n['var:{{input}}', 'path:{{infile.bn}}.txt']\n\nor even (only var type can be ignored):\n\n\n['{{input}}', 'file:{{infile.bn}}.txt']\n\n2. str : \n'output:var:{{input}}, outfile:file:{{infile.bn}}.txt'\n\n3. OrderedDict: \n{\"output:var:{{input}}\": channel1, \"outfile:file:{{infile.bn}}.txt\": channel2}\n\nor    \n{\"output:var:{{input}}, output:file:{{infile.bn}}.txt\" : channel3}\n\nfor 1,2 channels will be the property channel for this proc (i.e. p.channel)  \n\n\n_prepScript (self)\n\n\nBuild the script, interpret the placeholders  \n\n\n_reportItem (self, key, maxlen, data, loglevel)\n\n\nReport the item on logs  \n\n\n\n\nparams:\n\n\nkey\n: The key of the item\n\n\nmaxlen\n: The max length of the key\n\n\ndata\n: The data of the item\n\n\nloglevel\n: The log level  \n\n\n\n\ncache (self)\n\n\nTruly cache the job (by signature)  \n\n\ncheckOutfiles (self, expect)\n\n\nCheck whether output files are generated, if not, add - to rc.  \n\n\ndone (self)\n\n\nDo some cleanup when job finished  \n\n\nexport (self)\n\n\nExport the output files  \n\n\ninit (self)\n\n\nInitiate a job, make directory and prepare input, output and script.  \n\n\nisExptCached (self)\n\n\nPrepare to use export files as cached information\n\nTrue if succeed, otherwise False  \n\n\nisTrulyCached (self)\n\n\nCheck whether a job is truly cached (by signature)  \n\n\npid (self, val)\n\n\nGet/Set the job id (pid or the id from queue system)  \n\n\n\n\nparams:\n\n\nval\n: The id to be set  \n\n\n\n\nrc (self, val)\n\n\nGet/Set the return code  \n\n\n\n\n\n\nparams:\n\n\nval\n: The return code to be set. If it is None, return the return code. Default: \nNone\n\nIf val == -1000: the return code will be negative of current one. 0 will be '-0'  \n\n\n\n\n\n\nreturns:\n\nThe return code if \nval\n is \nNone\n\nIf rcfile does not exist or is empty, return 9999, otherwise return -rc\n\nA negative rc (including -0) means output files not generated  \n\n\n\n\n\n\nreport (self)\n\n\nReport the job information to logger  \n\n\nreset (self, retry)\n\n\nClear the intermediate files and output files  \n\n\nshowError (self, totalfailed)\n\n\nShow the error message if the job failed.  \n\n\nsignature (self)\n\n\nCalculate the signature of the job based on the input/output and the script  \n\n\n\n\nreturns:\n\nThe signature of the job  \n\n\n\n\nsucceed (self)\n\n\nTell if the job is successful by return code, and output file expectations.  \n\n\n\n\nreturns:\n\nTrue if succeed else False  \n\n\n\n\nModule \nJobmgr\n\n\n\n\nJob Manager\n\n\n\n\n__init__ (self, proc, runner)\n\n\nJob manager constructor  \n\n\n\n\nparams:\n\n\nproc\n     : The process\n\n\nrunner\n   : The runner class  \n\n\n\n\nallJobsDone (self)\n\n\nTell whether all jobs are done.\n\nNo need to lock as it only runs in one process (the watcher process)  \n\n\n\n\nreturns:\n\n\nTrue\n if all jobs are done else \nFalse\n  \n\n\n\n\ncanSubmit (self)\n\n\nTell whether we can submit jobs.  \n\n\n\n\nreturns:\n\n\nTrue\n if we can, otherwise \nFalse\n  \n\n\n\n\nprogressbar (self, jid, loglevel)\n\n\nrun (self)\n\n\nStart to run the jobs  \n\n\nrunPool (self, rq, sq)\n\n\nThe pool to run jobs (wait jobs to be done)  \n\n\n\n\nparams:\n\n\nrq\n: The run queue\n\n\nsq\n: The submit queue  \n\n\n\n\nsubmitPool (self, sq)\n\n\nThe pool to submit jobs  \n\n\n\n\nparams:\n\n\nsq\n: The submit queue  \n\n\n\n\nwatchPool (self, rq, sq)\n\n\nThe watchdog, checking whether all jobs are done.  \n\n\nModule \nAggr\n\n\n\n\nThe aggregation of a set of processes\n\n\n\n\n@magic methods:\n    `__setattr__(self, name, value)`: Set property value of an aggregation.\n    - if it's a common property, set it to all processes\n    - if it is `input` set it to starting processes\n    - if it is `depends` set it to the end processes\n    - if it is related to `export` (startswith `ex`), set it to the end processes\n    - if it is in ['starts', 'ends', 'id'], set it to the aggregation itself.\n    - Otherwise a `ValueError` raised.\n    - You can use `[aggr].[proc].[prop]` to set/get the properties of a processes in the aggregation.\n\n\n\n__init__ (self, *args, **kwargs)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\nargs\n: the set of processes\n\n\ndepends\n: Whether auto deduce depends. Default: True\n\n\nid\n: The id of the aggr. Default: None (the variable name)\n\n\ntag\n: The tag of the processes. Default: None (a unique 4-char str according to the id)  \n\n\n\n\naddProc (self, p, tag, where, copy)\n\n\nAdd a process to the aggregation.\n\nNote that you have to adjust the dependencies after you add processes.  \n\n\n\n\n\n\nparams:\n\n\np\n:     The process\n\n\nwhere\n: Add to where: 'starts', 'ends', 'both' or None (default)  \n\n\n\n\n\n\nreturns:\n\nthe aggregation itself  \n\n\n\n\n\n\ncopy (self, tag, deps, id)\n\n\nLike \nproc\n's \ncopy\n function, copy an aggregation. Each processes will be copied.  \n\n\n\n\nparams:\n\n\ntag\n:      The new tag of all copied processes\n\n\ndeps\n: Whether to copy the dependencies or not. Default: True  \n\n\n\n\ndependences for processes in starts will not be copied\n\n\nid\n:    Use a different id if you don't want to use the variant name  \n\n\n\n\n\n\nreturns:\n\nThe new aggregation  \n\n\n\n\n\n\ndelegate (self, attrs, procs)\n\n\nDelegate the procs to have the attributes set by:\n\n\naggr.args.a.b = 1\n\nInstead of setting \nargs.a.b\n of all processes, \nargs.a.b\n of only delegated processes will be set.\n\n\nprocs\n can be \nstarts\n/\nends\n, but it cannot be set with other procs, which means you can do:\n\n\naggr.delegate('args', 'starts')\n, but not \naggr.delegate('args', ['starts', 'pXXX'])\n  \n\n\nModule \nParameter\n\n\n\n\nThe class for a single parameter\n\n\n\n\n__init__ (self, name, value)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\nname\n:  The name of the parameter\n\n\nvalue\n: The initial value of the parameter  \n\n\n\n\n_forceType (self)\n\n\nCoerce the value to the type specified\n\nTypeError will be raised if error happens  \n\n\n_printName (self, prefix, keylen)\n\n\nGet the print name with type for the parameter  \n\n\n\n\nparams:\n\n\nprefix\n: The prefix of the option  \n\n\n\n\nsetDesc (self, d)\n\n\nSet the description of the parameter  \n\n\n\n\nparams:\n\n\nd\n: The description  \n\n\n\n\nsetName (self, n)\n\n\nSet the name of the parameter  \n\n\n\n\nparams:\n\n\nn\n: The name  \n\n\n\n\nsetRequired (self, r)\n\n\nSet whether this parameter is required  \n\n\n\n\nparams:\n\n\nr\n: True if required else False. Default: True  \n\n\n\n\nsetShow (self, s)\n\n\nSet whether this parameter should be shown in help information  \n\n\n\n\nparams:\n\n\ns\n: True if it shows else False. Default: True  \n\n\n\n\nsetType (self, t)\n\n\nSet the type of the parameter  \n\n\n\n\nparams:\n\n\nt\n: The type of the value. Default: str  \n\n\nNote: str rather then 'str'  \n\n\n\n\nsetValue (self, v)\n\n\nSet the value of the parameter  \n\n\n\n\nparams:\n\n\nv\n: The value  \n\n\n\n\nModule \nParameters\n\n\n\n\nA set of parameters\n\n\n\n\n__init__ (self)\n\n\nConstructor  \n\n\n_coerceValue (value, t) [@staticmethod]\n\n\n_getType (self, argname, argtype)\n\n\n_parseName (self, argname)\n\n\nIf \nargname\n is the name of an option  \n\n\n\n\n\n\nparams:\n\n\nargname\n: The argname  \n\n\n\n\n\n\nreturns:\n\n\nan\n: clean argument name\n\n\nat\n: normalized argument type\n\n\nav\n: the argument value, if \nargname\n is like: \n-a=1\n  \n\n\n\n\n\n\n_putValue (self, argname, argtype, argval)\n\n\n_shouldPrintHelp (self, args)\n\n\nasDict (self)\n\n\nConvert the parameters to Box object  \n\n\n\n\nreturns:\n\nThe Box object  \n\n\n\n\nhelp (self, error, printNexit)\n\n\nCalculate the help page  \n\n\n\n\n\n\nparams:\n\n\nerror\n: The error message to show before the help information. Default: \n''\n\n\nprintNexit\n: Print the help page and exit the program? Default: \nFalse\n (return the help information)  \n\n\n\n\n\n\nreturn:\n\nThe help information  \n\n\n\n\n\n\nloadDict (self, dictVar, show)\n\n\nLoad parameters from a dict  \n\n\n\n\nparams:\n\n\ndictVar\n: The dict variable.  \n\n\nProperties are set by \"\n.required\", \"\n.show\", ...\n\n\nshow\n:    Whether these parameters should be shown in help information  \n\n\nDefault: False (don't show parameter from config object in help page)  \n\n\nIt'll be overwritten by the \nshow\n property inside dict variable.  \n\n\nIf it is None, will inherit the param's show value  \n\n\n\n\nloadFile (self, cfgfile, show)\n\n\nLoad parameters from a json/config file\n\nIf the file name ends with '.json', \njson.load\n will be used,\n\notherwise, \nConfigParser\n will be used.\n\nFor config file other than json, a section name is needed, whatever it is.  \n\n\n\n\nparams:\n\n\ncfgfile\n: The config file\n\n\nshow\n:    Whether these parameters should be shown in help information  \n\n\nDefault: False (don't show parameter from config file in help page)  \n\n\nIt'll be overwritten by the \nshow\n property inside the config file.  \n\n\n\n\nparse (self, args)\n\n\nParse the arguments from \nsys.argv\n  \n\n\nModule \nlogger\n\n\n\n\nA customized logger for pyppl\n\n\n\n\nclass: LoggerThemeError\n\n\nTheme errors for logger\n\n\n\n\nclass: PyPPLLogFilter\n\n\nlogging filter by levels (flags)\n\n\n\n\nclass: PyPPLLogFormatter\n\n\nlogging formatter for pyppl\n\n\n\n\nclass: TemplatePyPPL\n\n\nBuilt-in template wrapper.\n\n\n\n\n_formatTheme (theme) [@staticmethod]\n\n\nMake them in the standard form with bgcolor and fgcolor in raw terminal color strings\n\nIf the theme is read from file, try to translate \"COLORS.xxx\" to terminal color strings  \n\n\n\n\n\n\nparams:\n\n\ntheme\n: The theme  \n\n\n\n\n\n\nreturns:\n\nThe formatted colors  \n\n\n\n\n\n\n_getColorFromTheme (level, theme) [@staticmethod]\n\n\nGet colors from a them  \n\n\n\n\n\n\nparams:\n\n\nlevel\n: Our own log record level\n\n\ntheme\n: The theme  \n\n\n\n\n\n\nreturns:\n\nThe colors  \n\n\n\n\n\n\n_getLevel (record) [@staticmethod]\n\n\nGet the flags of a record  \n\n\n\n\nparams:\n\n\nrecord\n:  The logging record  \n\n\n\n\ngetLogger (levels, theme, logfile, lvldiff, name) [@staticmethod]\n\n\nGet the default logger  \n\n\n\n\nparams:\n\n\nlevels\n: The log levels(tags), default: basic\n\n\ntheme\n:  The theme of the logs on terminal. Default: True (default theme will be used)  \n\n\nFalse to disable theme\n\n\nlogfile\n:The log file. Default: None (don't white to log file)\n\n\nlvldiff\n:The diff levels for log  \n\n\n\n\n[\"-depends\", \"jobdone\", \"+debug\"]: show jobdone, hide depends and debug\n\n\nname\n:   The name of the logger, default: PyPPL  \n\n\n\n\n\n\nreturns:\n\nThe logger  \n\n\n\n\n\n\nModule \nutils\n\n\n\n\nA set of utitities for PyPPL\n\n\n\n\nclass: OrderedDict\n\n\nDictionary that remembers insertion order\n\n\n\n\nclass: Parallel\n\n\n\n\n\n\n\nclass: ProcessPoolExecutor\n\n\n\n\n\n\n\nclass: ThreadPoolExecutor\n\n\n\n\n\n\n\n_cp (f1, f2)\n\n\nCopy a file or a directory  \n\n\n\n\nparams:\n\n\nf1\n: The source\n\n\nf2\n: The destination  \n\n\n\n\n_link (f1, f2)\n\n\nCreate a symbolic link for the given file  \n\n\n\n\nparams:\n\n\nf1\n: The source\n\n\nf2\n: The destination  \n\n\n\n\n_lockfile (f, real, tmpdir)\n\n\nGet the path of lockfile of a file  \n\n\n\n\n\n\nparams:\n\n\nf\n: The file  \n\n\n\n\n\n\nreturns:\n\nThe path of the lock file  \n\n\n\n\n\n\n_rm (fn)\n\n\nRemove an entry  \n\n\n\n\nparams:\n\n\nfn\n: The path of the entry  \n\n\n\n\nalwaysList (data)\n\n\nConvert a string or a list with element  \n\n\n\n\n\n\nparams:\n\n\ndata\n: the data to be converted  \n\n\n\n\n\n\nexamples:\n\n\ndata\n \n=\n \n[\na, b, c\n,\n \nd\n]\n  \n\nret\n  \n=\n \nalwaysList\n \n(\ndata\n)\n  \n\n# ret == [\na\n, \nb\n, \nc\n, \nd\n]  \n\n\n\n\n\n\n\n\nreturns:\n\nThe split list  \n\n\n\n\n\n\nasStr (s, encoding)\n\n\nConvert everything (str, unicode, bytes) to str with python2, python3 compatiblity  \n\n\nbasename (f)\n\n\nbriefList (l)\n\n\nBriefly show an integer list, combine the continuous numbers.  \n\n\n\n\n\n\nparams:\n\n\nl\n: The list  \n\n\n\n\n\n\nreturns:\n\nThe string to show for the briefed list.  \n\n\n\n\n\n\nchmodX (thefile)\n\n\nConvert script file to executable or add extract shebang to cmd line  \n\n\n\n\n\n\nparams:\n\n\nthefile\n: the script file  \n\n\n\n\n\n\nreturns:\n\nA list with or without the path of the interpreter as the first element and the script file as the last element  \n\n\n\n\n\n\ndictUpdate (origDict, newDict)\n\n\nUpdate a dictionary recursively.  \n\n\n\n\n\n\nparams:\n\n\norigDict\n: The original dictionary\n\n\nnewDict\n:  The new dictionary  \n\n\n\n\n\n\nexamples:\n\n\nod1\n \n=\n \n{\na\n:\n \n{\nb\n:\n \n{\nc\n:\n \n1\n,\n \nd\n:\n1\n}}}\n  \n\nod2\n \n=\n \n{\nkey\n:\nvalue\n \nfor\n \nkey\n:\nvalue\n \nin\n \nod1\n.\nitems\n()}\n  \n\nnd\n  \n=\n \n{\na\n:\n \n{\nb\n:\n \n{\nd\n:\n \n2\n}}}\n  \n\nod1\n.\nupdate\n(\nnd\n)\n  \n\n# od1 == {\na\n: {\nb\n: {\nd\n: 2}}}, od1[\na\n][\nb\n] is lost  \n\n\ndictUpdate\n(\nod2\n,\n \nnd\n)\n  \n\n# od2 == {\na\n: {\nb\n: {\nc\n: 1, \nd\n: 2}}}  \n\n\n\n\n\n\n\n\ndirmtime (d)\n\n\nCalculate the mtime for a directory.\n\nShould be the max mtime of all files in it.  \n\n\n\n\n\n\nparams:\n\n\nd\n:  the directory  \n\n\n\n\n\n\nreturns:\n\nThe mtime.  \n\n\n\n\n\n\ndumbPopen (cmd, shell)\n\n\nA dumb Popen (no stdout and stderr)  \n\n\n\n\n\n\nparams:\n\n\ncmd\n: The command for \nPopen\n\n\nshell\n: The shell argument for \nPopen\n  \n\n\n\n\n\n\nreturns:\n\nThe process object  \n\n\n\n\n\n\nfileExists (f, callback, tmpdir)\n\n\nTell whether a path exists under a lock  \n\n\n\n\nparams:\n\n\nf\n: the path\n\n\ncallback\n: the callback  \n\n\n\n\narguments: whether the file exists and the path of the file\n\n\ntmpdir\n: The tmpdir to save the lock file  \n\n\n\n\n\n\nreturns:\n\nTrue if yes, otherwise False\n\nIf any of the path does not exist, return False  \n\n\n\n\n\n\nfilesig (fn, dirsig)\n\n\nCalculate a signature for a file according to its path and mtime  \n\n\n\n\n\n\nparams:\n\n\nfn\n: the file  \n\n\n\n\n\n\nreturns:\n\nThe md5 deigested signature.  \n\n\n\n\n\n\nfilter (func, vec)\n\n\nPython2 and Python3 compatible filter  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: The filter function\n\n\nvec\n:  The list to be filtered  \n\n\n\n\n\n\nreturns:\n\nThe filtered list  \n\n\n\n\n\n\nflushFile (f, lastmsg, end)\n\n\nformatSecs (seconds)\n\n\nFormat a time duration  \n\n\n\n\n\n\nparams:\n\n\nseconds\n: the time duration in seconds  \n\n\n\n\n\n\nreturns:\n\nThe formated string.\n\nFor example: \"01:01:01.001\" stands for 1 hour 1 min 1 sec and 1 minisec.  \n\n\n\n\n\n\nfuncsig (func)\n\n\nGet the signature of a function\n\nTry to get the source first, if failed, try to get its name, otherwise return None  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: The function  \n\n\n\n\n\n\nreturns:\n\nThe signature  \n\n\n\n\n\n\ngz (srcfile, gzfile, overwrite, tmpdir)\n\n\nDo a \"gzip\"-like for a file  \n\n\n\n\nparams:\n\n\ngzfile\n:  the final .gz file\n\n\nsrcfile\n: the source file  \n\n\n\n\nmap (func, vec)\n\n\nPython2 and Python3 compatible map  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: The map function\n\n\nvec\n: The list to be maped  \n\n\n\n\n\n\nreturns:\n\nThe maped list  \n\n\n\n\n\n\nrange (i, *args, **kwargs)\n\n\nConvert a range to list, because in python3, range is not a list  \n\n\n\n\n\n\nparams:\n\n\nr\n: the range data  \n\n\n\n\n\n\nreturns:\n\nThe converted list  \n\n\n\n\n\n\nreduce (func, vec)\n\n\nPython2 and Python3 compatible reduce  \n\n\n\n\n\n\nparams:\n\n\nfunc\n: The reduce function\n\n\nvec\n: The list to be reduced  \n\n\n\n\n\n\nreturns:\n\nThe reduced value  \n\n\n\n\n\n\nsafeCopy (f1, f2, callback, overwrite, tmpdir)\n\n\nSafe copy  \n\n\n\n\nparams:\n\n\nsrc\n: The source file\n\n\ndst\n: The dist file\n\n\ncallback\n: The callback (r, f1, f2)\n\n\noverwrite\n: Overwrite target file?\n\n\ntmpdir\n: Tmpdir for lock file  \n\n\n\n\nsafeLink (f1, f2, callback, overwrite, tmpdir)\n\n\nSafe link  \n\n\n\n\nparams:\n\n\nsrc\n: The source file\n\n\ndst\n: The dist file\n\n\ncallback\n: The callback (r, f1, f2)\n\n\noverwrite\n: Overwrite target file?\n\n\ntmpdir\n: Tmpdir for lock file  \n\n\n\n\nsafeMove (f1, f2, callback, overwrite, tmpdir)\n\n\nMove a file/dir  \n\n\n\n\n\n\nparams:\n\n\nsrc\n: The source file\n\n\ndst\n: The destination\n\n\noverwrite\n: Whether overwrite the destination  \n\n\n\n\n\n\nreturn:\n\nTrue if succeed else False  \n\n\n\n\n\n\nsafeMoveWithLink (f1, f2, callback, overwrite, tmpdir)\n\n\nMove a file/dir and leave a link the source file with locks  \n\n\n\n\n\n\nparams:\n\n\nf1\n: The source file\n\n\nf2\n: The destination\n\n\noverwrite\n: Whether overwrite the destination  \n\n\n\n\n\n\nreturn:\n\nTrue if succeed else False  \n\n\n\n\n\n\nsafeRemove (f, callback, tmpdir)\n\n\nSafely remove a file/dir.  \n\n\n\n\nparams:\n\n\nf\n: the file or dir.\n\n\ncallback\n: The callback  \n\n\nargument \nr\n: Whether the file exists before removing  \n\n\nargument \nfn\n: The path of the file  \n\n\n\n\nsamefile (f1, f2, callback, tmpdir)\n\n\nTell whether two paths pointing to the same file under locks  \n\n\n\n\n\n\nparams:\n\n\nf1\n: the first path\n\n\nf2\n: the second path\n\n\ncallback\n: the callback  \n\n\n\n\n\n\nreturns:\n\nTrue if yes, otherwise False\n\nIf any of the path does not exist, return False  \n\n\n\n\n\n\nsplit (s, delimter, trim)\n\n\nSplit a string using a single-character delimter  \n\n\n\n\n\n\nparams:\n\n\ns\n: the string\n\n\ndelimter\n: the single-character delimter\n\n\ntrim\n: whether to trim each part. Default: True  \n\n\n\n\n\n\nexamples:\n\n\nret\n \n=\n \nsplit\n(\na,b\n,c\n,\n \n,\n)\n  \n\n# ret == [\na,b\n, \nc\n]  \n\n\n# \n,\n inside quotes will be recognized.  \n\n\n\n\n\n\n\n\nreturns:\n\nThe list of substrings  \n\n\n\n\n\n\ntargz (srcdir, tgzfile, overwrite, tmpdir)\n\n\nDo a \"tar zcf\"-like for a directory  \n\n\n\n\nparams:\n\n\ntgzfile\n: the final .tgz file\n\n\nsrcdir\n:  the source directory  \n\n\n\n\nuid (s, l, alphabet)\n\n\nCalculate a short uid based on a string.\n\nSafe enough, tested on 1000000 32-char strings, no repeated uid found.\n\nThis is used to calcuate a uid for a process  \n\n\n\n\n\n\nparams:\n\n\ns\n: the base string\n\n\nl\n: the length of the uid\n\n\nalphabet\n: the charset used to generate the uid  \n\n\n\n\n\n\nreturns:\n\nThe uid  \n\n\n\n\n\n\nungz (gzfile, dstfile, overwrite, tmpdir)\n\n\nDo a \"gunzip\"-like for a .gz file  \n\n\n\n\nparams:\n\n\ngzfile\n:  the .gz file\n\n\ndstfile\n: the extracted file  \n\n\n\n\nuntargz (tgzfile, dstdir, overwrite, tmpdir)\n\n\nDo a \"tar zxf\"-like for .tgz file  \n\n\n\n\nparams:\n\n\ntgzfile\n:  the .tgz file\n\n\ndstdir\n: which directory to extract the file to  \n\n\n\n\nvarname (maxline, incldot)\n\n\nGet the variable name for ini  \n\n\n\n\n\n\nparams:\n\n\nmaxline\n: The max number of lines to retrive. Default: 20\n\n\nincldot\n: Whether include dot in the variable name. Default: False  \n\n\n\n\n\n\nreturns:\n\nThe variable name  \n\n\n\n\n\n\nModule \nproctree.ProcNode\n\n\n\n\nThe node for processes to manage relations between each other\n\n\n\n\n__init__ (self, proc)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\nproc\n: The \nProc\n instance  \n\n\n\n\nsameIdTag (self, proc)\n\n\nCheck if the process has the same id and tag with me.  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The \nProc\n instance  \n\n\n\n\n\n\nreturns:\n\n\nTrue\n if it is.\n\n\nFalse\n if not.  \n\n\n\n\n\n\nModule \nproctree.ProcTree\n\n\n\n\n.\n\n\n\n\n__init__ (self)\n\n\nConstructor, set the status of all \nProcNode\ns  \n\n\ncheck (proc) [@staticmethod]\n\n\nCheck whether a process with the same id and tag exists  \n\n\n\n\nparams:\n\n\nproc\n: The \nProc\n instance  \n\n\n\n\ncheckPath (self, proc)\n\n\nCheck whether paths of a process can start from a start process  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The process  \n\n\n\n\n\n\nreturns:\n\n\nTrue\n if all paths can pass\n\nThe failed path otherwise  \n\n\n\n\n\n\ngetAllPaths (self)\n\n\ngetEnds (self)\n\n\nGet the end processes  \n\n\n\n\nreturns:\n\nThe end processes  \n\n\n\n\ngetNext (proc) [@staticmethod]\n\n\nGet next processes of process  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The \nProc\n instance  \n\n\n\n\n\n\nreturns:\n\nThe processes depend on this process  \n\n\n\n\n\n\ngetNextStr (proc) [@staticmethod]\n\n\nGet the names of processes depend on a process  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The \nProc\n instance  \n\n\n\n\n\n\nreturns:\n\nThe names  \n\n\n\n\n\n\ngetNextToRun (self)\n\n\nGet the process to run next  \n\n\n\n\nreturns:\n\nThe process next to run  \n\n\n\n\ngetPaths (self, proc, proc0)\n\n\nInfer the path to a process  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The process\n\n\nproc0\n: The original process, because this function runs recursively.  \n\n\n\n\n\n\nreturns:\n\n\np1 -\n p2 -\n p3  \np4  _/  \nPaths for p3: [[p4], [p2, p1]]  \n\n\n\n\n\n\n\ngetPathsToStarts (self, proc)\n\n\nFilter the paths with start processes  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The process  \n\n\n\n\n\n\nreturns:\n\nThe filtered path  \n\n\n\n\n\n\ngetPrevStr (proc) [@staticmethod]\n\n\nGet the names of processes a process depends on  \n\n\n\n\n\n\nparams:\n\n\nproc\n: The \nProc\n instance  \n\n\n\n\n\n\nreturns:\n\nThe names  \n\n\n\n\n\n\ngetStarts (self)\n\n\nGet the start processes  \n\n\n\n\nreturns:\n\nThe start processes  \n\n\n\n\nregister (proc) [@staticmethod]\n\n\nRegister the process  \n\n\n\n\nparams:\n\n\nproc\n: The \nProc\n instance  \n\n\n\n\nreset () [@staticmethod]\n\n\nReset the status of all \nProcNode\ns  \n\n\nsetStarts (self, starts)\n\n\nSet the start processes  \n\n\n\n\nparams:\n\n\nstarts\n: The start processes  \n\n\n\n\nunranProcs (self)\n\n\nModule \ntemplates.TemplatePyPPL\n\n\n\n\nBuilt-in template wrapper.\n\n\n\n\n__init__ (self, source, **envs)\n\n\nInitiate the engine with source and envs  \n\n\n\n\nparams:\n\n\nsource\n: The souce text\n\n\nenvs\n: The env data  \n\n\n\n\n_render (self, data)\n\n\nRender the template  \n\n\n\n\n\n\nparams:\n\n\ndata\n: The data used for rendering  \n\n\n\n\n\n\nreturns:\n\nThe rendered string  \n\n\n\n\n\n\nModule \ntemplates.TemplateJinja2\n\n\n\n\nJinja2 template wrapper\n\n\n\n\n__init__ (self, source, **envs)\n\n\nInitiate the engine with source and envs  \n\n\n\n\nparams:\n\n\nsource\n: The souce text\n\n\nenvs\n: The env data  \n\n\n\n\n_render (self, data)\n\n\nRender the template  \n\n\n\n\n\n\nparams:\n\n\ndata\n: The data used for rendering  \n\n\n\n\n\n\nreturns:\n\nThe rendered string  \n\n\n\n\n\n\nModule \nrunners.Runner\n\n\n\n\nThe base runner class\n\n\n\n\n__init__ (self, job)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\njob\n:    The job object  \n\n\n\n\n_flush (self, fout, ferr, lastout, lasterr, end)\n\n\nFlush stdout/stderr  \n\n\n\n\nparams:\n\n\nfout\n: The stdout file handler\n\n\nferr\n: The stderr file handler\n\n\nlastout\n: The leftovers of previously readlines of stdout\n\n\nlasterr\n: The leftovers of previously readlines of stderr\n\n\nend\n: Whether this is the last time to flush  \n\n\n\n\nfinish (self)\n\n\ngetpid (self)\n\n\nGet the job id  \n\n\nisRunning (self)\n\n\nTry to tell whether the job is still running.  \n\n\n\n\nreturns:\n\n\nTrue\n if yes, otherwise \nFalse\n  \n\n\n\n\nretry (self)\n\n\nrun (self)\n\n\n\n\nreturns:\n\nTrue: success/fail\n\nFalse: needs retry  \n\n\n\n\nsubmit (self)\n\n\nTry to submit the job use Popen  \n\n\nModule \nrunners.RunnerLocal\n\n\n\n\nConstructor\n    @params:\n        \njob\n:    The job object\n        \nconfig\n: The properties of the process\n\n\n\n\n__init__ (self, job)\n\n\nModule \nrunners.RunnerSsh\n\n\n\n\nThe ssh runner\n\n\n\n\n@static variables:\n    `SERVERID`: The incremental number used to calculate which server should be used.\n    - Don't touch unless you know what's going on!\n\n\n\n__init__ (self, job)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\njob\n:    The job object  \n\n\n\n\nisServerAlive (server, key) [@staticmethod]\n\n\nModule \nrunners.RunnerSge\n\n\n\n\nThe sge runner\n\n\n\n\n__init__ (self, job)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\njob\n:    The job object\n\n\nconfig\n: The properties of the process  \n\n\n\n\ngetpid (self)\n\n\nGet the job identity and save it to job.pidfile  \n\n\nisRunning (self)\n\n\nTell whether the job is still running  \n\n\n\n\nreturns:\n\nTrue if it is running else False  \n\n\n\n\nModule \nrunners.RunnerSlurm\n\n\n\n\nThe slurm runner\n\n\n\n\n__init__ (self, job)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\njob\n:    The job object\n\n\nconfig\n: The properties of the process  \n\n\n\n\ngetpid (self)\n\n\nGet the job identity and save it to job.pidfile  \n\n\nisRunning (self)\n\n\nTell whether the job is still running  \n\n\n\n\nreturns:\n\nTrue if it is running else False  \n\n\n\n\nModule \nrunners.RunnerDry\n\n\n\n\nThe dry runner\n\n\n\n\n__init__ (self, job)\n\n\nConstructor  \n\n\n\n\nparams:\n\n\njob\n:    The job object  \n\n\n\n\nfinish (self)\n\n\nDo some cleanup work when jobs finish", 
            "title": "API"
        }, 
        {
            "location": "/api/#api", 
            "text": "", 
            "title": "API"
        }, 
        {
            "location": "/api/#module-pyppl", 
            "text": "The PyPPL class   @static variables:\n    `TIPS`: The tips for users\n    `RUNNERS`: Registered runners\n    `DEFAULT_CFGFILES`: Default configuration file", 
            "title": "Module PyPPL"
        }, 
        {
            "location": "/api/#__init__-self-config-cfgfile", 
            "text": "Constructor     params:  config : the configurations for the pipeline, default: {}  cfgfile :  the configuration file for the pipeline, default:  ~/.PyPPL.json  or  ./.PyPPL", 
            "title": "__init__ (self, config, cfgfile)"
        }, 
        {
            "location": "/api/#_any2procs-args-staticmethod", 
            "text": "Get procs from anything (aggr.starts, proc, procs, proc names)      params:  arg : anything      returns: \nA set of procs", 
            "title": "_any2procs (*args) [@staticmethod]"
        }, 
        {
            "location": "/api/#_checkproc-proc-staticmethod", 
            "text": "Check processes, whether 2 processes have the same id and tag      params:  proc : The process      returns: \nIf there are 2 processes with the same id and tag, raise  ValueError .", 
            "title": "_checkProc (proc) [@staticmethod]"
        }, 
        {
            "location": "/api/#_registerproc-proc-staticmethod", 
            "text": "Register the process     params:  proc : The process", 
            "title": "_registerProc (proc) [@staticmethod]"
        }, 
        {
            "location": "/api/#_resume-self-args-kwargs", 
            "text": "Mark processes as to be resumed     params:  args : the processes to be marked. The last element is the mark for processes to be skipped.", 
            "title": "_resume (self, *args, **kwargs)"
        }, 
        {
            "location": "/api/#flowchart-self-fcfile-dotfile", 
            "text": "Generate graph in dot language and visualize it.     params:  dotfile : Where to same the dot graph. Default:  None  ( path.splitext(sys.argv[0])[0] + \".pyppl.dot\" )  fcfile :  The flowchart file. Default:  None  ( path.splitext(sys.argv[0])[0] + \".pyppl.svg\" )     For example: run  python pipeline.py  will save it to  pipeline.pyppl.svg  dot :     The dot visulizer. Default: \"dot -Tsvg {{dotfile}}   {{fcfile}}\"      returns: \nThe pipeline object itself.", 
            "title": "flowchart (self, fcfile, dotfile)"
        }, 
        {
            "location": "/api/#registerrunner-runner-staticmethod", 
            "text": "Register a runner     params:  runner : The runner to be registered.", 
            "title": "registerRunner (runner) [@staticmethod]"
        }, 
        {
            "location": "/api/#resume-self-args", 
            "text": "Mark processes as to be resumed      params:  args : the processes to be marked      returns: \nThe pipeline object itself.", 
            "title": "resume (self, *args)"
        }, 
        {
            "location": "/api/#resume2-self-args", 
            "text": "Mark processes as to be resumed      params:  args : the processes to be marked      returns: \nThe pipeline object itself.", 
            "title": "resume2 (self, *args)"
        }, 
        {
            "location": "/api/#run-self-profile", 
            "text": "Run the pipeline      params:  profile : the profile used to run, if not found, it'll be used as runner name. default: 'default'      returns: \nThe pipeline object itself.", 
            "title": "run (self, profile)"
        }, 
        {
            "location": "/api/#showallroutes-self", 
            "text": "", 
            "title": "showAllRoutes (self)"
        }, 
        {
            "location": "/api/#start-self-args", 
            "text": "Set the starting processes of the pipeline      params:  args : the starting processes      returns: \nThe pipeline object itself.", 
            "title": "start (self, *args)"
        }, 
        {
            "location": "/api/#module-proc", 
            "text": "The Proc class defining a process   @static variables:\n    `RUNNERS`:       The regiested runners\n    `ALIAS`:         The alias for the properties\n    `LOG_NLINE`:     The limit of lines of logging information of same type of messages\n\n@magic methods:\n    `__getattr__(self, name)`: get the value of a property in `self.props`\n    `__setattr__(self, name, value)`: set the value of a property in `self.config`", 
            "title": "Module Proc"
        }, 
        {
            "location": "/api/#__init__-self-tag-desc-id", 
            "text": "Constructor      params:  tag :  The tag of the process  desc : The description of the process  id :   The identify of the process      config: \nid, input, output, ppldir, forks, cache, cclean, rc, echo, runner, script, depends, tag, desc, dirsig \nexdir, exhow, exow, errhow, errntry, lang, beforeCmd, afterCmd, workdir, args, aggr \ncallfront, callback, expect, expart, template, tplenvs, resume, nthread      props \ninput, output, rc, echo, script, depends, beforeCmd, afterCmd, workdir, expect \nexpart, template, channel, jobs, ncjobids, size, sets, procvars, suffix, logs", 
            "title": "__init__ (self, tag, desc, id)"
        }, 
        {
            "location": "/api/#_buildinput-self", 
            "text": "Build the input data \nInput could be: \n1. list: ['input', 'infile:file']  =  ['input:var', 'infile:path'] \n2. str : \"input, infile:file\"  =  input:var, infile:path \n3. dict: {\"input\": channel1, \"infile:file\": channel2} \nor    {\"input:var, input:file\" : channel3} \nfor 1,2 channels will be the combined channel from dependents, if there is not dependents, it will be sys.argv[1:]", 
            "title": "_buildInput (self)"
        }, 
        {
            "location": "/api/#_buildjobs-self", 
            "text": "Build the jobs.", 
            "title": "_buildJobs (self)"
        }, 
        {
            "location": "/api/#_buildoutput-self", 
            "text": "Build the output data templates waiting to be rendered.", 
            "title": "_buildOutput (self)"
        }, 
        {
            "location": "/api/#_buildprocvars-self", 
            "text": "Build proc attribute values for template rendering, \nand also echo some out.", 
            "title": "_buildProcVars (self)"
        }, 
        {
            "location": "/api/#_buildprops-self", 
            "text": "Compute some properties", 
            "title": "_buildProps (self)"
        }, 
        {
            "location": "/api/#_buildscript-self", 
            "text": "Build the script template waiting to be rendered.", 
            "title": "_buildScript (self)"
        }, 
        {
            "location": "/api/#_checkcached-self", 
            "text": "Tell whether the jobs are cached     returns: \nTrue if all jobs are cached, otherwise False", 
            "title": "_checkCached (self)"
        }, 
        {
            "location": "/api/#_readconfig-self-profile-profiles", 
            "text": "Read the configuration     params:  config : The configuration", 
            "title": "_readConfig (self, profile, profiles)"
        }, 
        {
            "location": "/api/#_runcmd-self-key", 
            "text": "Run the  beforeCmd  or  afterCmd       params:  key : \"beforeCmd\" or \"afterCmd\"      returns: \nThe return code of the command", 
            "title": "_runCmd (self, key)"
        }, 
        {
            "location": "/api/#_runjobs-self", 
            "text": "Submit and run the jobs", 
            "title": "_runJobs (self)"
        }, 
        {
            "location": "/api/#_savesettings-self", 
            "text": "Save all settings in proc.settings, mostly for debug", 
            "title": "_saveSettings (self)"
        }, 
        {
            "location": "/api/#_suffix-self", 
            "text": "Calcuate a uid for the process according to the configuration \nThe philosophy: \n1. procs from different script must have different suffix (sys.argv[0]) \n2. procs from the same script: \n- procs with different id or tag have different suffix \n- procs with different input have different suffix (depends, input)     returns: \nThe uniq id of the process", 
            "title": "_suffix (self)"
        }, 
        {
            "location": "/api/#_tidyafterrun-self", 
            "text": "Do some cleaning after running jobs \nself.resume can only be: \n- '': normal process \n- skip+: skipped process but required workdir and data exists \n- resume: resume pipeline from this process, no requirement \n- resume+: get data from workdir/proc.settings, and resume", 
            "title": "_tidyAfterRun (self)"
        }, 
        {
            "location": "/api/#_tidybeforerun-self", 
            "text": "Do some preparation before running jobs", 
            "title": "_tidyBeforeRun (self)"
        }, 
        {
            "location": "/api/#copy-self-tag-desc-id", 
            "text": "Copy a process      params:  id : The new id of the process, default:  None  (use the varname)  tag :   The tag of the new process, default:  None  (used the old one)  desc :  The desc of the new process, default:  None  (used the old one)      returns: \nThe new process", 
            "title": "copy (self, tag, desc, id)"
        }, 
        {
            "location": "/api/#log-self-msg-level-key", 
            "text": "The log function with aggregation name, process id and tag integrated.     params:  msg :   The message to log  level : The log level  key :   The type of messages", 
            "title": "log (self, msg, level, key)"
        }, 
        {
            "location": "/api/#name-self-aggr", 
            "text": "Get my name include  aggr ,  id ,  tag      returns: \nthe name", 
            "title": "name (self, aggr)"
        }, 
        {
            "location": "/api/#run-self-profile-profiles", 
            "text": "Run the jobs with a configuration     params:  config : The configuration", 
            "title": "run (self, profile, profiles)"
        }, 
        {
            "location": "/api/#module-channel", 
            "text": "The channen class, extended from  list", 
            "title": "Module Channel"
        }, 
        {
            "location": "/api/#_tuplize-tu-staticmethod", 
            "text": "A private method, try to convert an element to tuple \nIf it's a string, convert it to  (tu, ) \nElse if it is iterable, convert it to  tuple(tu) \nOtherwise, convert it to  (tu, ) \nNotice that string is also iterable.      params:  tu : the element to be converted      returns: \nThe converted element", 
            "title": "_tuplize (tu) [@staticmethod]"
        }, 
        {
            "location": "/api/#attach-self-names-kwargs", 
            "text": "Attach columns to names of Channel, so we can access each column by:  ch.col0  == ch.colAt(0)     params:  names : The names. Have to be as length as channel's width. None of them should be Channel's property name  flatten : Whether flatten the channel for the name being attached", 
            "title": "attach (self, *names, **kwargs)"
        }, 
        {
            "location": "/api/#cbind-self-cols", 
            "text": "Add columns to the channel      params:  cols : The columns      returns: \nThe channel with the columns inserted.", 
            "title": "cbind (self, *cols)"
        }, 
        {
            "location": "/api/#colat-self-index", 
            "text": "Fetch one column of a Channel      params:  index : which column to fetch      returns: \nThe Channel with that column", 
            "title": "colAt (self, index)"
        }, 
        {
            "location": "/api/#collapse-self-col", 
            "text": "Do the reverse of expand \nlength: N -  1 \nwidth:  M -  M      params:  col :     the index of the column used to collapse      returns: \nThe collapsed Channel", 
            "title": "collapse (self, col)"
        }, 
        {
            "location": "/api/#copy-self", 
            "text": "Copy a Channel using  copy.copy      returns: \nThe copied Channel", 
            "title": "copy (self)"
        }, 
        {
            "location": "/api/#create-l-staticmethod", 
            "text": "Create a Channel from a list      params:  l : The list, default: []      returns: \nThe Channel created from the list", 
            "title": "create (l) [@staticmethod]"
        }, 
        {
            "location": "/api/#expand-self-col-pattern-t-sortby-reverse", 
            "text": "expand the Channel according to the files in  , other cols will keep the same  [(dir1/dir2, 1)].expand (0, \"*\")  will expand to  [(dir1/dir2/file1, 1), (dir1/dir2/file2, 1), ...] \nlength: 1 -  N \nwidth:  M -  M     params:  col :     the index of the column used to expand  pattern : use a pattern to filter the files/dirs, default:  *  t :       the type of the files/dirs to include    'dir', 'file', 'link' or 'any' (default)  sortby :  how the list is sorted     'name' (default), 'mtime', 'size'  reverse : reverse sort. Default: False      returns: \nThe expanded Channel", 
            "title": "expand (self, col, pattern, t, sortby, reverse)"
        }, 
        {
            "location": "/api/#filter-self-func", 
            "text": "Alias of python builtin  filter       params:  func : the function. Default: None      returns: \nThe filtered Channel", 
            "title": "filter (self, func)"
        }, 
        {
            "location": "/api/#filtercol-self-func-col", 
            "text": "Just filter on the first column      params:  func : the function  col : the column to filter      returns: \nThe filtered Channel", 
            "title": "filterCol (self, func, col)"
        }, 
        {
            "location": "/api/#flatten-self-col", 
            "text": "Convert a single-column Channel to a list (remove the tuple signs)  [(a,), (b,)]  to  [a, b]       params:  col : The column to flat. None for all columns (default)      returns: \nThe list converted from the Channel.", 
            "title": "flatten (self, col)"
        }, 
        {
            "location": "/api/#fold-self-n", 
            "text": "Fold a Channel. Make a row to n-length chunk rows  a1\ta2\ta3\ta4  \nb1\tb2\tb3\tb4  \nif n==2, fold(2) will change it to:  \na1\ta2  \na3\ta4  \nb1\tb2  \nb3\tb4      params:  n : the size of the chunk      returns \nThe new Channel", 
            "title": "fold (self, n)"
        }, 
        {
            "location": "/api/#fromargv-staticmethod", 
            "text": "Create a Channel from  sys.argv[1:] \n\"python test.py a b c\" creates a width=1 Channel \n\"python test.py a,1 b,2 c,3\" creates a width=2 Channel     returns: \nThe Channel created from the command line arguments", 
            "title": "fromArgv () [@staticmethod]"
        }, 
        {
            "location": "/api/#fromchannels-args-staticmethod", 
            "text": "Create a Channel from Channels      params:  args : The Channels      returns: \nThe Channel merged from other Channels", 
            "title": "fromChannels (*args) [@staticmethod]"
        }, 
        {
            "location": "/api/#fromfile-fn-header-skip-delimit-staticmethod", 
            "text": "Create Channel from the file content \nIt's like a matrix file, each row is a row for a Channel. \nAnd each column is a column for a Channel.     params:  fn :      the file  header :  Whether the file contains header. If True, will attach the header     So you can use  channel. header  to fetch the column  skip :    first lines to skip  delimit : the delimit for columns      returns: \nA Channel created from the file", 
            "title": "fromFile (fn, header, skip, delimit) [@staticmethod]"
        }, 
        {
            "location": "/api/#frompairs-pattern-staticmethod", 
            "text": "Create a width = 2 Channel from a pattern      params:  pattern : the pattern      returns: \nThe Channel create from every 2 files match the pattern", 
            "title": "fromPairs (pattern) [@staticmethod]"
        }, 
        {
            "location": "/api/#fromparams-pnames-staticmethod", 
            "text": "Create a Channel from params      params:  *pnames : The names of the option      returns: \nThe Channel", 
            "title": "fromParams (*pnames) [@staticmethod]"
        }, 
        {
            "location": "/api/#frompattern-pattern-t-sortby-reverse-staticmethod", 
            "text": "Create a Channel from a path pattern     params:  pattern : the pattern with wild cards  t :       the type of the files/dirs to include    'dir', 'file', 'link' or 'any' (default)  sortby :  how the list is sorted     'name' (default), 'mtime', 'size'  reverse : reverse sort. Default: False      returns: \nThe Channel created from the path", 
            "title": "fromPattern (pattern, t, sortby, reverse) [@staticmethod]"
        }, 
        {
            "location": "/api/#get-self-idx", 
            "text": "Get the element of a flattened channel      params:  idx : The index of the element to get. Default: 0      return: \nThe element", 
            "title": "get (self, idx)"
        }, 
        {
            "location": "/api/#insert-self-cidx-cols", 
            "text": "Insert columns to a channel      params:  cidx : Insert into which index of column?  cols : the columns to be bound to Channel      returns: \nThe combined Channel \nNote, self is also changed", 
            "title": "insert (self, cidx, *cols)"
        }, 
        {
            "location": "/api/#length-self", 
            "text": "Get the length of a Channel \nIt's just an alias of  len(chan)      returns: \nThe length of the Channel", 
            "title": "length (self)"
        }, 
        {
            "location": "/api/#map-self-func", 
            "text": "Alias of python builtin  map       params:  func : the function      returns: \nThe transformed Channel", 
            "title": "map (self, func)"
        }, 
        {
            "location": "/api/#mapcol-self-func-col", 
            "text": "Map for a column      params:  func : the function  col : the index of the column. Default: 0      returns: \nThe transformed Channel", 
            "title": "mapCol (self, func, col)"
        }, 
        {
            "location": "/api/#rbind-self-rows", 
            "text": "The multiple-argument versoin of  rbind       params:  rows : the rows to be bound to Channel      returns: \nThe combined Channel \nNote, self is also changed", 
            "title": "rbind (self, *rows)"
        }, 
        {
            "location": "/api/#reduce-self-func", 
            "text": "Alias of python builtin  reduce       params:  func : the function      returns: \nThe reduced value", 
            "title": "reduce (self, func)"
        }, 
        {
            "location": "/api/#reducecol-self-func-col", 
            "text": "Reduce a column      params:  func : the function  col : the column to reduce      returns: \nThe reduced value", 
            "title": "reduceCol (self, func, col)"
        }, 
        {
            "location": "/api/#repcol-self-n", 
            "text": "Repeat column and return a new channel      params:  n : how many times to repeat.      returns: \nThe new channel with repeated columns", 
            "title": "repCol (self, n)"
        }, 
        {
            "location": "/api/#reprow-self-n", 
            "text": "Repeat row and return a new channel      params:  n : how many times to repeat.      returns: \nThe new channel with repeated rows", 
            "title": "repRow (self, n)"
        }, 
        {
            "location": "/api/#rowat-self-index", 
            "text": "Fetch one row of a Channel      params:  index : which row to fetch      returns: \nThe Channel with that row", 
            "title": "rowAt (self, index)"
        }, 
        {
            "location": "/api/#slice-self-start-length", 
            "text": "Fetch some columns of a Channel      params:  start :  from column to start  length : how many columns to fetch, default: None (from start to the end)      returns: \nThe Channel with fetched columns", 
            "title": "slice (self, start, length)"
        }, 
        {
            "location": "/api/#split-self-flatten", 
            "text": "Split a Channel to single-column Channels     returns: \nThe list of single-column Channels", 
            "title": "split (self, flatten)"
        }, 
        {
            "location": "/api/#unfold-self-n", 
            "text": "Do the reverse thing as self.fold does      params:  n : How many rows to combind each time. default: 2      returns: \nThe unfolded Channel", 
            "title": "unfold (self, n)"
        }, 
        {
            "location": "/api/#unique-self", 
            "text": "Make the channel unique, remove duplicated rows \nTry to keep the order", 
            "title": "unique (self)"
        }, 
        {
            "location": "/api/#width-self", 
            "text": "Get the width of a Channel     returns: \nThe width of the Channel", 
            "title": "width (self)"
        }, 
        {
            "location": "/api/#module-job", 
            "text": "Job class, defining a job in a process", 
            "title": "Module Job"
        }, 
        {
            "location": "/api/#__init__-self-index-proc", 
            "text": "Constructor     params:  index :   The index of the job in a process  proc :    The process", 
            "title": "__init__ (self, index, proc)"
        }, 
        {
            "location": "/api/#_indexindicator-self", 
            "text": "Get the index indicator in the log     returns: \nThe \"[001/100]\" like indicator", 
            "title": "_indexIndicator (self)"
        }, 
        {
            "location": "/api/#_linkinfile-self-orgfile", 
            "text": "Create links for input files      params:  orgfile : The original input file      returns: \nThe link to the original file.", 
            "title": "_linkInfile (self, orgfile)"
        }, 
        {
            "location": "/api/#_prepinput-self", 
            "text": "Prepare input, create link to input files and set other placeholders", 
            "title": "_prepInput (self)"
        }, 
        {
            "location": "/api/#_prepoutput-self", 
            "text": "Build the output data. \nOutput could be: \n1. list:  ['output:var:{{input}}', 'outfile:file:{{infile.bn}}.txt'] \nor you can ignore the name if you don't put it in script:  ['var:{{input}}', 'path:{{infile.bn}}.txt'] \nor even (only var type can be ignored):  ['{{input}}', 'file:{{infile.bn}}.txt'] \n2. str :  'output:var:{{input}}, outfile:file:{{infile.bn}}.txt' \n3. OrderedDict:  {\"output:var:{{input}}\": channel1, \"outfile:file:{{infile.bn}}.txt\": channel2} \nor     {\"output:var:{{input}}, output:file:{{infile.bn}}.txt\" : channel3} \nfor 1,2 channels will be the property channel for this proc (i.e. p.channel)", 
            "title": "_prepOutput (self)"
        }, 
        {
            "location": "/api/#_prepscript-self", 
            "text": "Build the script, interpret the placeholders", 
            "title": "_prepScript (self)"
        }, 
        {
            "location": "/api/#_reportitem-self-key-maxlen-data-loglevel", 
            "text": "Report the item on logs     params:  key : The key of the item  maxlen : The max length of the key  data : The data of the item  loglevel : The log level", 
            "title": "_reportItem (self, key, maxlen, data, loglevel)"
        }, 
        {
            "location": "/api/#cache-self", 
            "text": "Truly cache the job (by signature)", 
            "title": "cache (self)"
        }, 
        {
            "location": "/api/#checkoutfiles-self-expect", 
            "text": "Check whether output files are generated, if not, add - to rc.", 
            "title": "checkOutfiles (self, expect)"
        }, 
        {
            "location": "/api/#done-self", 
            "text": "Do some cleanup when job finished", 
            "title": "done (self)"
        }, 
        {
            "location": "/api/#export-self", 
            "text": "Export the output files", 
            "title": "export (self)"
        }, 
        {
            "location": "/api/#init-self", 
            "text": "Initiate a job, make directory and prepare input, output and script.", 
            "title": "init (self)"
        }, 
        {
            "location": "/api/#isexptcached-self", 
            "text": "Prepare to use export files as cached information \nTrue if succeed, otherwise False", 
            "title": "isExptCached (self)"
        }, 
        {
            "location": "/api/#istrulycached-self", 
            "text": "Check whether a job is truly cached (by signature)", 
            "title": "isTrulyCached (self)"
        }, 
        {
            "location": "/api/#pid-self-val", 
            "text": "Get/Set the job id (pid or the id from queue system)     params:  val : The id to be set", 
            "title": "pid (self, val)"
        }, 
        {
            "location": "/api/#rc-self-val", 
            "text": "Get/Set the return code      params:  val : The return code to be set. If it is None, return the return code. Default:  None \nIf val == -1000: the return code will be negative of current one. 0 will be '-0'      returns: \nThe return code if  val  is  None \nIf rcfile does not exist or is empty, return 9999, otherwise return -rc \nA negative rc (including -0) means output files not generated", 
            "title": "rc (self, val)"
        }, 
        {
            "location": "/api/#report-self", 
            "text": "Report the job information to logger", 
            "title": "report (self)"
        }, 
        {
            "location": "/api/#reset-self-retry", 
            "text": "Clear the intermediate files and output files", 
            "title": "reset (self, retry)"
        }, 
        {
            "location": "/api/#showerror-self-totalfailed", 
            "text": "Show the error message if the job failed.", 
            "title": "showError (self, totalfailed)"
        }, 
        {
            "location": "/api/#signature-self", 
            "text": "Calculate the signature of the job based on the input/output and the script     returns: \nThe signature of the job", 
            "title": "signature (self)"
        }, 
        {
            "location": "/api/#succeed-self", 
            "text": "Tell if the job is successful by return code, and output file expectations.     returns: \nTrue if succeed else False", 
            "title": "succeed (self)"
        }, 
        {
            "location": "/api/#module-jobmgr", 
            "text": "Job Manager", 
            "title": "Module Jobmgr"
        }, 
        {
            "location": "/api/#__init__-self-proc-runner", 
            "text": "Job manager constructor     params:  proc      : The process  runner    : The runner class", 
            "title": "__init__ (self, proc, runner)"
        }, 
        {
            "location": "/api/#alljobsdone-self", 
            "text": "Tell whether all jobs are done. \nNo need to lock as it only runs in one process (the watcher process)     returns:  True  if all jobs are done else  False", 
            "title": "allJobsDone (self)"
        }, 
        {
            "location": "/api/#cansubmit-self", 
            "text": "Tell whether we can submit jobs.     returns:  True  if we can, otherwise  False", 
            "title": "canSubmit (self)"
        }, 
        {
            "location": "/api/#progressbar-self-jid-loglevel", 
            "text": "", 
            "title": "progressbar (self, jid, loglevel)"
        }, 
        {
            "location": "/api/#run-self", 
            "text": "Start to run the jobs", 
            "title": "run (self)"
        }, 
        {
            "location": "/api/#runpool-self-rq-sq", 
            "text": "The pool to run jobs (wait jobs to be done)     params:  rq : The run queue  sq : The submit queue", 
            "title": "runPool (self, rq, sq)"
        }, 
        {
            "location": "/api/#submitpool-self-sq", 
            "text": "The pool to submit jobs     params:  sq : The submit queue", 
            "title": "submitPool (self, sq)"
        }, 
        {
            "location": "/api/#watchpool-self-rq-sq", 
            "text": "The watchdog, checking whether all jobs are done.", 
            "title": "watchPool (self, rq, sq)"
        }, 
        {
            "location": "/api/#module-aggr", 
            "text": "The aggregation of a set of processes   @magic methods:\n    `__setattr__(self, name, value)`: Set property value of an aggregation.\n    - if it's a common property, set it to all processes\n    - if it is `input` set it to starting processes\n    - if it is `depends` set it to the end processes\n    - if it is related to `export` (startswith `ex`), set it to the end processes\n    - if it is in ['starts', 'ends', 'id'], set it to the aggregation itself.\n    - Otherwise a `ValueError` raised.\n    - You can use `[aggr].[proc].[prop]` to set/get the properties of a processes in the aggregation.", 
            "title": "Module Aggr"
        }, 
        {
            "location": "/api/#__init__-self-args-kwargs", 
            "text": "Constructor     params:  args : the set of processes  depends : Whether auto deduce depends. Default: True  id : The id of the aggr. Default: None (the variable name)  tag : The tag of the processes. Default: None (a unique 4-char str according to the id)", 
            "title": "__init__ (self, *args, **kwargs)"
        }, 
        {
            "location": "/api/#addproc-self-p-tag-where-copy", 
            "text": "Add a process to the aggregation. \nNote that you have to adjust the dependencies after you add processes.      params:  p :     The process  where : Add to where: 'starts', 'ends', 'both' or None (default)      returns: \nthe aggregation itself", 
            "title": "addProc (self, p, tag, where, copy)"
        }, 
        {
            "location": "/api/#copy-self-tag-deps-id", 
            "text": "Like  proc 's  copy  function, copy an aggregation. Each processes will be copied.     params:  tag :      The new tag of all copied processes  deps : Whether to copy the dependencies or not. Default: True     dependences for processes in starts will not be copied  id :    Use a different id if you don't want to use the variant name      returns: \nThe new aggregation", 
            "title": "copy (self, tag, deps, id)"
        }, 
        {
            "location": "/api/#delegate-self-attrs-procs", 
            "text": "Delegate the procs to have the attributes set by:  aggr.args.a.b = 1 \nInstead of setting  args.a.b  of all processes,  args.a.b  of only delegated processes will be set.  procs  can be  starts / ends , but it cannot be set with other procs, which means you can do:  aggr.delegate('args', 'starts') , but not  aggr.delegate('args', ['starts', 'pXXX'])", 
            "title": "delegate (self, attrs, procs)"
        }, 
        {
            "location": "/api/#module-parameter", 
            "text": "The class for a single parameter", 
            "title": "Module Parameter"
        }, 
        {
            "location": "/api/#__init__-self-name-value", 
            "text": "Constructor     params:  name :  The name of the parameter  value : The initial value of the parameter", 
            "title": "__init__ (self, name, value)"
        }, 
        {
            "location": "/api/#_forcetype-self", 
            "text": "Coerce the value to the type specified \nTypeError will be raised if error happens", 
            "title": "_forceType (self)"
        }, 
        {
            "location": "/api/#_printname-self-prefix-keylen", 
            "text": "Get the print name with type for the parameter     params:  prefix : The prefix of the option", 
            "title": "_printName (self, prefix, keylen)"
        }, 
        {
            "location": "/api/#setdesc-self-d", 
            "text": "Set the description of the parameter     params:  d : The description", 
            "title": "setDesc (self, d)"
        }, 
        {
            "location": "/api/#setname-self-n", 
            "text": "Set the name of the parameter     params:  n : The name", 
            "title": "setName (self, n)"
        }, 
        {
            "location": "/api/#setrequired-self-r", 
            "text": "Set whether this parameter is required     params:  r : True if required else False. Default: True", 
            "title": "setRequired (self, r)"
        }, 
        {
            "location": "/api/#setshow-self-s", 
            "text": "Set whether this parameter should be shown in help information     params:  s : True if it shows else False. Default: True", 
            "title": "setShow (self, s)"
        }, 
        {
            "location": "/api/#settype-self-t", 
            "text": "Set the type of the parameter     params:  t : The type of the value. Default: str    Note: str rather then 'str'", 
            "title": "setType (self, t)"
        }, 
        {
            "location": "/api/#setvalue-self-v", 
            "text": "Set the value of the parameter     params:  v : The value", 
            "title": "setValue (self, v)"
        }, 
        {
            "location": "/api/#module-parameters", 
            "text": "A set of parameters", 
            "title": "Module Parameters"
        }, 
        {
            "location": "/api/#__init__-self", 
            "text": "Constructor", 
            "title": "__init__ (self)"
        }, 
        {
            "location": "/api/#_coercevalue-value-t-staticmethod", 
            "text": "", 
            "title": "_coerceValue (value, t) [@staticmethod]"
        }, 
        {
            "location": "/api/#_gettype-self-argname-argtype", 
            "text": "", 
            "title": "_getType (self, argname, argtype)"
        }, 
        {
            "location": "/api/#_parsename-self-argname", 
            "text": "If  argname  is the name of an option      params:  argname : The argname      returns:  an : clean argument name  at : normalized argument type  av : the argument value, if  argname  is like:  -a=1", 
            "title": "_parseName (self, argname)"
        }, 
        {
            "location": "/api/#_putvalue-self-argname-argtype-argval", 
            "text": "", 
            "title": "_putValue (self, argname, argtype, argval)"
        }, 
        {
            "location": "/api/#_shouldprinthelp-self-args", 
            "text": "", 
            "title": "_shouldPrintHelp (self, args)"
        }, 
        {
            "location": "/api/#asdict-self", 
            "text": "Convert the parameters to Box object     returns: \nThe Box object", 
            "title": "asDict (self)"
        }, 
        {
            "location": "/api/#help-self-error-printnexit", 
            "text": "Calculate the help page      params:  error : The error message to show before the help information. Default:  ''  printNexit : Print the help page and exit the program? Default:  False  (return the help information)      return: \nThe help information", 
            "title": "help (self, error, printNexit)"
        }, 
        {
            "location": "/api/#loaddict-self-dictvar-show", 
            "text": "Load parameters from a dict     params:  dictVar : The dict variable.    Properties are set by \" .required\", \" .show\", ...  show :    Whether these parameters should be shown in help information    Default: False (don't show parameter from config object in help page)    It'll be overwritten by the  show  property inside dict variable.    If it is None, will inherit the param's show value", 
            "title": "loadDict (self, dictVar, show)"
        }, 
        {
            "location": "/api/#loadfile-self-cfgfile-show", 
            "text": "Load parameters from a json/config file \nIf the file name ends with '.json',  json.load  will be used, \notherwise,  ConfigParser  will be used. \nFor config file other than json, a section name is needed, whatever it is.     params:  cfgfile : The config file  show :    Whether these parameters should be shown in help information    Default: False (don't show parameter from config file in help page)    It'll be overwritten by the  show  property inside the config file.", 
            "title": "loadFile (self, cfgfile, show)"
        }, 
        {
            "location": "/api/#parse-self-args", 
            "text": "Parse the arguments from  sys.argv", 
            "title": "parse (self, args)"
        }, 
        {
            "location": "/api/#module-logger", 
            "text": "A customized logger for pyppl", 
            "title": "Module logger"
        }, 
        {
            "location": "/api/#class-loggerthemeerror", 
            "text": "Theme errors for logger", 
            "title": "class: LoggerThemeError"
        }, 
        {
            "location": "/api/#class-pyppllogfilter", 
            "text": "logging filter by levels (flags)", 
            "title": "class: PyPPLLogFilter"
        }, 
        {
            "location": "/api/#class-pyppllogformatter", 
            "text": "logging formatter for pyppl", 
            "title": "class: PyPPLLogFormatter"
        }, 
        {
            "location": "/api/#class-templatepyppl", 
            "text": "Built-in template wrapper.", 
            "title": "class: TemplatePyPPL"
        }, 
        {
            "location": "/api/#_formattheme-theme-staticmethod", 
            "text": "Make them in the standard form with bgcolor and fgcolor in raw terminal color strings \nIf the theme is read from file, try to translate \"COLORS.xxx\" to terminal color strings      params:  theme : The theme      returns: \nThe formatted colors", 
            "title": "_formatTheme (theme) [@staticmethod]"
        }, 
        {
            "location": "/api/#_getcolorfromtheme-level-theme-staticmethod", 
            "text": "Get colors from a them      params:  level : Our own log record level  theme : The theme      returns: \nThe colors", 
            "title": "_getColorFromTheme (level, theme) [@staticmethod]"
        }, 
        {
            "location": "/api/#_getlevel-record-staticmethod", 
            "text": "Get the flags of a record     params:  record :  The logging record", 
            "title": "_getLevel (record) [@staticmethod]"
        }, 
        {
            "location": "/api/#getlogger-levels-theme-logfile-lvldiff-name-staticmethod", 
            "text": "Get the default logger     params:  levels : The log levels(tags), default: basic  theme :  The theme of the logs on terminal. Default: True (default theme will be used)    False to disable theme  logfile :The log file. Default: None (don't white to log file)  lvldiff :The diff levels for log     [\"-depends\", \"jobdone\", \"+debug\"]: show jobdone, hide depends and debug  name :   The name of the logger, default: PyPPL      returns: \nThe logger", 
            "title": "getLogger (levels, theme, logfile, lvldiff, name) [@staticmethod]"
        }, 
        {
            "location": "/api/#module-utils", 
            "text": "A set of utitities for PyPPL", 
            "title": "Module utils"
        }, 
        {
            "location": "/api/#class-ordereddict", 
            "text": "Dictionary that remembers insertion order", 
            "title": "class: OrderedDict"
        }, 
        {
            "location": "/api/#class-parallel", 
            "text": "", 
            "title": "class: Parallel"
        }, 
        {
            "location": "/api/#class-processpoolexecutor", 
            "text": "", 
            "title": "class: ProcessPoolExecutor"
        }, 
        {
            "location": "/api/#class-threadpoolexecutor", 
            "text": "", 
            "title": "class: ThreadPoolExecutor"
        }, 
        {
            "location": "/api/#_cp-f1-f2", 
            "text": "Copy a file or a directory     params:  f1 : The source  f2 : The destination", 
            "title": "_cp (f1, f2)"
        }, 
        {
            "location": "/api/#_link-f1-f2", 
            "text": "Create a symbolic link for the given file     params:  f1 : The source  f2 : The destination", 
            "title": "_link (f1, f2)"
        }, 
        {
            "location": "/api/#_lockfile-f-real-tmpdir", 
            "text": "Get the path of lockfile of a file      params:  f : The file      returns: \nThe path of the lock file", 
            "title": "_lockfile (f, real, tmpdir)"
        }, 
        {
            "location": "/api/#_rm-fn", 
            "text": "Remove an entry     params:  fn : The path of the entry", 
            "title": "_rm (fn)"
        }, 
        {
            "location": "/api/#alwayslist-data", 
            "text": "Convert a string or a list with element      params:  data : the data to be converted      examples:  data   =   [ a, b, c ,   d ]    ret    =   alwaysList   ( data )    # ret == [ a ,  b ,  c ,  d ]       returns: \nThe split list", 
            "title": "alwaysList (data)"
        }, 
        {
            "location": "/api/#asstr-s-encoding", 
            "text": "Convert everything (str, unicode, bytes) to str with python2, python3 compatiblity", 
            "title": "asStr (s, encoding)"
        }, 
        {
            "location": "/api/#basename-f", 
            "text": "", 
            "title": "basename (f)"
        }, 
        {
            "location": "/api/#brieflist-l", 
            "text": "Briefly show an integer list, combine the continuous numbers.      params:  l : The list      returns: \nThe string to show for the briefed list.", 
            "title": "briefList (l)"
        }, 
        {
            "location": "/api/#chmodx-thefile", 
            "text": "Convert script file to executable or add extract shebang to cmd line      params:  thefile : the script file      returns: \nA list with or without the path of the interpreter as the first element and the script file as the last element", 
            "title": "chmodX (thefile)"
        }, 
        {
            "location": "/api/#dictupdate-origdict-newdict", 
            "text": "Update a dictionary recursively.      params:  origDict : The original dictionary  newDict :  The new dictionary      examples:  od1   =   { a :   { b :   { c :   1 ,   d : 1 }}}    od2   =   { key : value   for   key : value   in   od1 . items ()}    nd    =   { a :   { b :   { d :   2 }}}    od1 . update ( nd )    # od1 == { a : { b : { d : 2}}}, od1[ a ][ b ] is lost    dictUpdate ( od2 ,   nd )    # od2 == { a : { b : { c : 1,  d : 2}}}", 
            "title": "dictUpdate (origDict, newDict)"
        }, 
        {
            "location": "/api/#dirmtime-d", 
            "text": "Calculate the mtime for a directory. \nShould be the max mtime of all files in it.      params:  d :  the directory      returns: \nThe mtime.", 
            "title": "dirmtime (d)"
        }, 
        {
            "location": "/api/#dumbpopen-cmd-shell", 
            "text": "A dumb Popen (no stdout and stderr)      params:  cmd : The command for  Popen  shell : The shell argument for  Popen       returns: \nThe process object", 
            "title": "dumbPopen (cmd, shell)"
        }, 
        {
            "location": "/api/#fileexists-f-callback-tmpdir", 
            "text": "Tell whether a path exists under a lock     params:  f : the path  callback : the callback     arguments: whether the file exists and the path of the file  tmpdir : The tmpdir to save the lock file      returns: \nTrue if yes, otherwise False \nIf any of the path does not exist, return False", 
            "title": "fileExists (f, callback, tmpdir)"
        }, 
        {
            "location": "/api/#filesig-fn-dirsig", 
            "text": "Calculate a signature for a file according to its path and mtime      params:  fn : the file      returns: \nThe md5 deigested signature.", 
            "title": "filesig (fn, dirsig)"
        }, 
        {
            "location": "/api/#filter-func-vec", 
            "text": "Python2 and Python3 compatible filter      params:  func : The filter function  vec :  The list to be filtered      returns: \nThe filtered list", 
            "title": "filter (func, vec)"
        }, 
        {
            "location": "/api/#flushfile-f-lastmsg-end", 
            "text": "", 
            "title": "flushFile (f, lastmsg, end)"
        }, 
        {
            "location": "/api/#formatsecs-seconds", 
            "text": "Format a time duration      params:  seconds : the time duration in seconds      returns: \nThe formated string. \nFor example: \"01:01:01.001\" stands for 1 hour 1 min 1 sec and 1 minisec.", 
            "title": "formatSecs (seconds)"
        }, 
        {
            "location": "/api/#funcsig-func", 
            "text": "Get the signature of a function \nTry to get the source first, if failed, try to get its name, otherwise return None      params:  func : The function      returns: \nThe signature", 
            "title": "funcsig (func)"
        }, 
        {
            "location": "/api/#gz-srcfile-gzfile-overwrite-tmpdir", 
            "text": "Do a \"gzip\"-like for a file     params:  gzfile :  the final .gz file  srcfile : the source file", 
            "title": "gz (srcfile, gzfile, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#map-func-vec", 
            "text": "Python2 and Python3 compatible map      params:  func : The map function  vec : The list to be maped      returns: \nThe maped list", 
            "title": "map (func, vec)"
        }, 
        {
            "location": "/api/#range-i-args-kwargs", 
            "text": "Convert a range to list, because in python3, range is not a list      params:  r : the range data      returns: \nThe converted list", 
            "title": "range (i, *args, **kwargs)"
        }, 
        {
            "location": "/api/#reduce-func-vec", 
            "text": "Python2 and Python3 compatible reduce      params:  func : The reduce function  vec : The list to be reduced      returns: \nThe reduced value", 
            "title": "reduce (func, vec)"
        }, 
        {
            "location": "/api/#safecopy-f1-f2-callback-overwrite-tmpdir", 
            "text": "Safe copy     params:  src : The source file  dst : The dist file  callback : The callback (r, f1, f2)  overwrite : Overwrite target file?  tmpdir : Tmpdir for lock file", 
            "title": "safeCopy (f1, f2, callback, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#safelink-f1-f2-callback-overwrite-tmpdir", 
            "text": "Safe link     params:  src : The source file  dst : The dist file  callback : The callback (r, f1, f2)  overwrite : Overwrite target file?  tmpdir : Tmpdir for lock file", 
            "title": "safeLink (f1, f2, callback, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#safemove-f1-f2-callback-overwrite-tmpdir", 
            "text": "Move a file/dir      params:  src : The source file  dst : The destination  overwrite : Whether overwrite the destination      return: \nTrue if succeed else False", 
            "title": "safeMove (f1, f2, callback, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#safemovewithlink-f1-f2-callback-overwrite-tmpdir", 
            "text": "Move a file/dir and leave a link the source file with locks      params:  f1 : The source file  f2 : The destination  overwrite : Whether overwrite the destination      return: \nTrue if succeed else False", 
            "title": "safeMoveWithLink (f1, f2, callback, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#saferemove-f-callback-tmpdir", 
            "text": "Safely remove a file/dir.     params:  f : the file or dir.  callback : The callback    argument  r : Whether the file exists before removing    argument  fn : The path of the file", 
            "title": "safeRemove (f, callback, tmpdir)"
        }, 
        {
            "location": "/api/#samefile-f1-f2-callback-tmpdir", 
            "text": "Tell whether two paths pointing to the same file under locks      params:  f1 : the first path  f2 : the second path  callback : the callback      returns: \nTrue if yes, otherwise False \nIf any of the path does not exist, return False", 
            "title": "samefile (f1, f2, callback, tmpdir)"
        }, 
        {
            "location": "/api/#split-s-delimter-trim", 
            "text": "Split a string using a single-character delimter      params:  s : the string  delimter : the single-character delimter  trim : whether to trim each part. Default: True      examples:  ret   =   split ( a,b ,c ,   , )    # ret == [ a,b ,  c ]    #  ,  inside quotes will be recognized.       returns: \nThe list of substrings", 
            "title": "split (s, delimter, trim)"
        }, 
        {
            "location": "/api/#targz-srcdir-tgzfile-overwrite-tmpdir", 
            "text": "Do a \"tar zcf\"-like for a directory     params:  tgzfile : the final .tgz file  srcdir :  the source directory", 
            "title": "targz (srcdir, tgzfile, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#uid-s-l-alphabet", 
            "text": "Calculate a short uid based on a string. \nSafe enough, tested on 1000000 32-char strings, no repeated uid found. \nThis is used to calcuate a uid for a process      params:  s : the base string  l : the length of the uid  alphabet : the charset used to generate the uid      returns: \nThe uid", 
            "title": "uid (s, l, alphabet)"
        }, 
        {
            "location": "/api/#ungz-gzfile-dstfile-overwrite-tmpdir", 
            "text": "Do a \"gunzip\"-like for a .gz file     params:  gzfile :  the .gz file  dstfile : the extracted file", 
            "title": "ungz (gzfile, dstfile, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#untargz-tgzfile-dstdir-overwrite-tmpdir", 
            "text": "Do a \"tar zxf\"-like for .tgz file     params:  tgzfile :  the .tgz file  dstdir : which directory to extract the file to", 
            "title": "untargz (tgzfile, dstdir, overwrite, tmpdir)"
        }, 
        {
            "location": "/api/#varname-maxline-incldot", 
            "text": "Get the variable name for ini      params:  maxline : The max number of lines to retrive. Default: 20  incldot : Whether include dot in the variable name. Default: False      returns: \nThe variable name", 
            "title": "varname (maxline, incldot)"
        }, 
        {
            "location": "/api/#module-proctreeprocnode", 
            "text": "The node for processes to manage relations between each other", 
            "title": "Module proctree.ProcNode"
        }, 
        {
            "location": "/api/#__init__-self-proc", 
            "text": "Constructor     params:  proc : The  Proc  instance", 
            "title": "__init__ (self, proc)"
        }, 
        {
            "location": "/api/#sameidtag-self-proc", 
            "text": "Check if the process has the same id and tag with me.      params:  proc : The  Proc  instance      returns:  True  if it is.  False  if not.", 
            "title": "sameIdTag (self, proc)"
        }, 
        {
            "location": "/api/#module-proctreeproctree", 
            "text": ".", 
            "title": "Module proctree.ProcTree"
        }, 
        {
            "location": "/api/#__init__-self_1", 
            "text": "Constructor, set the status of all  ProcNode s", 
            "title": "__init__ (self)"
        }, 
        {
            "location": "/api/#check-proc-staticmethod", 
            "text": "Check whether a process with the same id and tag exists     params:  proc : The  Proc  instance", 
            "title": "check (proc) [@staticmethod]"
        }, 
        {
            "location": "/api/#checkpath-self-proc", 
            "text": "Check whether paths of a process can start from a start process      params:  proc : The process      returns:  True  if all paths can pass \nThe failed path otherwise", 
            "title": "checkPath (self, proc)"
        }, 
        {
            "location": "/api/#getallpaths-self", 
            "text": "", 
            "title": "getAllPaths (self)"
        }, 
        {
            "location": "/api/#getends-self", 
            "text": "Get the end processes     returns: \nThe end processes", 
            "title": "getEnds (self)"
        }, 
        {
            "location": "/api/#getnext-proc-staticmethod", 
            "text": "Get next processes of process      params:  proc : The  Proc  instance      returns: \nThe processes depend on this process", 
            "title": "getNext (proc) [@staticmethod]"
        }, 
        {
            "location": "/api/#getnextstr-proc-staticmethod", 
            "text": "Get the names of processes depend on a process      params:  proc : The  Proc  instance      returns: \nThe names", 
            "title": "getNextStr (proc) [@staticmethod]"
        }, 
        {
            "location": "/api/#getnexttorun-self", 
            "text": "Get the process to run next     returns: \nThe process next to run", 
            "title": "getNextToRun (self)"
        }, 
        {
            "location": "/api/#getpaths-self-proc-proc0", 
            "text": "Infer the path to a process      params:  proc : The process  proc0 : The original process, because this function runs recursively.      returns:  p1 -  p2 -  p3  \np4  _/  \nPaths for p3: [[p4], [p2, p1]]", 
            "title": "getPaths (self, proc, proc0)"
        }, 
        {
            "location": "/api/#getpathstostarts-self-proc", 
            "text": "Filter the paths with start processes      params:  proc : The process      returns: \nThe filtered path", 
            "title": "getPathsToStarts (self, proc)"
        }, 
        {
            "location": "/api/#getprevstr-proc-staticmethod", 
            "text": "Get the names of processes a process depends on      params:  proc : The  Proc  instance      returns: \nThe names", 
            "title": "getPrevStr (proc) [@staticmethod]"
        }, 
        {
            "location": "/api/#getstarts-self", 
            "text": "Get the start processes     returns: \nThe start processes", 
            "title": "getStarts (self)"
        }, 
        {
            "location": "/api/#register-proc-staticmethod", 
            "text": "Register the process     params:  proc : The  Proc  instance", 
            "title": "register (proc) [@staticmethod]"
        }, 
        {
            "location": "/api/#reset-staticmethod", 
            "text": "Reset the status of all  ProcNode s", 
            "title": "reset () [@staticmethod]"
        }, 
        {
            "location": "/api/#setstarts-self-starts", 
            "text": "Set the start processes     params:  starts : The start processes", 
            "title": "setStarts (self, starts)"
        }, 
        {
            "location": "/api/#unranprocs-self", 
            "text": "", 
            "title": "unranProcs (self)"
        }, 
        {
            "location": "/api/#module-templatestemplatepyppl", 
            "text": "Built-in template wrapper.", 
            "title": "Module templates.TemplatePyPPL"
        }, 
        {
            "location": "/api/#__init__-self-source-envs", 
            "text": "Initiate the engine with source and envs     params:  source : The souce text  envs : The env data", 
            "title": "__init__ (self, source, **envs)"
        }, 
        {
            "location": "/api/#_render-self-data", 
            "text": "Render the template      params:  data : The data used for rendering      returns: \nThe rendered string", 
            "title": "_render (self, data)"
        }, 
        {
            "location": "/api/#module-templatestemplatejinja2", 
            "text": "Jinja2 template wrapper", 
            "title": "Module templates.TemplateJinja2"
        }, 
        {
            "location": "/api/#__init__-self-source-envs_1", 
            "text": "Initiate the engine with source and envs     params:  source : The souce text  envs : The env data", 
            "title": "__init__ (self, source, **envs)"
        }, 
        {
            "location": "/api/#_render-self-data_1", 
            "text": "Render the template      params:  data : The data used for rendering      returns: \nThe rendered string", 
            "title": "_render (self, data)"
        }, 
        {
            "location": "/api/#module-runnersrunner", 
            "text": "The base runner class", 
            "title": "Module runners.Runner"
        }, 
        {
            "location": "/api/#__init__-self-job", 
            "text": "Constructor     params:  job :    The job object", 
            "title": "__init__ (self, job)"
        }, 
        {
            "location": "/api/#_flush-self-fout-ferr-lastout-lasterr-end", 
            "text": "Flush stdout/stderr     params:  fout : The stdout file handler  ferr : The stderr file handler  lastout : The leftovers of previously readlines of stdout  lasterr : The leftovers of previously readlines of stderr  end : Whether this is the last time to flush", 
            "title": "_flush (self, fout, ferr, lastout, lasterr, end)"
        }, 
        {
            "location": "/api/#finish-self", 
            "text": "", 
            "title": "finish (self)"
        }, 
        {
            "location": "/api/#getpid-self", 
            "text": "Get the job id", 
            "title": "getpid (self)"
        }, 
        {
            "location": "/api/#isrunning-self", 
            "text": "Try to tell whether the job is still running.     returns:  True  if yes, otherwise  False", 
            "title": "isRunning (self)"
        }, 
        {
            "location": "/api/#retry-self", 
            "text": "", 
            "title": "retry (self)"
        }, 
        {
            "location": "/api/#run-self_1", 
            "text": "returns: \nTrue: success/fail \nFalse: needs retry", 
            "title": "run (self)"
        }, 
        {
            "location": "/api/#submit-self", 
            "text": "Try to submit the job use Popen", 
            "title": "submit (self)"
        }, 
        {
            "location": "/api/#module-runnersrunnerlocal", 
            "text": "Constructor\n    @params:\n         job :    The job object\n         config : The properties of the process", 
            "title": "Module runners.RunnerLocal"
        }, 
        {
            "location": "/api/#__init__-self-job_1", 
            "text": "", 
            "title": "__init__ (self, job)"
        }, 
        {
            "location": "/api/#module-runnersrunnerssh", 
            "text": "The ssh runner   @static variables:\n    `SERVERID`: The incremental number used to calculate which server should be used.\n    - Don't touch unless you know what's going on!", 
            "title": "Module runners.RunnerSsh"
        }, 
        {
            "location": "/api/#__init__-self-job_2", 
            "text": "Constructor     params:  job :    The job object", 
            "title": "__init__ (self, job)"
        }, 
        {
            "location": "/api/#isserveralive-server-key-staticmethod", 
            "text": "", 
            "title": "isServerAlive (server, key) [@staticmethod]"
        }, 
        {
            "location": "/api/#module-runnersrunnersge", 
            "text": "The sge runner", 
            "title": "Module runners.RunnerSge"
        }, 
        {
            "location": "/api/#__init__-self-job_3", 
            "text": "Constructor     params:  job :    The job object  config : The properties of the process", 
            "title": "__init__ (self, job)"
        }, 
        {
            "location": "/api/#getpid-self_1", 
            "text": "Get the job identity and save it to job.pidfile", 
            "title": "getpid (self)"
        }, 
        {
            "location": "/api/#isrunning-self_1", 
            "text": "Tell whether the job is still running     returns: \nTrue if it is running else False", 
            "title": "isRunning (self)"
        }, 
        {
            "location": "/api/#module-runnersrunnerslurm", 
            "text": "The slurm runner", 
            "title": "Module runners.RunnerSlurm"
        }, 
        {
            "location": "/api/#__init__-self-job_4", 
            "text": "Constructor     params:  job :    The job object  config : The properties of the process", 
            "title": "__init__ (self, job)"
        }, 
        {
            "location": "/api/#getpid-self_2", 
            "text": "Get the job identity and save it to job.pidfile", 
            "title": "getpid (self)"
        }, 
        {
            "location": "/api/#isrunning-self_2", 
            "text": "Tell whether the job is still running     returns: \nTrue if it is running else False", 
            "title": "isRunning (self)"
        }, 
        {
            "location": "/api/#module-runnersrunnerdry", 
            "text": "The dry runner", 
            "title": "Module runners.RunnerDry"
        }, 
        {
            "location": "/api/#__init__-self-job_5", 
            "text": "Constructor     params:  job :    The job object", 
            "title": "__init__ (self, job)"
        }, 
        {
            "location": "/api/#finish-self_1", 
            "text": "Do some cleanup work when jobs finish", 
            "title": "finish (self)"
        }, 
        {
            "location": "/faq/", 
            "text": "FAQ\n\n\nQ: How should I migrate from 0.8.x?\n\n\n\n\nFirst letters of class names are capitalized (i.e. \nproc\n -\n \nProc\n, \naggr\n -\n \nAggr\n). Note that previous class \npyppl\n was changed to \nPyPPL\n.\n\n\nDefault configuration files were changed to \n~/.PyPPL.json\n and \n~/.PyPPL\n\n\nLog configurations were grouped to \n{\"log\": {...}}\n instead of \n{\"logtheme\": ..., \"loglevels\": ...}\n\n\nFlowchart is themeable now: in configuration file: \n{\"flowchart\": {...}}\n\n\nTemplating enhanced from previous placeholders (Jinja2 supported). See \ntemplating\n\n\nInput and output placeholders are now under \nin\n and \nout\n namespaces, respectively.\n\n\nupdateArgs\n is merged into \nset\n for \nAggr\n.\n\n\nModule \ndoct\n removed, \npython-box\n is used instead.\n\n\n\n\nQ: Do I have to use the variable name as the process id?\n\n\nA: No, you can use a different one by \npWhatever = Proc (id=pYourId)\n, or \npWhatever = Proc ()\n, and then change the id by \npWhatever.id = 'pYourId'\n\n\nQ: What's the difference between \ninput\n and \nargs\n?\n\n\nA: Basically, \nargs\n are supposed to be arguments shared among all jobs in the process. Files in \nargs\n are not linked in the \njob.indir\n folder.\n\n\nQ: Does a \nProc\n remain the same after it's used to construct an \nAggr\n?\n\n\nA: No, it will be a copy of the original one. So the original be used somewhere else.\n\n\nQ: Can I dry-run a process?\n  \n\n\nA: Yes, just use the dry  runner: \np.runner = \"dry\"\n. The runner will just create empty files/directories for output, and skip to run the script.\n\n\nQ: Can I disable the logs on the terminal?\n  \n\n\nA: Yes, just set \n{\"log\": {\"levels\": None}}\n in pipeline configurations.", 
            "title": "FAQ"
        }, 
        {
            "location": "/faq/#faq", 
            "text": "Q: How should I migrate from 0.8.x?   First letters of class names are capitalized (i.e.  proc  -   Proc ,  aggr  -   Aggr ). Note that previous class  pyppl  was changed to  PyPPL .  Default configuration files were changed to  ~/.PyPPL.json  and  ~/.PyPPL  Log configurations were grouped to  {\"log\": {...}}  instead of  {\"logtheme\": ..., \"loglevels\": ...}  Flowchart is themeable now: in configuration file:  {\"flowchart\": {...}}  Templating enhanced from previous placeholders (Jinja2 supported). See  templating  Input and output placeholders are now under  in  and  out  namespaces, respectively.  updateArgs  is merged into  set  for  Aggr .  Module  doct  removed,  python-box  is used instead.   Q: Do I have to use the variable name as the process id?  A: No, you can use a different one by  pWhatever = Proc (id=pYourId) , or  pWhatever = Proc () , and then change the id by  pWhatever.id = 'pYourId'  Q: What's the difference between  input  and  args ?  A: Basically,  args  are supposed to be arguments shared among all jobs in the process. Files in  args  are not linked in the  job.indir  folder.  Q: Does a  Proc  remain the same after it's used to construct an  Aggr ?  A: No, it will be a copy of the original one. So the original be used somewhere else.  Q: Can I dry-run a process?     A: Yes, just use the dry  runner:  p.runner = \"dry\" . The runner will just create empty files/directories for output, and skip to run the script.  Q: Can I disable the logs on the terminal?     A: Yes, just set  {\"log\": {\"levels\": None}}  in pipeline configurations.", 
            "title": "FAQ"
        }, 
        {
            "location": "/change-log/", 
            "text": "Change log\n\n\nJuly 10, 2018: 1.0.0 !\n\n\n\n\nFix runner name issue #31.\n\n\nUse mkdocs to generate documentations and host them on GitHub pages.\n\n\nKeep stdout and stderr when a job is cached: #30.\n\n\nAllow command line arguments to overwrite Parameter's type.\n\n\nHost the testing procedures with Travis.\n\n\nFix other bugs.\n\n\n\n\nJune 8, 2018: 0.9.6\n\n\n\n\nAuto-delegate common proc config names to aggr\n\n\nAdd proc.origin to save the original proc id for copied procs\n\n\nRemove brings, add proc.infile to swith '{{in.(infile)}}' to job.indir path, original path or realpath of the input file\n\n\nAdd process lock to avoid the same processes run simultaneously\n\n\nUse built-in Box instead of box.Box\n\n\nMerge template function Rvec and Rlist into R, add repr\n\n\nFix #29 and #31, and fix other bugs\n\n\n\n\nMar 6, 2018: 0.9.5\n\n\n\n\nAdd proc.dirsig to disable/enable calculating signatures from deep directories\n\n\nAdd Jobmgr class to handle job distribution\n\n\nAllow channel.rowAt and colAt to return multiple rows and columns, respectively\n\n\nAllow empty channel as input (process will be skipped)\n\n\nRefine all tests\n\n\nRewrite thread-safe file system helper functions\n\n\nAdd specific exception classes\n\n\nReport line # when error happens in template\n\n\nAdd progress bar for jobs\n\n\nAllow stdout and stderr file as output\n\n\n\n\nDec 27, 2017: 0.9.4\n\n\n\n\nAdd yaml support for config file (#26).\n\n\nAllow empty list for input files.\n\n\nMerge continuous job ids in log (Make the list shorter).\n\n\nMore details when internal template failed to render (#25)\n\n\nIgnore .yaml config files if yaml module is not installed.\n\n\nsleep before testing isRunning to avoid all jobs running it at the same time.\n\n\nUse repr to output p.args and p.props.\n\n\nMerge Proc attributes profile and runner. Profile is now an alias of runner, and will be removed finally.\n\n\n\n\nNov 20, 2017: 0.9.3\n\n\n\n\nBeautify parameters help page.\n\n\nEnable multithreading for job construction and cache checking (set by proc.nthread).\n\n\nUniform multiprocessing/threading.\n\n\nFix Aggr delegate problems.\n\n\nAdd ProcTree to manage process relations.\n\n\nReport processes will not run due to prior processes not ran.\n\n\nAdd cclean option for enable/disable cleanup (output check/export) if a job is cached.\n\n\nAdd tooltip for flowchart svg.\n\n\nFix job id not saved for runner_sge.\n\n\nFix resume assignment issue.\n\n\nRewrite proc.log function so that logs from jobs do not mess up when they are multithreaded.\n\n\nFix params loaded from file get overwriten.\n\n\nAdd coverage report.\n\n\n\n\nOct 23, 2017: 0.9.2\n\n\n\n\nAdd profile for Proc so different procs can run with different profiles.\n\n\nAdd delegate for Aggr.\n\n\nAdd get, repCol, repRow, rowAt method for Channel.\n\n\nDont't sleep for batch interval if jobs are cached or running.\n\n\nAdd header argument for Channel.fromFile.\n\n\nFix a bunch of bugs.\n\n\n\n\nOct 6, 2017: 0.9.1\n\n\n\n\nFix issues reported by codacy\n\n\nFix an issue checking whether output files generated\n\n\nDeepcopy args and tplenvs when copy a process\n\n\nRefer relative path in p.script (with \"file:\" prefix) where p.script is defined\n\n\nFix a utils.dictUpdate bug\n\n\nTemplate function 'Rlist' now can deal with python list\n\n\nAdd log for export using move method (previously missed)\n\n\nAllow Aggr instance to set input directly\n\n\nSwitch default showing of parameters loaded from object or file to False\n\n\nOptimize utils.varname\n\n\nAdd warning if more input data columns than number of input keys\n\n\nFix output channel key sequence does not keep\n\n\nUse object id instead of name as key for PyPPL nexts/paths in case tag is set in pipeline configrations\n\n\n\n\nSept 22, 2017: 0.9.0\n\n\n\n\nChange class name with first letter capitalized\n\n\nAdd resuming from processes (#20)\n\n\nFix #19\n\n\nGroup log configuration\n\n\nMake flowchart themeable and configurable\n\n\nMake attributes of \nProc\n clearer\n\n\nAdd tutorials\n\n\nMake tests more robust\n\n\nEnhancer templating, support Jinja2\n\n\nSet attributes of processes in aggregation with \nset\n\n\n\n\nAug 4, 2017: 0.8.1\n\n\n\n\nAdd partial echo\n\n\nAdd interactive log from the script\n\n\nAdd partial export\n\n\nAdd log themes and filters\n\n\nAdd compare to command line tool\n\n\nFix bugs\n\n\n\n\nAug 1, 2017: 0.8.0\n\n\n\n\nAdd slurm and dry runner\n\n\nFix bugs when more than 2 input files have same basename\n\n\nAdd indent mark for script, specially useful for python\n\n\nMake stdout/stderr flushes out for instant runners when p.echo = True\n\n\nAdd \nsortby\n, \nreverse\n for \nchannel.fromPath\n\n\nAdd command line argument parse\n\n\nFix a bug that threads do not exit after process is done\n\n\n\n\nJuly 18, 2017: 0.7.4\n\n\n\n\nDocs updated (thanks @marchon for some grammer corrections)\n\n\nSome shortcut functions for placeholders added\n\n\nCheck running during polling removed\n\n\nLogfile added\n\n\np.args['key']\n can be set also by \np.args.key\n now\n\n\nBug fixes\n\n\n\n\nJuly 3, 2017: 0.7.3\n\n\n\n\nConfig file defaults to \n~/.pyppl.json\n (\n~/.pyppl\n also works)\n\n\nCallfront added\n\n\nEmpty input allowed\n\n\nSame basename name allowed for input files of a job\n\n\nDescription of a proc added\n\n\nAggr Optimized\n\n\nBug #9 Fixed\n\n\nPrivate key supported for ssh runner\n\n\nFeature #7 Implemented\n\n\n\n\nJune 20, 2017: 0.7.2\n\n\n\n\nOptimize isRunning function (using specific job id)\n\n\nSupport python3 now\n\n\nTest on OSX\n\n\nMore debug information for caching\n\n\nBug fixes\n\n\n\n\nJune 15, 2017: 0.7.1\n\n\n\n\nMove pyppl-cli to bin/pyppl\n\n\nchannel.collapse now return the most common directory of paths\n\n\nReport oringinal file of input and bring files\n\n\nShow number of omitted logs\n\n\nBug fixes\n\n\n\n\nJune 13, 2017: 0.7.0\n\n\n\n\nAdd colored log\n\n\nPut jobs in different directories (files with same basename can be used as input files, otherwise it will be overwritten).\n\n\nAdd configuration \ncheckrun\n for \npyppl\n allow \nrunner.isRunning\n to be disabled (save resources on local machine).\n\n\nAdd built-in functions for placeholders; lambda functions do not need to call (just define)\n\n\nFile placeholders (.fn, .bn, .prefix, etc) removed, please use built-in functions instead.\n\n\nAdd an alias \np.ppldir\n for \np.tmpdir\n to avoid confusion.\n\n\nUpdate command line tool accordingly\n\n\nSplit base runner class into two.\n\n\n\n\nMay 30, 2017: 0.6.2\n\n\n\n\nUpdate docs and fix compilation errors from gitbook\n\n\nChange pyppl.dot to pyppl.pyppl.dot; \n\n\nAdd channel.fromFile method; \n\n\nAdd aggr.addProc method; \n\n\nFix proc/aggr copy bugs; \n\n\nFix utils.varname bugs;\n\n\nFix bugs: channel._tuplize does not change list to tuple any more.\n\n\nAdd fold/unfold to channel; \n\n\ncache job immediately after it's done; \n\n\nremove proc in nexts of its depends when its depends are reset; \n\n\nadd dir for input files, prefix for output files;\n\n\nFix utilfs.dirmtime if file not exists; \n\n\nadd pyppl-cli;\n\n\nChange rc code, make it consistent with real rc code.\n\n\n\n\nApr 27, 2017: 0.6.1\n\n\n\n\nOverwrite input file if it exists and not the same file; \n\n\nfix varname bug when there are dots in the function name;\n\n\nAdd brings feature;\n\n\nAdd features to README, and brings to docs\n\n\n\n\nApr 26, 2017: 0.6.0\n\n\n\n\nSet job signature to False if any of the item is False (that means expected files not exists); - Do cache by job itself; \n\n\nMake it possible to cache and export successful jobs even when some jobs failed\n\n\nHost docs in gitbook\n\n\nInit job with log func from proc; \n\n\nAdd docstring for API generation; \n\n\nRedefine return code for outfile not generated; \n\n\nError ignore works now; \n\n\nRewrite runner_local so it fits other runners to extend;\n\n\nFix proc depends on mixed list of procs and aggrs\n\n\n\n\nApr 18, 2017: 0.5.0\n\n\n\n\nFix local runner not waiting (continuiously submitting jobs);\n\n\nAdd property alias for aggr; \n\n\nOutput cleared if job not cached\n\n\nFix bugs when export if outfiles are links; \n\n\nchange default export method to move; \n\n\nadd id and tag to calculate suffix for proc; \n\n\nadd timer; \n\n\nadd isRunning for job so that even if the main thread quit, we can still retrieve the job status;\n\n\n\n\nApr 13, 2017: 0.4.0\n\n\n\n\nAdd files (array) support for input; \n\n\nRecursive update for configuration;\n\n\nAdd aggregations;\n\n\nMove functions to utils; \n\n\nSeparate run for runners to submit and wait;\n\n\nAdd use job class for jobs in a proc; \n\n\nUse \"1,2 3,4\" for channel.fromArgs for multi-width channels; \n\n\nAdd rbind, cbind, slice for channel; \n\n\nAdd alias for some proc properties; \n\n\nRemove callfront for proc; \n\n\nAdd export cache mode; \n\n\nAdd gzip export support (#1); \n\n\nUnify loggers; \n\n\nUse job cache instead of proc cache so that a proc can be partly cached; \n\n\nRewrite buildInput and buildOutput; \n\n\nUse job to construct runners;\n\n\n\n\nMar 14, 2017: 0.2.0\n\n\n\n\nBasic functions\n\n\n\n\nJan 27, 2017: Initiate", 
            "title": "Change log"
        }, 
        {
            "location": "/change-log/#change-log", 
            "text": "", 
            "title": "Change log"
        }, 
        {
            "location": "/change-log/#july-10-2018-100", 
            "text": "Fix runner name issue #31.  Use mkdocs to generate documentations and host them on GitHub pages.  Keep stdout and stderr when a job is cached: #30.  Allow command line arguments to overwrite Parameter's type.  Host the testing procedures with Travis.  Fix other bugs.", 
            "title": "July 10, 2018: 1.0.0 !"
        }, 
        {
            "location": "/change-log/#june-8-2018-096", 
            "text": "Auto-delegate common proc config names to aggr  Add proc.origin to save the original proc id for copied procs  Remove brings, add proc.infile to swith '{{in.(infile)}}' to job.indir path, original path or realpath of the input file  Add process lock to avoid the same processes run simultaneously  Use built-in Box instead of box.Box  Merge template function Rvec and Rlist into R, add repr  Fix #29 and #31, and fix other bugs", 
            "title": "June 8, 2018: 0.9.6"
        }, 
        {
            "location": "/change-log/#mar-6-2018-095", 
            "text": "Add proc.dirsig to disable/enable calculating signatures from deep directories  Add Jobmgr class to handle job distribution  Allow channel.rowAt and colAt to return multiple rows and columns, respectively  Allow empty channel as input (process will be skipped)  Refine all tests  Rewrite thread-safe file system helper functions  Add specific exception classes  Report line # when error happens in template  Add progress bar for jobs  Allow stdout and stderr file as output", 
            "title": "Mar 6, 2018: 0.9.5"
        }, 
        {
            "location": "/change-log/#dec-27-2017-094", 
            "text": "Add yaml support for config file (#26).  Allow empty list for input files.  Merge continuous job ids in log (Make the list shorter).  More details when internal template failed to render (#25)  Ignore .yaml config files if yaml module is not installed.  sleep before testing isRunning to avoid all jobs running it at the same time.  Use repr to output p.args and p.props.  Merge Proc attributes profile and runner. Profile is now an alias of runner, and will be removed finally.", 
            "title": "Dec 27, 2017: 0.9.4"
        }, 
        {
            "location": "/change-log/#nov-20-2017-093", 
            "text": "Beautify parameters help page.  Enable multithreading for job construction and cache checking (set by proc.nthread).  Uniform multiprocessing/threading.  Fix Aggr delegate problems.  Add ProcTree to manage process relations.  Report processes will not run due to prior processes not ran.  Add cclean option for enable/disable cleanup (output check/export) if a job is cached.  Add tooltip for flowchart svg.  Fix job id not saved for runner_sge.  Fix resume assignment issue.  Rewrite proc.log function so that logs from jobs do not mess up when they are multithreaded.  Fix params loaded from file get overwriten.  Add coverage report.", 
            "title": "Nov 20, 2017: 0.9.3"
        }, 
        {
            "location": "/change-log/#oct-23-2017-092", 
            "text": "Add profile for Proc so different procs can run with different profiles.  Add delegate for Aggr.  Add get, repCol, repRow, rowAt method for Channel.  Dont't sleep for batch interval if jobs are cached or running.  Add header argument for Channel.fromFile.  Fix a bunch of bugs.", 
            "title": "Oct 23, 2017: 0.9.2"
        }, 
        {
            "location": "/change-log/#oct-6-2017-091", 
            "text": "Fix issues reported by codacy  Fix an issue checking whether output files generated  Deepcopy args and tplenvs when copy a process  Refer relative path in p.script (with \"file:\" prefix) where p.script is defined  Fix a utils.dictUpdate bug  Template function 'Rlist' now can deal with python list  Add log for export using move method (previously missed)  Allow Aggr instance to set input directly  Switch default showing of parameters loaded from object or file to False  Optimize utils.varname  Add warning if more input data columns than number of input keys  Fix output channel key sequence does not keep  Use object id instead of name as key for PyPPL nexts/paths in case tag is set in pipeline configrations", 
            "title": "Oct 6, 2017: 0.9.1"
        }, 
        {
            "location": "/change-log/#sept-22-2017-090", 
            "text": "Change class name with first letter capitalized  Add resuming from processes (#20)  Fix #19  Group log configuration  Make flowchart themeable and configurable  Make attributes of  Proc  clearer  Add tutorials  Make tests more robust  Enhancer templating, support Jinja2  Set attributes of processes in aggregation with  set", 
            "title": "Sept 22, 2017: 0.9.0"
        }, 
        {
            "location": "/change-log/#aug-4-2017-081", 
            "text": "Add partial echo  Add interactive log from the script  Add partial export  Add log themes and filters  Add compare to command line tool  Fix bugs", 
            "title": "Aug 4, 2017: 0.8.1"
        }, 
        {
            "location": "/change-log/#aug-1-2017-080", 
            "text": "Add slurm and dry runner  Fix bugs when more than 2 input files have same basename  Add indent mark for script, specially useful for python  Make stdout/stderr flushes out for instant runners when p.echo = True  Add  sortby ,  reverse  for  channel.fromPath  Add command line argument parse  Fix a bug that threads do not exit after process is done", 
            "title": "Aug 1, 2017: 0.8.0"
        }, 
        {
            "location": "/change-log/#july-18-2017-074", 
            "text": "Docs updated (thanks @marchon for some grammer corrections)  Some shortcut functions for placeholders added  Check running during polling removed  Logfile added  p.args['key']  can be set also by  p.args.key  now  Bug fixes", 
            "title": "July 18, 2017: 0.7.4"
        }, 
        {
            "location": "/change-log/#july-3-2017-073", 
            "text": "Config file defaults to  ~/.pyppl.json  ( ~/.pyppl  also works)  Callfront added  Empty input allowed  Same basename name allowed for input files of a job  Description of a proc added  Aggr Optimized  Bug #9 Fixed  Private key supported for ssh runner  Feature #7 Implemented", 
            "title": "July 3, 2017: 0.7.3"
        }, 
        {
            "location": "/change-log/#june-20-2017-072", 
            "text": "Optimize isRunning function (using specific job id)  Support python3 now  Test on OSX  More debug information for caching  Bug fixes", 
            "title": "June 20, 2017: 0.7.2"
        }, 
        {
            "location": "/change-log/#june-15-2017-071", 
            "text": "Move pyppl-cli to bin/pyppl  channel.collapse now return the most common directory of paths  Report oringinal file of input and bring files  Show number of omitted logs  Bug fixes", 
            "title": "June 15, 2017: 0.7.1"
        }, 
        {
            "location": "/change-log/#june-13-2017-070", 
            "text": "Add colored log  Put jobs in different directories (files with same basename can be used as input files, otherwise it will be overwritten).  Add configuration  checkrun  for  pyppl  allow  runner.isRunning  to be disabled (save resources on local machine).  Add built-in functions for placeholders; lambda functions do not need to call (just define)  File placeholders (.fn, .bn, .prefix, etc) removed, please use built-in functions instead.  Add an alias  p.ppldir  for  p.tmpdir  to avoid confusion.  Update command line tool accordingly  Split base runner class into two.", 
            "title": "June 13, 2017: 0.7.0"
        }, 
        {
            "location": "/change-log/#may-30-2017-062", 
            "text": "Update docs and fix compilation errors from gitbook  Change pyppl.dot to pyppl.pyppl.dot;   Add channel.fromFile method;   Add aggr.addProc method;   Fix proc/aggr copy bugs;   Fix utils.varname bugs;  Fix bugs: channel._tuplize does not change list to tuple any more.  Add fold/unfold to channel;   cache job immediately after it's done;   remove proc in nexts of its depends when its depends are reset;   add dir for input files, prefix for output files;  Fix utilfs.dirmtime if file not exists;   add pyppl-cli;  Change rc code, make it consistent with real rc code.", 
            "title": "May 30, 2017: 0.6.2"
        }, 
        {
            "location": "/change-log/#apr-27-2017-061", 
            "text": "Overwrite input file if it exists and not the same file;   fix varname bug when there are dots in the function name;  Add brings feature;  Add features to README, and brings to docs", 
            "title": "Apr 27, 2017: 0.6.1"
        }, 
        {
            "location": "/change-log/#apr-26-2017-060", 
            "text": "Set job signature to False if any of the item is False (that means expected files not exists); - Do cache by job itself;   Make it possible to cache and export successful jobs even when some jobs failed  Host docs in gitbook  Init job with log func from proc;   Add docstring for API generation;   Redefine return code for outfile not generated;   Error ignore works now;   Rewrite runner_local so it fits other runners to extend;  Fix proc depends on mixed list of procs and aggrs", 
            "title": "Apr 26, 2017: 0.6.0"
        }, 
        {
            "location": "/change-log/#apr-18-2017-050", 
            "text": "Fix local runner not waiting (continuiously submitting jobs);  Add property alias for aggr;   Output cleared if job not cached  Fix bugs when export if outfiles are links;   change default export method to move;   add id and tag to calculate suffix for proc;   add timer;   add isRunning for job so that even if the main thread quit, we can still retrieve the job status;", 
            "title": "Apr 18, 2017: 0.5.0"
        }, 
        {
            "location": "/change-log/#apr-13-2017-040", 
            "text": "Add files (array) support for input;   Recursive update for configuration;  Add aggregations;  Move functions to utils;   Separate run for runners to submit and wait;  Add use job class for jobs in a proc;   Use \"1,2 3,4\" for channel.fromArgs for multi-width channels;   Add rbind, cbind, slice for channel;   Add alias for some proc properties;   Remove callfront for proc;   Add export cache mode;   Add gzip export support (#1);   Unify loggers;   Use job cache instead of proc cache so that a proc can be partly cached;   Rewrite buildInput and buildOutput;   Use job to construct runners;", 
            "title": "Apr 13, 2017: 0.4.0"
        }, 
        {
            "location": "/change-log/#mar-14-2017-020", 
            "text": "Basic functions", 
            "title": "Mar 14, 2017: 0.2.0"
        }, 
        {
            "location": "/change-log/#jan-27-2017-initiate", 
            "text": "", 
            "title": "Jan 27, 2017: Initiate"
        }
    ]
}